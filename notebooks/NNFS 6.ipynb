{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so far, our neural network can do the folowing: \n",
    "- 1) perform a forward pass (pass data through in one direction)\n",
    "- 2) calculate the loss (how wrong the model is)\n",
    "- 3) calculate the accuracy (percentage the model makes the correct classification)\n",
    "---\n",
    "- the next step is to determine a way to adjust the weights and biases to decrease the loss \n",
    "- finding an intelligent way to adjust the neurons' input's weights and biases to minimize loss is the main challenge of training an accurate neural network\n",
    "---\n",
    "- one approach is to randomly search for weight and bias combinations, but this takes far too long\n",
    "- another idea might be to slowly adjust weights in random directions\n",
    "- if the adjustment provides a decrease in loss, then we will make this the new point to adjust from\n",
    "- if loss instead increases due to the adjustment, then we will revert back\n",
    "- essentially, we will be randomly *adjusting* the weights and biases to find the optimal combination (**optimization**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "New set of weights found, iteration: 0 loss: 1.0989783139303964 acc: 0.3333333333333333\nNew set of weights found, iteration: 38 loss: 1.096916730345913 acc: 0.32666666666666666\nNew set of weights found, iteration: 39 loss: 1.0940855512458771 acc: 0.3333333333333333\nNew set of weights found, iteration: 41 loss: 1.0915705155652229 acc: 0.3333333333333333\nNew set of weights found, iteration: 5934 loss: 1.0802260846826355 acc: 0.7066666666666667\nNew set of weights found, iteration: 5935 loss: 0.8892744380814573 acc: 0.7533333333333333\nNew set of weights found, iteration: 5981 loss: 0.8323725673406076 acc: 0.6966666666666667\nNew set of weights found, iteration: 5984 loss: 0.8104052439857645 acc: 0.68\nNew set of weights found, iteration: 5985 loss: 0.7535038170810421 acc: 0.7366666666666667\nNew set of weights found, iteration: 5986 loss: 0.6975772441844947 acc: 0.74\nNew set of weights found, iteration: 5987 loss: 0.6313569181442653 acc: 0.7666666666666667\nNew set of weights found, iteration: 5991 loss: 0.5761907884088608 acc: 0.7866666666666666\n"
    }
   ],
   "source": [
    "# Our temporary sample dataset\n",
    "def create_data(points, classes):\n",
    "    X = np.zeros((points*classes, 2)) # list of given number of points per each class, containing pairs of values\n",
    "    y = np.zeros(points*classes, dtype='uint8')  # same as above, but containing simple values - classes\n",
    "    for class_number in range(classes):\n",
    "        ix = range(points*class_number, points*(class_number+1))  # index in class\n",
    "        X[ix] = np.c_[np.random.randn(points)*.1 + (class_number)/3, np.random.randn(points)*.1 + 0.5]\n",
    "        y[ix] = class_number\n",
    "    return X, y\n",
    "\n",
    "# Create dataset\n",
    "X, y = create_data(100, 3)\n",
    "\n",
    "# Create model\n",
    "dense1 = Layer_Dense(2, 3)  # first dense layer, 2 inputs (each sample has 2 features), 3 outputs\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(3, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Helper variables\n",
    "lowest_loss = 9999999  # some initial value\n",
    "best_dense1_weights = dense1.weights\n",
    "best_dense1_biases = dense1.biases\n",
    "best_dense2_weights = dense2.weights\n",
    "best_dense2_biases = dense2.biases\n",
    "\n",
    "for iteration in range(10000):\n",
    "\n",
    "    # Update weights with some small random values\n",
    "    dense1.weights += 0.05 * np.random.randn(2, 3)\n",
    "    dense1.biases += 0.05 * np.random.randn(1, 3)\n",
    "    dense2.weights += 0.05 * np.random.randn(3, 3)\n",
    "    dense2.biases += 0.05 * np.random.randn(1, 3)\n",
    "\n",
    "    # Make a forward pass of our training data thru this layer\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Calculate loss from output of activation2 so softmax activation and accuracy\n",
    "    loss = loss_function.forward(activation2.output, y)\n",
    "    predictions = np.argmax(activation2.output, axis=1)  # calculate values along first axis\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    # If loss is smaller - print and save weights and biases aside\n",
    "    if loss < lowest_loss:\n",
    "        print('New set of weights found, iteration:', iteration, 'loss:', loss, 'acc:', accuracy)\n",
    "        best_dense1_weights = dense1.weights\n",
    "        best_dense1_biases = dense1.biases\n",
    "        best_dense2_weights = dense2.weights\n",
    "        best_dense2_biases = dense2.biases\n",
    "        lowest_loss = loss\n",
    "    # revert weights and biases\n",
    "    else:\n",
    "        dense1.weights = best_dense1_weights\n",
    "        dense1.biases = best_dense1_biases\n",
    "        dense2.weights = best_dense2_weights\n",
    "        dense2.biases = best_dense2_biases"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}