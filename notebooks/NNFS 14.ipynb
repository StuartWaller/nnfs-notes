{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14: L1 and L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- techniques that reduce generalization error (overfitting, among others) fall under the umbrella of **regularization**\n",
    "- L1 and L2 regularization both calculate a number (called a **penalty**) that is added to the loss value in an effort to **penalize the model for large weights**\n",
    "- note that it is better to have many neurons contributing the output rather than a select few\n",
    "---\n",
    "- the **L1 regularization** penality is the sum of all the absolute values for the weights and biases\n",
    "- this is a linear penality as the returned regularization loss is directly proportional to the weight values\n",
    "- L1 regularization penalizes small weights more, causing the model to grow invariant to small inputs and variant only to the bigger ones\n",
    "- the **L2 regularization** penalty is the sum of the squared weights and biases \n",
    "- this is a non-linear approach because it penalizes larger weights and biases more than smaller ones\n",
    "- with that being said, **L2 regularization is commonly used** as it does not affect small parameter values and does not allow the model's weights to grow too large\n",
    "- L1 regularization is only used with L2 regularization, but L2 regularization, as just mentioned, is commonly used individually\n",
    "- regularization functions of this type drive the sum of weights towards 0, which is helpful towards alleviating exploding gradients\n",
    "---\n",
    "- we also want to dictate how much of an impact of the regularization penalty\n",
    "- in the mathematical equation, this value is referred to as **lambda**, where a higher value yields a larger penalty\n",
    "---\n",
    "- using code notation:\n",
    "- `l1 = lambda_l1 * sum(abs(weights))`\n",
    "- `l2 = lambda_l2 * sum(weights**2)`\n",
    "- `loss = data_loss + l1 + l2`\n",
    "---\n",
    "- to implement regularization in our neural network code, we’ll start with the `__init__()` method of the `Layer_Dense()` class, which will house the lambda values for regularization as these can be set separately for each individual layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Layer initialization\n",
    "    def __init__(self, inputs, neurons, weight_regularizer_l1=0, weight_regularizer_l2=0, bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(inputs, neurons)\n",
    "        self.biases = np.zeros((1, neurons))\n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- again, this method sets the lambda values\n",
    "- now we’re going to create a new general `Loss()` class, which can be inherited by any of our specific loss functions (such as our existing `Loss_CategoricalCrossentropy()` class) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Regularization loss calculation\n",
    "    def regularization_loss(self, layer):\n",
    "\n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "\n",
    "        # L1 regularization - weights\n",
    "        if layer.weight_regularizer_l1 > 0:  # only calculate when factor greater than 0\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "\n",
    "        # L2 regularization - weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "\n",
    "        # L1 regularization - biases\n",
    "        if layer.bias_regularizer_l1 > 0:  # only calculate when factor greater than 0\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "\n",
    "        # L2 regularization - biases\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "\n",
    "        return regularization_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- next, as described above, we will update the `Loss_CategoricalCrossentropy()` class to inherit the result from the general `Loss()` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy(Loss): # pass it as a parameter through the class\n",
    "    pass # temporary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- then we’ll calculate the regularization loss and add it to our calculated class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "   # Calculate loss from output of activation2 so softmax activation\n",
    "    data_loss = loss_function.forward(activation2.output, y)\n",
    "\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = loss_function.regularization_loss(dense1) + loss_function.regularization_loss(dense2)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this completes the forward pass for regularization, but this also means our overall loss has changed as part of the calculation can possibly include regularization, which must be accounted for in the backpropagation of the gradients\n",
    "- thus, we will now cover the partial derivatives for both L1 and L2 regularization\n",
    "- we are calculating the derivative with respect to the weights, and the resulting gradient is what we’ll use to update the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[1, 1, -1]"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "weights = [0.2, 0.8, -0.5]  # weights of one neuron\n",
    "dL1 = []  # array of partial derivatives of L1 regularization\n",
    "for weight in weights:\n",
    "    if weight >= 0:\n",
    "        dL1.append(1)\n",
    "    else:\n",
    "        dL1.append(-1)\n",
    "dL1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now let's try to modify our `Loss()` class to work with multiple neurons in a layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[1, 1, -1, 1], [1, -1, 1, -1], [-1, -1, 1, 1]]"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "weights = [[0.2, 0.8, -0.5, 1], # now we have 3 sets of weights\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "dL1 = []  # array of partial derivatives of L1 regularization (eventual list of lists)\n",
    "for neuron in weights:\n",
    "    neuron_dL1 = []  # derivatives related to one neuron\n",
    "    for weight in neuron:\n",
    "        if weight >= 0:\n",
    "            neuron_dL1.append(1)\n",
    "        else:\n",
    "            neuron_dL1.append(-1)\n",
    "    dL1.append(neuron_dL1)\n",
    "dL1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- as with most of our functions, the above code can be simplified using NumPy\n",
    "- with NumPy, we’re going to use conditions and binary masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[ 1.  1. -1.  1.]\n [ 1. -1.  1. -1.]\n [-1. -1.  1.  1.]]\n"
    }
   ],
   "source": [
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -0.27, 0.17, 0.87]])\n",
    "\n",
    "# two conditions\n",
    "dL1 = weights.copy()\n",
    "dL1[dL1 >= 0] = 1\n",
    "dL1[dL1 < 0] = -1\n",
    "\n",
    "print(dL1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this returned an array of the same shape containing values of 1 and -1 (the gradient of the absolute function)\n",
    "- we can now take these and update the `backward()` pass method for the `Layer_Dense()` class\n",
    "- for L1 regularization, we’ll multiply the code above by lambda for the weights and biases (separately)\n",
    "- for L2 regularization, as previously discussed, we simply take the weights/biases, multiply them by (2 * lambda), and add that product to the gradients: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    ...\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "        # Gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = self.weights.copy()\n",
    "            dL1[dL1 >= 0] = 1\n",
    "            dL1[dL1 < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = self.biases.copy()\n",
    "            dL1[dL1 >= 0] = 1\n",
    "            dL1[dL1 < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "\n",
    "        # Gradient on values\n",
    "        self.dvalues = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's also update our `print()` statement to output the regularization loss and the overall loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 0 acc: 0.360 loss: 1.099 (data_loss: 1.099 reg_loss: 0.000) ) lr: 0.05\nepoch: 100 acc: 0.707 loss: 0.698 (data_loss: 0.696 reg_loss: 0.002) ) lr: 0.04999752506207612\nepoch: 200 acc: 0.797 loss: 0.527 (data_loss: 0.523 reg_loss: 0.004) ) lr: 0.049990050996574775\nepoch: 300 acc: 0.843 loss: 0.419 (data_loss: 0.413 reg_loss: 0.006) ) lr: 0.04997758005043209\nepoch: 400 acc: 0.850 loss: 0.364 (data_loss: 0.357 reg_loss: 0.007) ) lr: 0.04996011596895705\nepoch: 500 acc: 0.883 loss: 0.322 (data_loss: 0.314 reg_loss: 0.008) ) lr: 0.04993766399395728\nepoch: 600 acc: 0.897 loss: 0.291 (data_loss: 0.282 reg_loss: 0.009) ) lr: 0.04991023086111661\nepoch: 700 acc: 0.907 loss: 0.269 (data_loss: 0.259 reg_loss: 0.010) ) lr: 0.049877824796627425\nepoch: 800 acc: 0.897 loss: 0.257 (data_loss: 0.246 reg_loss: 0.011) ) lr: 0.0498404555130797\nepoch: 900 acc: 0.917 loss: 0.230 (data_loss: 0.218 reg_loss: 0.012) ) lr: 0.04979813420460921\nepoch: 1000 acc: 0.917 loss: 0.216 (data_loss: 0.203 reg_loss: 0.013) ) lr: 0.04975087354130951\nepoch: 1100 acc: 0.923 loss: 0.205 (data_loss: 0.192 reg_loss: 0.013) ) lr: 0.04969868766290968\nepoch: 1200 acc: 0.913 loss: 0.209 (data_loss: 0.195 reg_loss: 0.014) ) lr: 0.04964159217172425\nepoch: 1300 acc: 0.920 loss: 0.192 (data_loss: 0.178 reg_loss: 0.014) ) lr: 0.04957960412487896\nepoch: 1400 acc: 0.937 loss: 0.183 (data_loss: 0.168 reg_loss: 0.015) ) lr: 0.04951274202581829\nepoch: 1500 acc: 0.947 loss: 0.176 (data_loss: 0.160 reg_loss: 0.016) ) lr: 0.049441025815100244\nepoch: 1600 acc: 0.900 loss: 0.244 (data_loss: 0.228 reg_loss: 0.016) ) lr: 0.049364476860485375\nepoch: 1700 acc: 0.940 loss: 0.160 (data_loss: 0.143 reg_loss: 0.017) ) lr: 0.04928311794632677\nepoch: 1800 acc: 0.943 loss: 0.156 (data_loss: 0.139 reg_loss: 0.017) ) lr: 0.04919697326226773\nepoch: 1900 acc: 0.943 loss: 0.153 (data_loss: 0.136 reg_loss: 0.017) ) lr: 0.04910606839125617\nepoch: 2000 acc: 0.960 loss: 0.151 (data_loss: 0.134 reg_loss: 0.017) ) lr: 0.049010430296883054\nepoch: 2100 acc: 0.960 loss: 0.148 (data_loss: 0.131 reg_loss: 0.017) ) lr: 0.048910087310054\nepoch: 2200 acc: 0.957 loss: 0.146 (data_loss: 0.128 reg_loss: 0.017) ) lr: 0.04880506911500336\nepoch: 2300 acc: 0.957 loss: 0.143 (data_loss: 0.126 reg_loss: 0.017) ) lr: 0.048695406734660295\nepoch: 2400 acc: 0.957 loss: 0.141 (data_loss: 0.124 reg_loss: 0.018) ) lr: 0.048581132515377004\nepoch: 2500 acc: 0.960 loss: 0.140 (data_loss: 0.122 reg_loss: 0.018) ) lr: 0.048462280111029786\nepoch: 2600 acc: 0.957 loss: 0.138 (data_loss: 0.120reg_loss: 0.018) ) lr: 0.04833888446650344\nepoch: 2700 acc: 0.960 loss: 0.136 (data_loss: 0.118 reg_loss: 0.018) ) lr: 0.0482109818005708\nepoch: 2800 acc: 0.967 loss: 0.135 (data_loss: 0.116 reg_loss: 0.018) ) lr: 0.048078609588178944\nepoch: 2900 acc: 0.967 loss:0.133 (data_loss: 0.115 reg_loss: 0.018) ) lr: 0.04794180654215414\nepoch: 3000 acc: 0.967 loss: 0.132 (data_loss: 0.114 reg_loss: 0.019) ) lr: 0.047800612594338286\nepoch: 3100 acc: 0.967 loss: 0.133 (data_loss: 0.114 reg_loss: 0.019) ) lr: 0.0476550688761695\nepoch: 3200 acc: 0.967 loss: 0.131 (data_loss: 0.112 reg_loss: 0.019) ) lr: 0.0475052176987199\nepoch: 3300 acc: 0.967 loss: 0.130 (data_loss: 0.111 reg_loss: 0.019) ) lr: 0.0473511025322044\nepoch: 3400 acc: 0.970 loss: 0.129 (data_loss: 0.110 reg_loss: 0.019) ) lr: 0.047192767984974744\nepoch: 3500 acc: 0.970 loss: 0.128 (data_loss: 0.109 reg_loss: 0.019) ) lr: 0.047030259782012335\nepoch: 3600 acc: 0.967 loss: 0.127 (data_loss: 0.108 reg_loss: 0.019) ) lr: 0.04686362474293419\nepoch: 3700 acc: 0.970 loss: 0.126 (data_loss: 0.106 reg_loss: 0.019) ) lr: 0.046692910759528077\nepoch: 3800 acc: 0.967 loss: 0.125 (data_loss: 0.105 reg_loss: 0.019) ) lr: 0.04651816677283049\nepoch: 3900 acc: 0.967 loss: 0.124 (data_loss: 0.104 reg_loss: 0.019) ) lr: 0.046339442749763586\nepoch: 4000 acc: 0.967 loss: 0.122 (data_loss: 0.103 reg_loss: 0.020) ) lr: 0.04615678965934673\nepoch: 4100 acc: 0.967 loss: 0.121(data_loss: 0.102 reg_loss: 0.020) ) lr: 0.04597025944849779\nepoch: 4200 acc: 0.963 loss: 0.121 (data_loss: 0.102 reg_loss: 0.020) ) lr: 0.045779905017441204\nepoch: 4300 acc: 0.967 loss: 0.120 (data_loss: 0.100 reg_loss: 0.020) ) lr: 0.04558578019473819\nepoch: 4400 acc: 0.937 loss: 0.152 (data_loss: 0.132 reg_loss: 0.020) ) lr: 0.04538793971195641\nepoch: 4500 acc: 0.967 loss: 0.124 (data_loss: 0.104 reg_loss: 0.020) ) lr: 0.04518643917799511\nepoch: 4600 acc: 0.963 loss: 0.122 (data_loss: 0.101 reg_loss: 0.020) ) lr: 0.04498133505308287\nepoch: 4700 acc: 0.963 loss: 0.121 (data_loss: 0.100 reg_loss: 0.020) ) lr: 0.044772684622464705\nepoch:4800 acc: 0.963 loss: 0.119 (data_loss: 0.099 reg_loss: 0.020) ) lr: 0.04456054596979586\nepoch: 4900 acc: 0.963 loss: 0.118 (data_loss: 0.098 reg_loss: 0.020) ) lr: 0.04434497795025991\nepoch: 5000 acc: 0.963 loss: 0.117 (data_loss: 0.097 reg_loss: 0.020) ) lr: 0.04412604016342762\nepoch: 5100 acc: 0.963 loss: 0.117 (data_loss: 0.096 reg_loss: 0.020) ) lr: 0.04390379292587474\nepoch: 5200 acc: 0.963 loss: 0.116 (data_loss: 0.095 reg_loss: 0.021) ) lr: 0.043678297243576227\nepoch: 5300 acc: 0.963 loss: 0.115 (data_loss: 0.095 reg_loss: 0.021) ) lr: 0.04344961478409441\nepoch: 5400 acc: 0.963 loss: 0.115 (data_loss: 0.094 reg_loss: 0.021) ) lr: 0.04321780784857837\nepoch: 5500 acc: 0.963 loss: 0.114 (data_loss: 0.093 reg_loss: 0.021) ) lr: 0.042982939343593214\nepoch: 5600 acc: 0.963 loss: 0.113 (data_loss: 0.093 reg_loss: 0.021) ) lr: 0.042745072752796036\nepoch: 5700 acc: 0.970 loss: 0.113 (data_loss: 0.092 reg_loss: 0.021) ) lr: 0.04250427210847695\nepoch: 5800 acc: 0.970 loss: 0.112 (data_loss: 0.091 reg_loss: 0.021) ) lr: 0.04226060196298256\nepoch: 5900 acc: 0.970 loss: 0.111 (data_loss: 0.090 reg_loss: 0.021) ) lr: 0.04201412736003991\nepoch: 6000 acc: 0.970 loss: 0.111 (data_loss: 0.090 reg_loss: 0.021) ) lr: 0.041764913805998484\nepoch: 6100 acc: 0.833 loss: 0.577 (data_loss: 0.555 reg_loss: 0.021) ) lr: 0.041513027241007916\nepoch: 6200 acc: 0.963 loss: 0.112 (data_loss: 0.091 reg_loss: 0.021) ) lr: 0.04125853401014897\nepoch: 6300 acc: 0.967 loss: 0.111 (data_loss: 0.090 reg_loss: 0.021) ) lr: 0.04100150083453588\nepoch: 6400 acc: 0.967 loss: 0.110 (data_loss: 0.089 reg_loss: 0.021) ) lr: 0.040741994782406296\nepoch: 6500 acc: 0.970 loss: 0.109 (data_loss: 0.088 reg_loss: 0.021) ) lr: 0.040480083240217575\nepoch: 6600 acc: 0.970 loss: 0.109 (data_loss: 0.088 reg_loss: 0.021) ) lr: 0.04021583388376586\nepoch: 6700 acc: 0.970 loss: 0.108 (data_loss: 0.087 reg_loss: 0.021) ) lr: 0.03994931464934522\nepoch: 6800 acc: 0.970 loss: 0.108 (data_loss: 0.087 reg_loss: 0.021) ) lr: 0.03968059370496389\nepoch: 6900 acc: 0.970 loss: 0.108 (data_loss: 0.086 reg_loss: 0.021) ) lr: 0.03940973942163448\nepoch: 7000 acc: 0.970 loss: 0.107 (data_loss: 0.086 reg_loss: 0.021) ) lr: 0.03913682034475463\nepoch: 7100 acc: 0.970 loss: 0.107 (data_loss: 0.086 reg_loss: 0.021) ) lr: 0.03886190516559515\nepoch: 7200 acc: 0.970 loss: 0.106 (data_loss: 0.085 reg_loss: 0.021) ) lr: 0.03858506269291113\nepoch: 7300 acc: 0.970 loss: 0.106 (data_loss: 0.085 reg_loss: 0.021) ) lr: 0.03830636182469295\nepoch: 7400 acc: 0.970 loss:0.106 (data_loss: 0.084 reg_loss: 0.021) ) lr: 0.03802587152007248\nepoch: 7500 acc: 0.970 loss: 0.105 (data_loss: 0.084 reg_loss: 0.021) ) lr: 0.03774366077140049\nepoch: 7600 acc: 0.970 loss: 0.105 (data_loss: 0.084 reg_loss: 0.021) ) lr: 0.037459798576510786\nepoch: 7700 acc: 0.970 loss: 0.105 (data_loss: 0.083 reg_loss: 0.021) ) lr: 0.03717435391118595\nepoch: 7800 acc: 0.970 loss: 0.104 (data_loss: 0.083 reg_loss: 0.021) ) lr: 0.03688739570184001\nepoch: 7900 acc: 0.970 loss: 0.104 (data_loss: 0.083 reg_loss: 0.021) ) lr: 0.0365989927984322\nepoch: 8000 acc: 0.970 loss: 0.103 (data_loss: 0.082 reg_loss: 0.021) ) lr: 0.03630921394762722\nepoch: 8100 acc: 0.967 loss: 0.104 (data_loss: 0.083 reg_loss: 0.021) ) lr: 0.0360181277662148\nepoch: 8200 acc: 0.967 loss: 0.104 (data_loss: 0.083 reg_loss: 0.021) ) lr: 0.03572580271480377\nepoch: 8300 acc: 0.977 loss: 0.108 (data_loss: 0.087 reg_loss: 0.021) ) lr: 0.035432307071803025\nepoch: 8400 acc: 0.967 loss: 0.107 (data_loss: 0.085 reg_loss: 0.021) ) lr: 0.03513770890770358\nepoch: 8500 acc: 0.970 loss: 0.104 (data_loss: 0.083 reg_loss: 0.021) ) lr: 0.034842076059673724\nepoch: 8600 acc: 0.970 loss: 0.103 (data_loss: 0.082 reg_loss: 0.021) ) lr: 0.034545476106480955\nepoch: 8700 acc: 0.970 loss: 0.103 (data_loss: 0.081 reg_loss: 0.021) ) lr: 0.03424797634375167\nepoch: 8800 acc: 0.970 loss: 0.102 (data_loss: 0.081 reg_loss: 0.021) ) lr: 0.03394964375958191\nepoch: 8900 acc: 0.970 loss: 0.102 (data_loss: 0.080 reg_loss: 0.021) ) lr: 0.0336505450105096\nepoch: 9000 acc: 0.970 loss: 0.102 (data_loss: 0.080 reg_loss: 0.021) ) lr: 0.03335074639786025\nepoch: 9100 acc: 0.970 loss: 0.101 (data_loss: 0.080 reg_loss: 0.021) ) lr: 0.033050313844476605\nepoch: 9200 acc: 0.970 loss: 0.101 (data_loss: 0.079 reg_loss: 0.022) ) lr: 0.03274931287184286\nepoch: 9300 acc: 0.970 loss: 0.101 (data_loss: 0.079 reg_loss: 0.022) ) lr: 0.03244780857761376\nepoch: 9400 acc: 0.970 loss: 0.100 (data_loss: 0.079 reg_loss: 0.022) ) lr: 0.03214586561355804\nepoch: 9500 acc: 0.970 loss: 0.100 (data_loss: 0.078 reg_loss: 0.022) ) lr: 0.03184354816392586\nepoch: 9600 acc: 0.970 loss: 0.100 (data_loss: 0.078 reg_loss: 0.022) ) lr: 0.03154091992424932\nepoch: 9700 acc: 0.970 loss: 0.099 (data_loss: 0.078 reg_loss: 0.022) ) lr: 0.031238044080584084\nepoch: 9800 acc: 0.970 loss: 0.099 (data_loss: 0.077 reg_loss: 0.022) ) lr: 0.03093498328920125\nepoch: 9900 acc: 0.970 loss: 0.099 (data_loss: 0.077 reg_loss: 0.022) ) lr: 0.030631799656736635\nepoch: 10000 acc: 0.970 loss: 0.098 (data_loss: 0.077 reg_loss: 0.022) ) lr: 0.030328554720805104\nvalidation, acc: 0.837, loss: 0.907\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "# Our sample dataset\n",
    "def create_data(n, k):\n",
    "    X = np.zeros((n*k, 2))  # data matrix (each row = single example)\n",
    "    y = np.zeros(n*k, dtype='uint8')  # class labels\n",
    "    for j in range(k):\n",
    "        ix = range(n*j, n*(j+1))\n",
    "        r = np.linspace(0.0, 1, n)  # radius\n",
    "        t = np.linspace(j*4, (j+1)*4, n) + np.random.randn(n)*0.2  # theta\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = j\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, inputs, neurons, weight_regularizer_l1=0, weight_regularizer_l2=0, bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(inputs, neurons)\n",
    "        self.biases = np.zeros((1, neurons))\n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from input ones, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "        # Gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = self.weights.copy()\n",
    "            dL1[dL1 >= 0] = 1\n",
    "            dL1[dL1 < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = self.biases.copy()\n",
    "            dL1[dL1 >= 0] = 1\n",
    "            dL1[dL1 < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "\n",
    "        # Gradient on values\n",
    "        self.dvalues = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from input ones\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        dvalues = dvalues.copy()  # Since we need to modify original variable, let;s make a copy of values first\n",
    "        dvalues[self.inputs <= 0] = 0  # Zero gradient where input values were negative\n",
    "        self.dvalues = dvalues\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        self.dvalues = dvalues\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Regularization loss calculation\n",
    "    def regularization_loss(self, layer):\n",
    "\n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "\n",
    "        # L1 regularization - weights\n",
    "        if layer.weight_regularizer_l1 > 0:  # only calculate when factor greater than 0\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "\n",
    "        # L2 regularization - weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "\n",
    "        # L1 regularization - biases\n",
    "        if layer.bias_regularizer_l1 > 0:  # only calculate when factor greater than 0\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "\n",
    "        # L2 regularization - biases\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = y_pred.shape[0]\n",
    "\n",
    "        # Probabilities for target values - only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_pred = y_pred[range(samples), y_true]\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(y_pred)\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        if len(y_true.shape) == 2:\n",
    "            negative_log_likelihoods *= y_true\n",
    "\n",
    "        # Overall loss\n",
    "        data_loss = np.sum(negative_log_likelihoods) / samples\n",
    "        return data_loss\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        samples = dvalues.shape[0]\n",
    "\n",
    "        dvalues = dvalues.copy()  # We need to modify variable directly, make a copy first then\n",
    "        dvalues[range(samples), y_true] -= 1\n",
    "        dvalues = dvalues / samples\n",
    "\n",
    "        self.dvalues = dvalues\n",
    "\n",
    "\n",
    "# SGD Optimizer\n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0., nesterov=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        self.nesterov = nesterov\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.current_learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain momentum arrays, create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_momentums'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "\n",
    "            # Build weight updates with momentum - take previous updates multiplied by retain factor and update with current gradients\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "            # Apply Nesterov as well?\n",
    "            if self.nesterov:\n",
    "                weight_updates = self.momentum * weight_updates - self.current_learning_rate * layer.dweights\n",
    "                bias_updates = self.momentum * bias_updates - self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        # Update weights with updates which are either vanilla, momentum or momentum+nesterov updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Adagrad Optimizer\n",
    "class Optimizer_Adagrad:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.current_learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays, create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# RMSprop Optimizer\n",
    "class Optimizer_RMSprop:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.current_learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays, create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Adam Optimizer\n",
    "class Optimizer_Adam:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.current_learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays, create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum  with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "        # Get corrected momentum\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))  # self.iteration is 0 at first pass ans we need to start with 1 here\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "        # Get corrected bias\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "X, y = create_data(100, 3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=1e-5, bias_regularizer_l2=1e-5)  # first dense layer, 2 inputs (each sample has 2 features), 3 outputs\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "#optimizer = Optimizer_SGD(decay=1e-8, momentum=0.9)\n",
    "#optimizer = Optimizer_Adagrad(decay=1e-8)\n",
    "#optimizer = Optimizer_RMSprop(learning_rate=0.05, decay=4e-8, rho=0.999)\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, decay=1e-8)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Make a forward pass of our training data thru this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Make a forward pass thru second Dense layer - it takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Calculate loss from output of activation2 so softmax activation\n",
    "    data_loss = loss_function.forward(activation2.output, y)\n",
    "\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = loss_function.regularization_loss(dense1) + loss_function.regularization_loss(dense2)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    predictions = np.argmax(activation2.output, axis=1)  # calculate values along first axis\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print('epoch:', epoch, 'acc:', f'{accuracy:.3f}', 'loss:', f'{loss:.3f}', '(data_loss:', f'{data_loss:.3f}', 'reg_loss:', f'{regularization_loss:.3f})', ')', 'lr:', optimizer.current_learning_rate)\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dvalues)\n",
    "    dense2.backward(activation2.dvalues)\n",
    "    activation1.backward(dense2.dvalues)\n",
    "    dense1.backward(activation1.dvalues)\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "# Validate model (we just do a forward pass)\n",
    "\n",
    "# Create test dataset\n",
    "X_test, y_test = create_data(100, 3)\n",
    "\n",
    "# Make a forward pass of our training data thru this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# Make a forward pass thru activation function - we take output of previous layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Make a forward pass thru second Dense layer - it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Make a forward pass thru activation function - we take output of previous layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Calculate loss from output of activation2 so softmax activation\n",
    "loss = loss_function.forward(activation2.output, y_test)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "predictions = np.argmax(activation2.output, axis=1)  # calculate values along first axis\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- after adding the L2 regularization term into the hidden layer, we've achieved a lower loss and a higher accuracy\n",
    "- let's also take a moment to exemplify how a simple increase in training data can make a large difference (100 --> 1000 samples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 0 acc: 0.323 loss: 1.099 (data_loss: 1.099 reg_loss: 0.000) ) lr: 0.05\nepoch: 100 acc: 0.633 loss: 0.806 (data_loss: 0.804 reg_loss: 0.001) ) lr: 0.04999752506207612\nepoch: 200 acc: 0.741 loss: 0.645 (data_loss: 0.642 reg_loss: 0.003) ) lr: 0.049990050996574775\nepoch: 300 acc: 0.755 loss: 0.583 (data_loss: 0.579 reg_loss: 0.004) ) lr: 0.04997758005043209\nepoch: 400 acc: 0.766 loss: 0.547 (data_loss: 0.542 reg_loss: 0.005) ) lr: 0.04996011596895705\nepoch: 500 acc: 0.796 loss: 0.504 (data_loss: 0.498 reg_loss: 0.006) ) lr: 0.04993766399395728\nepoch: 600 acc: 0.818 loss: 0.480 (data_loss: 0.473 reg_loss: 0.007) ) lr: 0.04991023086111661\nepoch: 700 acc: 0.848 loss: 0.409 (data_loss: 0.401 reg_loss: 0.008) ) lr: 0.049877824796627425\nepoch: 800 acc: 0.855 loss: 0.366 (data_loss: 0.357 reg_loss: 0.009) ) lr: 0.0498404555130797\nepoch: 900 acc: 0.865 loss: 0.364 (data_loss: 0.354 reg_loss: 0.010) ) lr: 0.04979813420460921\nepoch: 1000 acc: 0.875 loss: 0.334 (data_loss: 0.323 reg_loss: 0.011) ) lr: 0.04975087354130951\nepoch: 1100 acc: 0.882 loss: 0.321 (data_loss: 0.310 reg_loss: 0.012) ) lr: 0.04969868766290968\nepoch: 1200 acc: 0.880 loss: 0.312 (data_loss: 0.300 reg_loss: 0.012) ) lr: 0.04964159217172425\nepoch: 1300 acc: 0.883 loss: 0.314 (data_loss: 0.302 reg_loss: 0.013) ) lr: 0.04957960412487896\nepoch: 1400 acc: 0.888 loss: 0.302 (data_loss: 0.289 reg_loss: 0.013) ) lr: 0.04951274202581829\nepoch: 1500 acc: 0.881 loss: 0.302 (data_loss: 0.289 reg_loss: 0.013) ) lr: 0.049441025815100244\nepoch: 1600 acc: 0.881 loss: 0.295 (data_loss: 0.281 reg_loss: 0.014) ) lr: 0.049364476860485375\nepoch: 1700 acc: 0.886 loss: 0.292 (data_loss: 0.278 reg_loss: 0.014) ) lr: 0.04928311794632677\nepoch: 1800 acc: 0.893 loss: 0.285 (data_loss: 0.271 reg_loss: 0.014) ) lr: 0.04919697326226773\nepoch: 1900 acc: 0.893 loss: 0.283 (data_loss: 0.268 reg_loss: 0.014) ) lr: 0.04910606839125617\nepoch: 2000 acc: 0.893 loss: 0.280 (data_loss: 0.266 reg_loss: 0.015) ) lr: 0.049010430296883054\nepoch: 2100 acc: 0.880 loss: 0.313 (data_loss: 0.298 reg_loss: 0.015) ) lr: 0.048910087310054\nepoch: 2200 acc: 0.894 loss: 0.276 (data_loss: 0.261 reg_loss: 0.015) ) lr: 0.04880506911500336\nepoch: 2300 acc: 0.894 loss: 0.277 (data_loss: 0.261 reg_loss: 0.015) ) lr: 0.048695406734660295\nepoch: 2400 acc: 0.867 loss: 0.356 (data_loss: 0.340 reg_loss: 0.016) ) lr: 0.048581132515377004\nepoch: 2500 acc: 0.897 loss: 0.273 (data_loss: 0.257 reg_loss: 0.016) ) lr: 0.048462280111029786\nepoch: 2600 acc: 0.899 loss: 0.270 (data_loss: 0.254 reg_loss: 0.016) ) lr: 0.04833888446650344\nepoch: 2700 acc: 0.899 loss: 0.268 (data_loss: 0.253 reg_loss: 0.016) ) lr: 0.0482109818005708\nepoch: 2800 acc: 0.900 loss: 0.266 (data_loss: 0.251 reg_loss: 0.016) ) lr: 0.048078609588178944\nepoch: 2900 acc: 0.900 loss: 0.265 (data_loss: 0.249 reg_loss: 0.016) ) lr: 0.04794180654215414\nepoch: 3000 acc: 0.900 loss: 0.264 (data_loss: 0.248 reg_loss: 0.016) ) lr: 0.047800612594338286\nepoch: 3100 acc: 0.900 loss: 0.266 (data_loss: 0.250 reg_loss: 0.016) ) lr: 0.0476550688761695\nepoch: 3200 acc: 0.900 loss: 0.264 (data_loss: 0.248 reg_loss: 0.016) ) lr: 0.0475052176987199\nepoch: 3300 acc: 0.895 loss: 0.262 (data_loss: 0.246 reg_loss: 0.016) ) lr: 0.0473511025322044\nepoch: 3400 acc: 0.898 loss: 0.266 (data_loss: 0.249 reg_loss: 0.016) ) lr: 0.047192767984974744\nepoch: 3500 acc: 0.899 loss: 0.261 (data_loss: 0.244 reg_loss: 0.016) ) lr: 0.047030259782012335\nepoch: 3600 acc: 0.901 loss: 0.259 (data_loss: 0.243 reg_loss: 0.016) ) lr: 0.04686362474293419\nepoch: 3700 acc: 0.895 loss: 0.261 (data_loss: 0.244 reg_loss: 0.016) ) lr: 0.046692910759528077\nepoch: 3800 acc: 0.900 loss: 0.259 (data_loss: 0.243 reg_loss: 0.016) ) lr: 0.04651816677283049\nepoch: 3900 acc: 0.903 loss: 0.257 (data_loss: 0.241 reg_loss: 0.017) ) lr: 0.046339442749763586\nepoch: 4000 acc: 0.899 loss: 0.261 (data_loss: 0.244 reg_loss: 0.017) ) lr: 0.04615678965934673\nepoch: 4100 acc: 0.892 loss: 0.266 (data_loss: 0.249 reg_loss: 0.017) ) lr: 0.04597025944849779\nepoch: 4200 acc: 0.890 loss: 0.275 (data_loss: 0.258 reg_loss: 0.017) ) lr: 0.045779905017441204\nepoch: 4300 acc: 0.902 loss: 0.254 (data_loss: 0.238 reg_loss: 0.017) ) lr: 0.04558578019473819\nepoch: 4400 acc: 0.900 loss: 0.256 (data_loss: 0.239 reg_loss: 0.017) ) lr: 0.04538793971195641\nepoch: 4500 acc: 0.904 loss: 0.253 (data_loss: 0.236 reg_loss: 0.017) ) lr: 0.04518643917799511\nepoch: 4600 acc: 0.905 loss: 0.252 (data_loss: 0.235 reg_loss: 0.017) ) lr: 0.04498133505308287\nepoch: 4700 acc: 0.901 loss: 0.255 (data_loss: 0.238 reg_loss: 0.017) ) lr: 0.044772684622464705\nepoch: 4800 acc: 0.905 loss: 0.250 (data_loss: 0.233 reg_loss: 0.017) ) lr: 0.04456054596979586\nepoch: 4900 acc: 0.903 loss: 0.250 (data_loss: 0.233 reg_loss: 0.017) ) lr: 0.04434497795025991\nepoch: 5000 acc: 0.900 loss: 0.250 (data_loss: 0.233 reg_loss: 0.017) ) lr: 0.04412604016342762\nepoch: 5100 acc: 0.904 loss: 0.249 (data_loss: 0.232 reg_loss: 0.017) ) lr: 0.04390379292587474\nepoch: 5200 acc: 0.904 loss: 0.249 (data_loss: 0.232 reg_loss: 0.017) ) lr: 0.043678297243576227\nepoch: 5300 acc: 0.905 loss: 0.247 (data_loss: 0.230 reg_loss: 0.017) ) lr: 0.04344961478409441\nepoch: 5400 acc: 0.901 loss: 0.254 (data_loss: 0.237 reg_loss: 0.017) ) lr: 0.04321780784857837\nepoch: 5500 acc: 0.900 loss: 0.259 (data_loss: 0.241 reg_loss: 0.017) ) lr: 0.042982939343593214\nepoch: 5600 acc: 0.904 loss: 0.247 (data_loss: 0.230 reg_loss: 0.017) ) lr: 0.042745072752796036\nepoch: 5700 acc: 0.906 loss: 0.247 (data_loss: 0.230 reg_loss: 0.017) ) lr: 0.04250427210847695\nepoch: 5800 acc: 0.905 loss: 0.246 (data_loss: 0.229 reg_loss: 0.017) ) lr: 0.04226060196298256\nepoch: 5900 acc: 0.905 loss: 0.246 (data_loss: 0.229 reg_loss: 0.017) ) lr: 0.04201412736003991\nepoch: 6000 acc: 0.905 loss: 0.246 (data_loss: 0.229 reg_loss: 0.017) ) lr: 0.041764913805998484\nepoch: 6100 acc: 0.905 loss: 0.246 (data_loss: 0.229 reg_loss: 0.017) ) lr: 0.041513027241007916\nepoch: 6200 acc: 0.905 loss: 0.246 (data_loss: 0.228 reg_loss: 0.017) ) lr: 0.04125853401014897\nepoch: 6300 acc: 0.904 loss: 0.246 (data_loss: 0.229 reg_loss: 0.017) ) lr: 0.04100150083453588\nepoch: 6400 acc: 0.906 loss: 0.245 (data_loss: 0.228 reg_loss: 0.017) ) lr: 0.040741994782406296\nepoch: 6500 acc: 0.901 loss: 0.248 (data_loss: 0.231 reg_loss: 0.017) ) lr: 0.040480083240217575\nepoch: 6600 acc: 0.902 loss: 0.245 (data_loss: 0.228 reg_loss: 0.017) ) lr: 0.04021583388376586\nepoch: 6700 acc: 0.901 loss: 0.247 (data_loss: 0.230 reg_loss: 0.017) ) lr: 0.03994931464934522\nepoch: 6800 acc: 0.895 loss: 0.259 (data_loss: 0.242 reg_loss: 0.017) ) lr: 0.03968059370496389\nepoch: 6900 acc: 0.903 loss: 0.247 (data_loss: 0.230 reg_loss: 0.017) ) lr: 0.03940973942163448\nepoch: 7000 acc: 0.901 loss: 0.248 (data_loss: 0.231 reg_loss: 0.017) ) lr: 0.03913682034475463\nepoch: 7100 acc: 0.903 loss: 0.246 (data_loss: 0.229 reg_loss: 0.017) ) lr: 0.03886190516559515\nepoch: 7200 acc: 0.903 loss: 0.246 (data_loss: 0.229 reg_loss: 0.017) ) lr: 0.03858506269291113\nepoch: 7300 acc: 0.902 loss: 0.244 (data_loss: 0.227 reg_loss: 0.017) ) lr: 0.03830636182469295\nepoch: 7400 acc: 0.902 loss: 0.246 (data_loss: 0.229 reg_loss: 0.017) ) lr: 0.03802587152007248\nepoch: 7500 acc: 0.903 loss: 0.246 (data_loss: 0.229 reg_loss: 0.017) ) lr: 0.03774366077140049\nepoch: 7600 acc: 0.905 loss: 0.242 (data_loss: 0.226 reg_loss: 0.017) ) lr: 0.037459798576510786\nepoch: 7700 acc: 0.905 loss: 0.245 (data_loss: 0.228 reg_loss: 0.017) ) lr: 0.03717435391118595\nepoch: 7800 acc: 0.905 loss: 0.241 (data_loss: 0.225 reg_loss: 0.017) ) lr: 0.03688739570184001\nepoch: 7900 acc: 0.905 loss: 0.241 (data_loss: 0.224 reg_loss: 0.017) ) lr: 0.0365989927984322\nepoch: 8000 acc: 0.903 loss: 0.250 (data_loss: 0.233 reg_loss: 0.017) ) lr: 0.03630921394762722\nepoch: 8100 acc: 0.904 loss: 0.244 (data_loss: 0.227 reg_loss: 0.017) ) lr: 0.0360181277662148\nepoch: 8200 acc: 0.904 loss: 0.242 (data_loss: 0.225 reg_loss: 0.017) ) lr: 0.03572580271480377\nepoch: 8300 acc: 0.906 loss: 0.241 (data_loss: 0.224 reg_loss: 0.017) ) lr: 0.035432307071803025\nepoch: 8400 acc: 0.906 loss: 0.241 (data_loss: 0.225 reg_loss: 0.017) ) lr: 0.03513770890770358\nepoch: 8500 acc: 0.906 loss: 0.239 (data_loss: 0.223 reg_loss: 0.017) ) lr: 0.034842076059673724\nepoch: 8600 acc: 0.900 loss: 0.253 (data_loss: 0.236 reg_loss: 0.017) ) lr: 0.034545476106480955\nepoch: 8700 acc: 0.902 loss: 0.241 (data_loss: 0.225 reg_loss: 0.016) ) lr: 0.03424797634375167\nepoch: 8800 acc: 0.906 loss: 0.239 (data_loss: 0.223 reg_loss: 0.016) ) lr: 0.03394964375958191\nepoch: 8900 acc: 0.907 loss: 0.239 (data_loss: 0.223 reg_loss: 0.016) ) lr: 0.0336505450105096\nepoch: 9000 acc: 0.908 loss: 0.238 (data_loss: 0.221 reg_loss: 0.016) ) lr: 0.03335074639786025\nepoch: 9100 acc: 0.909 loss: 0.237 (data_loss: 0.221 reg_loss: 0.016) ) lr: 0.033050313844476605\nepoch: 9200 acc: 0.907 loss: 0.237 (data_loss: 0.221 reg_loss: 0.016) ) lr: 0.03274931287184286\nepoch: 9300 acc: 0.909 loss: 0.237 (data_loss: 0.221 reg_loss: 0.016) ) lr: 0.03244780857761376\nepoch: 9400 acc: 0.909 loss: 0.237 (data_loss: 0.220 reg_loss: 0.016) ) lr: 0.03214586561355804\nepoch: 9500 acc: 0.907 loss: 0.237 (data_loss: 0.221 reg_loss: 0.016) ) lr: 0.03184354816392586\nepoch: 9600 acc: 0.906 loss: 0.238 (data_loss: 0.222 reg_loss: 0.016) ) lr: 0.03154091992424932\nepoch: 9700 acc: 0.909 loss: 0.236 (data_loss: 0.220 reg_loss: 0.016) ) lr: 0.031238044080584084\nepoch: 9800 acc: 0.908 loss: 0.236 (data_loss: 0.219 reg_loss: 0.016) ) lr: 0.03093498328920125\nepoch: 9900 acc: 0.910 loss: 0.235 (data_loss: 0.219 reg_loss: 0.016) ) lr: 0.030631799656736635\nepoch: 10000 acc: 0.906 loss: 0.237 (data_loss: 0.221 reg_loss: 0.016) ) lr: 0.030328554720805104\nvalidation, acc: 0.890, loss: 0.284\n"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Create dataset\n",
    "X, y = create_data(1000, 3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=1e-5, bias_regularizer_l2=1e-5)  # first dense layer, 2 inputs (each sample has 2 features), 3 outputs\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "#optimizer = Optimizer_SGD(decay=1e-8, momentum=0.9)\n",
    "#optimizer = Optimizer_Adagrad(decay=1e-8)\n",
    "#optimizer = Optimizer_RMSprop(learning_rate=0.05, decay=4e-8, rho=0.999)\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, decay=1e-8)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Make a forward pass of our training data thru this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Make a forward pass thru second Dense layer - it takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Calculate loss from output of activation2 so softmax activation\n",
    "    data_loss = loss_function.forward(activation2.output, y)\n",
    "\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = loss_function.regularization_loss(dense1) + loss_function.regularization_loss(dense2)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    predictions = np.argmax(activation2.output, axis=1)  # calculate values along first axis\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print('epoch:', epoch, 'acc:', f'{accuracy:.3f}', 'loss:', f'{loss:.3f}', '(data_loss:', f'{data_loss:.3f}', 'reg_loss:', f'{regularization_loss:.3f})', ')', 'lr:', optimizer.current_learning_rate)\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dvalues)\n",
    "    dense2.backward(activation2.dvalues)\n",
    "    activation1.backward(dense2.dvalues)\n",
    "    dense1.backward(activation1.dvalues)\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "# Validate model (we just do a forward pass)\n",
    "\n",
    "# Create test dataset\n",
    "X_test, y_test = create_data(100, 3)\n",
    "\n",
    "# Make a forward pass of our training data thru this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# Make a forward pass thru activation function - we take output of previous layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Make a forward pass thru second Dense layer - it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Make a forward pass thru activation function - we take output of previous layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Calculate loss from output of activation2 so softmax activation\n",
    "loss = loss_function.forward(activation2.output, y_test)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "predictions = np.argmax(activation2.output, axis=1)  # calculate values along first axis\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in theory, this regularization should also allow us to create much larger models without as much fear of overfitting\n",
    "- we can test this by increasing the number of neurons per layer to 512 (instead of our usual 64/layer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 0 acc: 0.380 loss: 1.099 (data_loss: 1.099 reg_loss: 0.000) ) lr: 0.05\nepoch: 100 acc: 0.802 loss: 0.507 (data_loss: 0.502 reg_loss: 0.005) ) lr: 0.04999752506207612\nepoch: 200 acc: 0.886 loss: 0.308 (data_loss: 0.299 reg_loss: 0.009) ) lr: 0.049990050996574775\nepoch: 300 acc: 0.907 loss: 0.258 (data_loss: 0.246 reg_loss: 0.011) ) lr: 0.04997758005043209\nepoch: 400 acc: 0.909 loss: 0.240 (data_loss: 0.228 reg_loss: 0.013) ) lr: 0.04996011596895705\nepoch: 500 acc: 0.905 loss: 0.249 (data_loss: 0.235 reg_loss: 0.013) ) lr: 0.04993766399395728\nepoch: 600 acc: 0.916 loss: 0.223 (data_loss: 0.209 reg_loss: 0.014) ) lr: 0.04991023086111661\nepoch: 700 acc: 0.919 loss: 0.218 (data_loss: 0.204 reg_loss: 0.014) ) lr: 0.049877824796627425\nepoch: 800 acc: 0.918 loss: 0.215 (data_loss: 0.200 reg_loss: 0.015) ) lr: 0.0498404555130797\nepoch: 900 acc: 0.920 loss: 0.226 (data_loss: 0.210 reg_loss: 0.016) ) lr: 0.04979813420460921\nepoch: 1000 acc: 0.915 loss: 0.219 (data_loss: 0.203 reg_loss: 0.017) ) lr: 0.04975087354130951\nepoch: 1100 acc: 0.922 loss: 0.211 (data_loss: 0.195 reg_loss: 0.017) ) lr: 0.04969868766290968\nepoch: 1200 acc: 0.921 loss: 0.210 (data_loss: 0.193 reg_loss: 0.017) ) lr: 0.04964159217172425\nepoch: 1300 acc: 0.922 loss: 0.209 (data_loss: 0.192 reg_loss: 0.017) ) lr: 0.04957960412487896\nepoch: 1400 acc: 0.921 loss: 0.209 (data_loss: 0.192 reg_loss: 0.017) ) lr: 0.04951274202581829\nepoch: 1500 acc: 0.921 loss: 0.209 (data_loss: 0.193 reg_loss: 0.017) ) lr: 0.049441025815100244\nepoch: 1600 acc: 0.910 loss: 0.240 (data_loss: 0.223 reg_loss: 0.017) ) lr: 0.049364476860485375\nepoch: 1700 acc: 0.920 loss: 0.216 (data_loss: 0.200 reg_loss: 0.017) ) lr: 0.04928311794632677\nepoch: 1800 acc: 0.925 loss: 0.203 (data_loss: 0.187 reg_loss: 0.017) ) lr: 0.04919697326226773\nepoch: 1900 acc: 0.925 loss: 0.202 (data_loss: 0.186 reg_loss: 0.016) ) lr: 0.04910606839125617\nepoch: 2000 acc: 0.922 loss: 0.205 (data_loss: 0.189 reg_loss: 0.017) ) lr: 0.049010430296883054\nepoch: 2100 acc: 0.926 loss: 0.201 (data_loss: 0.184 reg_loss: 0.016) ) lr: 0.048910087310054\nepoch: 2200 acc: 0.924 loss: 0.200 (data_loss: 0.184 reg_loss: 0.016) ) lr: 0.04880506911500336\nepoch: 2300 acc: 0.925 loss: 0.207 (data_loss: 0.191 reg_loss: 0.016) ) lr: 0.048695406734660295\nepoch: 2400 acc: 0.921 loss: 0.212 (data_loss: 0.196 reg_loss: 0.016) ) lr: 0.048581132515377004\nepoch: 2500 acc: 0.926 loss: 0.199 (data_loss: 0.183 reg_loss: 0.016) ) lr: 0.048462280111029786\nepoch: 2600 acc: 0.924 loss: 0.198 (data_loss: 0.182 reg_loss: 0.016) ) lr: 0.04833888446650344\nepoch: 2700 acc: 0.926 loss: 0.197 (data_loss: 0.181 reg_loss: 0.016) ) lr: 0.0482109818005708\nepoch: 2800 acc: 0.926 loss: 0.196 (data_loss: 0.181 reg_loss: 0.016) ) lr: 0.048078609588178944\nepoch: 2900 acc: 0.924 loss: 0.219 (data_loss: 0.203 reg_loss: 0.016) ) lr: 0.04794180654215414\nepoch: 3000 acc: 0.923 loss: 0.200 (data_loss: 0.184 reg_loss: 0.016) ) lr: 0.047800612594338286\nepoch: 3100 acc: 0.916 loss: 0.211 (data_loss: 0.195 reg_loss: 0.016) ) lr: 0.0476550688761695\nepoch: 3200 acc: 0.922 loss: 0.208 (data_loss: 0.193 reg_loss: 0.016) ) lr: 0.0475052176987199\nepoch: 3300 acc: 0.926 loss: 0.196 (data_loss: 0.180 reg_loss: 0.016) ) lr: 0.0473511025322044\nepoch: 3400 acc: 0.925 loss: 0.194 (data_loss: 0.179 reg_loss: 0.016) ) lr: 0.047192767984974744\nepoch: 3500 acc: 0.926 loss: 0.194 (data_loss: 0.178 reg_loss: 0.015) ) lr: 0.047030259782012335\nepoch: 3600 acc: 0.918 loss: 0.234 (data_loss: 0.219 reg_loss: 0.016) ) lr: 0.04686362474293419\nepoch: 3700 acc: 0.925 loss: 0.194 (data_loss: 0.179 reg_loss: 0.015) ) lr: 0.046692910759528077\nepoch: 3800 acc: 0.926 loss: 0.194 (data_loss: 0.178 reg_loss: 0.015) ) lr: 0.04651816677283049\nepoch: 3900 acc: 0.924 loss: 0.199 (data_loss: 0.184 reg_loss: 0.015) ) lr: 0.046339442749763586\nepoch: 4000 acc: 0.913 loss: 0.222 (data_loss: 0.207 reg_loss: 0.015) ) lr: 0.04615678965934673\nepoch: 4100 acc: 0.926 loss: 0.199 (data_loss: 0.184 reg_loss: 0.015) ) lr: 0.04597025944849779\nepoch: 4200 acc: 0.922 loss: 0.201 (data_loss: 0.186 reg_loss: 0.015) ) lr: 0.045779905017441204\nepoch: 4300 acc: 0.923 loss: 0.204 (data_loss: 0.189 reg_loss: 0.015) ) lr: 0.04558578019473819\nepoch: 4400 acc: 0.925 loss: 0.193 (data_loss: 0.178 reg_loss: 0.015) ) lr: 0.04538793971195641\nepoch: 4500 acc: 0.926 loss: 0.193 (data_loss: 0.179 reg_loss: 0.015) ) lr: 0.04518643917799511\nepoch: 4600 acc: 0.925 loss: 0.191 (data_loss: 0.177 reg_loss: 0.015) ) lr: 0.04498133505308287\nepoch: 4700 acc: 0.927 loss: 0.191 (data_loss: 0.176 reg_loss: 0.015) ) lr: 0.044772684622464705\nepoch: 4800 acc: 0.923 loss: 0.198 (data_loss: 0.183 reg_loss: 0.014) ) lr: 0.04456054596979586\nepoch: 4900 acc: 0.926 loss: 0.191 (data_loss: 0.177 reg_loss: 0.014) ) lr: 0.04434497795025991\nepoch: 5000 acc: 0.925 loss: 0.193 (data_loss: 0.178 reg_loss: 0.014) ) lr: 0.04412604016342762\nepoch: 5100 acc: 0.928 loss: 0.191 (data_loss: 0.176 reg_loss: 0.014) ) lr: 0.04390379292587474\nepoch: 5200 acc: 0.925 loss: 0.190 (data_loss: 0.175 reg_loss: 0.014) ) lr: 0.043678297243576227\nepoch: 5300 acc: 0.916 loss: 0.230 (data_loss: 0.215 reg_loss: 0.016) ) lr: 0.04344961478409441\nepoch: 5400 acc: 0.924 loss: 0.203 (data_loss: 0.187 reg_loss: 0.016) ) lr: 0.04321780784857837\nepoch: 5500 acc: 0.924 loss: 0.200 (data_loss: 0.184 reg_loss: 0.016) ) lr: 0.042982939343593214\nepoch: 5600 acc: 0.924 loss: 0.199 (data_loss: 0.183 reg_loss: 0.016) ) lr: 0.042745072752796036\nepoch: 5700 acc: 0.923 loss: 0.198 (data_loss: 0.182 reg_loss: 0.016) ) lr: 0.04250427210847695\nepoch: 5800 acc: 0.925 loss: 0.197 (data_loss: 0.181 reg_loss: 0.016) ) lr: 0.04226060196298256\nepoch: 5900 acc: 0.926 loss: 0.197 (data_loss: 0.181 reg_loss: 0.016) ) lr: 0.04201412736003991\nepoch: 6000 acc: 0.927 loss: 0.196 (data_loss: 0.180 reg_loss: 0.016) ) lr: 0.041764913805998484\nepoch: 6100 acc: 0.926 loss: 0.201 (data_loss: 0.185 reg_loss: 0.016) ) lr: 0.041513027241007916\nepoch: 6200 acc: 0.911 loss: 0.223 (data_loss: 0.208 reg_loss: 0.016) ) lr: 0.04125853401014897\nepoch: 6300 acc: 0.924 loss: 0.200 (data_loss: 0.185 reg_loss: 0.016) ) lr: 0.04100150083453588\nepoch: 6400 acc: 0.927 loss: 0.194 (data_loss: 0.178 reg_loss: 0.015) ) lr: 0.040741994782406296\nepoch: 6500 acc: 0.926 loss: 0.193 (data_loss: 0.178 reg_loss: 0.015) ) lr: 0.040480083240217575\nepoch: 6600 acc: 0.928 loss: 0.196 (data_loss: 0.181 reg_loss: 0.015) ) lr: 0.04021583388376586\nepoch: 6700 acc: 0.925 loss: 0.193 (data_loss: 0.177 reg_loss: 0.015) ) lr: 0.03994931464934522\nepoch: 6800 acc: 0.903 loss: 0.288 (data_loss: 0.273 reg_loss: 0.015) ) lr: 0.03968059370496389\nepoch: 6900 acc: 0.925 loss: 0.194 (data_loss: 0.179 reg_loss: 0.015) ) lr: 0.03940973942163448\nepoch: 7000 acc: 0.927 loss: 0.193 (data_loss: 0.178 reg_loss: 0.015) ) lr: 0.03913682034475463\nepoch: 7100 acc: 0.928 loss: 0.195 (data_loss: 0.179 reg_loss: 0.015) ) lr: 0.03886190516559515\nepoch: 7200 acc: 0.924 loss: 0.197 (data_loss: 0.182 reg_loss: 0.015) ) lr: 0.03858506269291113\nepoch: 7300 acc: 0.925 loss: 0.195 (data_loss: 0.180 reg_loss: 0.015) ) lr: 0.03830636182469295\nepoch: 7400 acc: 0.926 loss: 0.191 (data_loss: 0.176 reg_loss: 0.015) ) lr: 0.03802587152007248\nepoch: 7500 acc: 0.928 loss: 0.202 (data_loss: 0.188 reg_loss: 0.015) ) lr: 0.03774366077140049\nepoch: 7600 acc: 0.927 loss: 0.191 (data_loss: 0.176 reg_loss: 0.015) ) lr: 0.037459798576510786\nepoch: 7700 acc: 0.927 loss: 0.191 (data_loss: 0.176 reg_loss: 0.015) ) lr: 0.03717435391118595\nepoch: 7800 acc: 0.927 loss: 0.191 (data_loss: 0.176 reg_loss: 0.015) ) lr: 0.03688739570184001\nepoch: 7900 acc: 0.922 loss: 0.207 (data_loss: 0.193 reg_loss: 0.015) ) lr: 0.0365989927984322\nepoch: 8000 acc: 0.926 loss: 0.191 (data_loss: 0.176 reg_loss: 0.015) ) lr: 0.03630921394762722\nepoch: 8100 acc: 0.927 loss: 0.190 (data_loss: 0.175 reg_loss: 0.015) ) lr: 0.0360181277662148\nepoch: 8200 acc: 0.927 loss: 0.189 (data_loss: 0.175 reg_loss: 0.014) ) lr: 0.03572580271480377\nepoch: 8300 acc: 0.926 loss: 0.189 (data_loss: 0.175 reg_loss: 0.014) ) lr: 0.035432307071803025\nepoch: 8400 acc: 0.927 loss: 0.189 (data_loss: 0.175 reg_loss: 0.014) ) lr: 0.03513770890770358\nepoch: 8500 acc: 0.925 loss: 0.189 (data_loss: 0.174 reg_loss: 0.014) ) lr: 0.034842076059673724\nepoch: 8600 acc: 0.925 loss: 0.197 (data_loss: 0.183 reg_loss: 0.014) ) lr: 0.034545476106480955\nepoch: 8700 acc: 0.927 loss: 0.188 (data_loss: 0.174 reg_loss: 0.014) ) lr: 0.03424797634375167\nepoch: 8800 acc: 0.922 loss: 0.198 (data_loss: 0.184 reg_loss: 0.014) ) lr: 0.03394964375958191\nepoch: 8900 acc: 0.926 loss: 0.188 (data_loss: 0.174 reg_loss: 0.014) ) lr: 0.0336505450105096\nepoch: 9000 acc: 0.927 loss: 0.188 (data_loss: 0.174 reg_loss: 0.014) ) lr: 0.03335074639786025\nepoch: 9100 acc: 0.927 loss: 0.188 (data_loss: 0.174 reg_loss: 0.014) ) lr: 0.033050313844476605\nepoch: 9200 acc: 0.927 loss: 0.191 (data_loss: 0.177 reg_loss: 0.014) ) lr: 0.03274931287184286\nepoch: 9300 acc: 0.927 loss: 0.191 (data_loss: 0.177 reg_loss: 0.014) ) lr: 0.03244780857761376\nepoch: 9400 acc: 0.928 loss: 0.190 (data_loss: 0.176 reg_loss: 0.014) ) lr: 0.03214586561355804\nepoch: 9500 acc: 0.927 loss: 0.187 (data_loss: 0.173 reg_loss: 0.014) ) lr: 0.03184354816392586\nepoch: 9600 acc: 0.927 loss: 0.190 (data_loss: 0.176 reg_loss: 0.014) ) lr: 0.03154091992424932\nepoch: 9700 acc: 0.929 loss: 0.187 (data_loss: 0.173 reg_loss: 0.014) ) lr: 0.031238044080584084\nepoch: 9800 acc: 0.927 loss: 0.186 (data_loss: 0.173 reg_loss: 0.014) ) lr: 0.03093498328920125\nepoch: 9900 acc: 0.926 loss: 0.191 (data_loss: 0.177 reg_loss: 0.014) ) lr: 0.030631799656736635\nepoch: 10000 acc: 0.928 loss: 0.186 (data_loss: 0.173 reg_loss: 0.014) ) lr: 0.030328554720805104\nvalidation, acc: 0.920, loss: 0.326\n"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Create dataset\n",
    "X, y = create_data(1000, 3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 512, weight_regularizer_l2=1e-5, bias_regularizer_l2=1e-5)  # first dense layer, 2 inputs (each sample has 2 features), 3 outputs\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(512, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "#optimizer = Optimizer_SGD(decay=1e-8, momentum=0.9)\n",
    "#optimizer = Optimizer_Adagrad(decay=1e-8)\n",
    "#optimizer = Optimizer_RMSprop(learning_rate=0.05, decay=4e-8, rho=0.999)\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, decay=1e-8)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Make a forward pass of our training data thru this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Make a forward pass thru second Dense layer - it takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Calculate loss from output of activation2 so softmax activation\n",
    "    data_loss = loss_function.forward(activation2.output, y)\n",
    "\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = loss_function.regularization_loss(dense1) + loss_function.regularization_loss(dense2)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    predictions = np.argmax(activation2.output, axis=1)  # calculate values along first axis\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print('epoch:', epoch, 'acc:', f'{accuracy:.3f}', 'loss:', f'{loss:.3f}', '(data_loss:', f'{data_loss:.3f}', 'reg_loss:', f'{regularization_loss:.3f})', ')', 'lr:', optimizer.current_learning_rate)\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dvalues)\n",
    "    dense2.backward(activation2.dvalues)\n",
    "    activation1.backward(dense2.dvalues)\n",
    "    dense1.backward(activation1.dvalues)\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "# Validate model (we just do a forward pass)\n",
    "\n",
    "# Create test dataset\n",
    "X_test, y_test = create_data(100, 3)\n",
    "\n",
    "# Make a forward pass of our training data thru this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# Make a forward pass thru activation function - we take output of previous layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Make a forward pass thru second Dense layer - it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Make a forward pass thru activation function - we take output of previous layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Calculate loss from output of activation2 so softmax activation\n",
    "loss = loss_function.forward(activation2.output, y_test)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "predictions = np.argmax(activation2.output, axis=1)  # calculate values along first axis\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in this case, we see that the accuracies and losses for both in-sample and out-of-sample data are almost identical\n",
    "- fom here, we could try to add even more layers and/or neurons (feel free to tinker around)\n",
    "- next, we’re going to cover another regularization method: **dropout** regularization"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}