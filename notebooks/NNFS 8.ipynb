{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Gradients, Partial Derivatives, and the Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Derivatives\n",
    "- the derivatives we’ve solved thus far have been cases where there is only one independent variable ($x$)\n",
    "- our neural network's loss function, however, has many independent variables \n",
    "- hence, we will be using **Partial Derivatives**\n",
    "- the partial derivative measures how much impact a single variable has on a function’s output\n",
    "- the method for calculating a partial derivative is the same as earlier; we simply have to repeat this process for each of the independent variables\n",
    "---\n",
    "- here are some examples: \n",
    "- $f(x, y) = 2x + 3y^2 = 6y$\n",
    "- $f(x, y) = 3x^3 - y^2 + 5x +2 = -2y$\n",
    "- $f(x, y, z) = 3x^3z - y^2 + 5z +2yz = 3x^3 + 5 + 2y$\n",
    "- $f(x, y) = x ⋅ y = 1$ (input variables are multipled instead of summed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Function\n",
    "- the next type of derivative is the derivative of the `max()` function:\n",
    "- $f(x, y) = max(x, y) = 1(x >= y)$\n",
    "- the `max()` function returns whatever input is biggest\n",
    "- one special case for the derivative of the `max()` function is when we have only one variable parameter while the other parameter is always 0\n",
    "---\n",
    "- with that being said, this will be useful when we calculate the derivative of the ReLU activation function as said function is defined as:\n",
    "- $max(x, 0).f(x) = max(x, 0) = 1(x >= 0)$\n",
    "- in this case, the partial derivative is 1 when $x$ is greater than 0 (otherwise it's 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Gradient\n",
    "- when we solve these partial derivatives, we can build the **gradient**\n",
    "- the gradient is a vector comprised of the partial derivatives calculated with respect to each input variable\n",
    "---\n",
    "- let’s return to the partial derivatives of the sum operation that we calculated earlier:\n",
    "- $f(x, y, z) = 3x^3z - y^2 + 5z +2yz = 3x^3 + 5 + 2y$\n",
    "- we can now denote the gradient as: \n",
    "- $f(x, y, z) = [-9x^2z]$ (d/dx)\n",
    "- $f(x, y, z) = [-2y + 2z$] (d/dy)\n",
    "- $f(x, y, z) = [3x^3 + 5 + 2y]$ (d/dz)\n",
    "---\n",
    "- recall that these partial derivatives tell us the impact of each variable on a function\n",
    "- in our case, this is the impact of weights and biases on the loss function\n",
    "- **our goal is to decrease the loss**, so we need to raise the weights and biases that decrease the output of the loss function and lower the weights and biases that increase the output of the loss function\n",
    "- we now know that we can calculate each weight and bias’ impact on the loss function\n",
    "---\n",
    "- **the gradient points in the direction that maximizes the function the most so the opposite direction minimizes it the most**\n",
    "- we want to adjust the model parameters by small increments at a time because the impact of inputs in these functions will change non-linearly as we change their respective values\n",
    "---\n",
    "- the **learning rate** is a value that affects how quickly parameters in neural networks are adjusted\n",
    "---\n",
    "- we can solve the derivatives of more complex functions using the **chain rule**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}