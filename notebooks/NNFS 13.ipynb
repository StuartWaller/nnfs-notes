{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13: Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- because we are talking about datasets and testing, it’s worth mentioning the operations that we can perform on the training dataset\n",
    "- this is referred to as **preprocessing**\n",
    "- any preprocessing done to our training data must also be done to our validation and testing data as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: Scaling\n",
    "- **neural networks usually perform best on data consisting of numbers in ranges of (0 to 1) or (-1 to 1)**\n",
    "- this is true for a few reasons, but notably, many activation functions (softmax, sigmoid, tanh) behave properly within these described ranges\n",
    "---\n",
    "- another reason why this scaling is ideal is because neural networks reliance reply on many multiplication operations\n",
    "- within the range of (-1 to 1), the result becomes a fraction (a smaller value)\n",
    "- on the other hand, multiplying big numbers from our training data with weights can cause weights to grow too fast, which leads to severe instability\n",
    "- essentially, it's easier to control the training process with smaller numbers\n",
    "---\n",
    "- there are many operations related to data preprocessing: **standardization**, **scaling**, **variance scaling** **mean removal** (as mentioned above), **non-linear transformations**, **scaling to outliers**, etc\n",
    "- however, we're only going to scale data to a range by simply dividing all of the numbers by the maximum of their absolute values\n",
    "- for example, for an image consisting of numbers in a range of (0 to 255), we can divide the whole dataset by 255 to get data in range of (0 to 1)\n",
    "---\n",
    "- again, we must ensure the same scaling for all datasets (same scale parameters)\n",
    "- for example, we can find the maximum of the training data and divide the training, validation, and testing data by this value\n",
    "- in general, we should prepare a scaler and use its instance on all the datasets\n",
    "- once we train our model and we want to make predictions using new samples, we need to scale those new samples by the same scaler value that we applied to all our datasets\n",
    "- in most cases, we need to save the scaler value “next to” the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: Data Augmentation\n",
    "- when we do not have many training samples, we can use **data augmentation**\n",
    "- let's imagine that our goal is to detect rotten fruits; we are going to take a photo of an apple from different angles and predict whether or not it's rotten\n",
    "- let's assume that we are only able to aquire a limited number of photos\n",
    "- we can take the photos that we have, rotate and crop them, and save these \"new\" images as worthy training data\n",
    "- however, refrain from using a rotation technique when creating a model to detect road signs, for example, as road signs are not found rotated in a real-life scenarios"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}