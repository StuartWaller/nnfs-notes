{"cells":[{"cell_type":"markdown","metadata":{"id":"2FQQ_0ZFyJsB","colab_type":"text"},"source":["# Chapter 4: Activation Functions"]},{"cell_type":"markdown","metadata":{"id":"pBMRYFoXyKxw","colab_type":"text"},"source":["- in general, your neural network will have two types of activation functions:\n","- the first will be the activation function used in your hidden layers and the second will be the activation function in the output layer\n","- the purpose of this activation function is to mimic a neuron \"firing\" or not based on input information\n","- as discussed, the simplest version of this is the step function\n","- if the `weights * inputs + biases` result in a value greater than 0, the neuron will \"fire\" an output of 1, otherwise it will output a 0"]},{"cell_type":"markdown","metadata":{"id":"1lfnXhV4z6wq","colab_type":"text"},"source":["### Rectified Linear Activation Function (hidden layers)\n","- the step function is not ideal because we want something a bit more granular, with finer control over the outputs than simply \"1\" or \"0\"\n","- the original activation function for neural networks was the Sigmoid activation function, but this was eventually replaced by the *Rectified Linear Activation Function*\n","---\n","- the ReLU is very simple: it is quite literally y=x, clipped at 0\n","- if x is less than 0, y=0, otherwise y=x\n","- the ReLU outperforms the Sigmoid activation function and is the default/most used activation function in neural networks \n","- the ReLU has very straightforward code: "]},{"cell_type":"code","metadata":{"id":"knNMst951OAa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6e93b1fa-d53e-4f3d-ae0b-04ff606177c2","executionInfo":{"status":"ok","timestamp":1589239153056,"user_tz":420,"elapsed":790,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["# ReLU activation function\n","inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n","\n","output = []\n","for i in inputs:\n","    if i > 0:\n","        output.append(i)\n","    else:\n","        output.append(0)\n","\n","output"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"47syJP2K2dNS","colab_type":"text"},"source":["- there's actually a NumPy equivalent we can use called `np.maximum`\n","- we'll incorporate this in our rectified linear activation class:"]},{"cell_type":"code","metadata":{"id":"U3kD2N-S2sM1","colab_type":"code","colab":{}},"source":["class Activation_ReLU:\n","\n","    def forward(self, inputs):\n","        self.output = np.maximum(0, inputs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VQIMT6JX264N","colab_type":"text"},"source":["- let's apply this activation function to the code we have so far:"]},{"cell_type":"code","metadata":{"id":"UKePuJjw3sNn","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","np.random.seed(0)\n","\n","def create_data(points, classes):\n","    X = np.zeros((points*classes, 2))\n","    y = np.zeros(points*classes, dtype='uint8')\n","    for class_number in range(classes):\n","        ix = range(points*class_number, points*(class_number+1))\n","        r = np.linspace(0.0, 1, points)  # radius\n","        t = np.linspace(class_number*4, (class_number+1)*4, points) + np.random.randn(points)*0.05\n","        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n","        y[ix] = class_number\n","    return X, y\n","\n","class Layer_Dense:\n","\n","    def __init__(self, inputs, neurons):\n","        self.weights = 0.01 * np.random.randn(inputs, neurons)\n","        self.biases = np.zeros((1, neurons))\n","\n","    def forward(self, inputs):\n","        self.output = np.dot(inputs, self.weights) + self.biases\n","\n","class Activation_ReLU:\n","\n","    def forward(self, inputs):\n","        self.output = np.maximum(0, inputs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LV2DuFic4Ddx","colab_type":"text"},"source":["---"]},{"cell_type":"code","metadata":{"id":"hZ0iE8Fi4Tg4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"4cdcebb7-3b89-437d-ca19-4bfb9aea7dfe","executionInfo":{"status":"ok","timestamp":1589239153058,"user_tz":420,"elapsed":766,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["# Create dataset\n","X, y = create_data(100, 3)\n","\n","# Create Dense layer with 2 input features and 3 output values (3 neurons)\n","dense1 = Layer_Dense(2, 3)  # first dense layer, 2 inputs (each sample has 2 features), 3 outputs\n","\n","# Create ReLU activation (to be used with Dense layer):\n","activation1 = Activation_ReLU()\n","\n","# Make a forward pass of our training data through this layer\n","dense1.forward(X)\n","\n","# Fwd pass thru activation func. Takes in output from prev layer\n","activation1.forward(dense1.output)\n","\n","# Let's see output of few first samples:\n","print(activation1.output[:5])"],"execution_count":36,"outputs":[{"output_type":"stream","text":["[[0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [0.00000000e+00 9.17448014e-05 0.00000000e+00]\n"," [0.00000000e+00 2.34361157e-04 0.00000000e+00]\n"," [0.00000000e+00 4.45243326e-04 0.00000000e+00]\n"," [0.00000000e+00 6.15104009e-04 0.00000000e+00]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_8tyhct24oyi","colab_type":"text"},"source":["- as you can see, negative values have been **clipped** (modified to be 0)\n","- that's all there is to the ReLU\n","- the second type of activation function is for your output layer"]},{"cell_type":"markdown","metadata":{"id":"rM5jejDS48ft","colab_type":"text"},"source":["### Softmax Activation Function (output layer)\n","- in our case, we want our model to be a classifier, so we want an activation function designed for classification: let's use the *Softmax activation function*\n","- the softmax activation function on the output layer can take-in non-normalized inputs and compute a normalized distribution of probabilities for our classes\n","- in the case of classification, we want to see a prediction of which the class the network thinks the input represents\n","- this distribution returned by the softmax activation function represents **confidence** scores for each class, which will add up to 1\n","- the predicted class is associated with the output neuron with the largest confidence score\n","---\n","- let's break down the softmax activation function into simple pieces with Python code\n","- to start, here are some example outputs from a neural network layer: "]},{"cell_type":"code","metadata":{"id":"gNci3JnV_E5y","colab_type":"code","colab":{}},"source":["layer_outputs = [4.8, 1.21, 2.385]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U2rNy0fE_LKX","colab_type":"text"},"source":["- the first step is to \"exponentiate\" the outputs using Euler's number, *e*, which is roughly *2.71828182846* and referred to as the “exponential growth” number"]},{"cell_type":"code","metadata":{"id":"BO9VAiz8_cE8","colab_type":"code","colab":{}},"source":["# e - mathematical constant, we use E here to match a common coding  style where constants are uppercased\n","E = 2.71828182846  # you can also use math.e"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C8pbeerM_hCs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"dad3e6c9-c2a0-45c6-d6e4-478c47433eb7","executionInfo":{"status":"ok","timestamp":1589239153060,"user_tz":420,"elapsed":754,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["# For each value in a vector, calculate the exponential value\n","exp_values = []\n","for output in layer_outputs:\n","    exp_values.append(E ** output)  # ** - power operator in Python\n","print('exponentiated values:')\n","print(exp_values)"],"execution_count":39,"outputs":[{"output_type":"stream","text":["exponentiated values:\n","[121.51041751893969, 3.3534846525504487, 10.85906266492961]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FBz80oB0AVFi","colab_type":"text"},"source":["- exponentiation serves multiple purposes, but is mainly used for calculating a more meaningful loss\n","- once we've exponentiated, we want to convert these values to a probability distribution"]},{"cell_type":"code","metadata":{"id":"CsfmzJ3jAj6G","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"c6578359-033a-45a2-a2be-50ff884b82ce","executionInfo":{"status":"ok","timestamp":1589239153060,"user_tz":420,"elapsed":748,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["# Now normalize values\n","norm_base = sum(exp_values)  # We sum all values\n","norm_values = []\n","for value in exp_values:\n","    norm_values.append(value / norm_base) \n","print('normalized exponentiated values:')\n","print(norm_values)\n","\n","print('sum of normalized values:', sum(norm_values))"],"execution_count":40,"outputs":[{"output_type":"stream","text":["normalized exponentiated values:\n","[0.8952826639573506, 0.024708306782070668, 0.08000902926057876]\n","sum of normalized values: 1.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rffOLpXgAwBa","colab_type":"text"},"source":["- we can do everything we just did above with NumPy"]},{"cell_type":"code","metadata":{"id":"v--xv_MTAy1E","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"54c4ba05-56b8-47f9-c5b6-ff916f1ef4a9","executionInfo":{"status":"ok","timestamp":1589239153060,"user_tz":420,"elapsed":743,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["# softmax activation function\n","import numpy as np\n","\n","layer_outputs = [4.8, 1.21, 2.385]  # values we got as an output earlier when we described what neural network is\n","\n","# For each value in a vector, calculate the exponential value\n","exp_values = np.exp(layer_outputs)\n","print('exponentiated values:')\n","print(exp_values)\n","\n","# Now normalize values\n","norm_values = exp_values / np.sum(exp_values)\n","print('normalized exponentiated values:')\n","print(norm_values)\n","print('sum of normalized values:', np.sum(norm_values))"],"execution_count":41,"outputs":[{"output_type":"stream","text":["exponentiated values:\n","[121.51041752   3.35348465  10.85906266]\n","normalized exponentiated values:\n","[0.89528266 0.02470831 0.08000903]\n","sum of normalized values: 0.9999999999999999\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xrAo8nj6ByFy","colab_type":"text"},"source":["- along with dead neurons (whose values become 0), another common issue is values becoming astronomically large (there's a lot of multiplication and exponentiation being applied to values) "]},{"cell_type":"code","metadata":{"id":"tzmI-p3fCJAb","colab_type":"code","colab":{}},"source":["# get unnormalized probabilities\n","exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n","# normalize them for each sample\n","probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rHaj-iSMDnmZ","colab_type":"text"},"source":["- here, we have some new functions and a newly-added process for subtracting the max value from the inputs\n","- `np.exp()` does the exponentiation part\n","- in the case of a 2D array/matrix, `axis=0` refers to columns, and `axis=1` refers to rows\n","- first, let's see the *default*, which is `None`"]},{"cell_type":"code","metadata":{"id":"KAhJdBM5ERDa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"90b12480-e3a8-4581-8a32-1df05b9f7b89","executionInfo":{"status":"ok","timestamp":1589239733918,"user_tz":420,"elapsed":2242,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["layer_outputs = np.array([[4.8, 1.21, 2.385],\n","                          [8.9, -1.81, 0.2],\n","                          [1.41, 1.051, 0.026]])\n","\n","print(\"sum without axis\")\n","print(np.sum(layer_outputs))"],"execution_count":45,"outputs":[{"output_type":"stream","text":["sum without axis\n","18.172\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6pytS1bYEiXD","colab_type":"text"},"source":["- with no axis specified, we are just summing *all* the values\n","- next, we will sum the columns:"]},{"cell_type":"code","metadata":{"id":"50rg6ZWkEvv3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"881279e3-a89b-45d4-a1e3-8c96643db5c2","executionInfo":{"status":"ok","timestamp":1589239812381,"user_tz":420,"elapsed":359,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["print(\"another way to think of it w/ a matrix == axis 0: columns:\")\n","print(np.sum(layer_outputs, axis=0))"],"execution_count":46,"outputs":[{"output_type":"stream","text":["another way to think of it w/ a matrix == axis 0: columns:\n","[15.11   0.451  2.611]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8rkLcagKFqIi","colab_type":"text"},"source":["- next, we will sum the rows: "]},{"cell_type":"code","metadata":{"id":"U9844yZKFsGS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"3679d01a-eaa4-48cf-ca93-c90132ed9467","executionInfo":{"status":"ok","timestamp":1589240067284,"user_tz":420,"elapsed":461,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["print(\"so we can sum axis 1, but note the current shape:\")\n","print(np.sum(layer_outputs, axis=1))"],"execution_count":49,"outputs":[{"output_type":"stream","text":["so we can sum axis 1, but note the current shape:\n","[8.395 7.29  2.487]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tr8Eq3GxFzi8","colab_type":"text"},"source":["- we got the outputs we wanted and expected, but we need to simplify the outputs to a single value per sample\n","- we're trying to sum all the outputs from a layer for each sample in a batch\n","- we can accomplish this by using `keepdims=True`"]},{"cell_type":"code","metadata":{"id":"U4HbKPmyGKnX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"a7430c74-ac99-40d8-b2d9-f2dff6f6f8f3","executionInfo":{"status":"ok","timestamp":1589240181536,"user_tz":420,"elapsed":407,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["print(\"sum axis 1, but keep the same dimensions as input:\")\n","print(np.sum(layer_outputs, axis=1, keepdims=True))"],"execution_count":50,"outputs":[{"output_type":"stream","text":["sum axis 1, but keep the same dimensions as input:\n","[[8.395]\n"," [7.29 ]\n"," [2.487]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iLMyrEySGYXK","colab_type":"text"},"source":["- now let's combine all of this into a softmax class: "]},{"cell_type":"code","metadata":{"id":"P-ZAABUEGcns","colab_type":"code","colab":{}},"source":["# Softmax activation\n","class Activation_Softmax:\n","\n","    # Forward pass\n","    def forward(self, inputs):\n","\n","        # get unnormalized probabilities\n","        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n","        # normalize them for each sample\n","        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n","\n","        self.output = probabilities"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8nQMBTSQGtDk","colab_type":"text"},"source":["- finally, in `exp_values` we included the subtraction of the largest of the inputs before we did the exponentiation to combat the \"exploding\" values problem\n","- here is an example of how easily values can become large:"]},{"cell_type":"code","metadata":{"id":"JpR-njMGHSQl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"f3275525-f43f-437b-dea1-753494d3c69c","executionInfo":{"status":"ok","timestamp":1589240483374,"user_tz":420,"elapsed":393,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["np.exp(1)"],"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.718281828459045"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"code","metadata":{"id":"Xcje8uQzHWuQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"604c38fb-388b-4df7-b8a3-aa7ccbadd7b5","executionInfo":{"status":"ok","timestamp":1589240494282,"user_tz":420,"elapsed":436,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["np.exp(10)"],"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["22026.465794806718"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"LP6bDw_RHall","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8f4b6dbd-80dc-4af2-bc0d-a44e3ded8aeb","executionInfo":{"status":"ok","timestamp":1589240531760,"user_tz":420,"elapsed":359,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["np.exp(100)"],"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.6881171418161356e+43"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"markdown","metadata":{"id":"D71284FYHreZ","colab_type":"text"},"source":["- what we know about this exponential function is that its output value tends to 0 as the input value decreases to negative infinity, and the output value is 1 when the input is 0"]},{"cell_type":"code","metadata":{"id":"wRg5YC-HH373","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"de0c4499-b4b9-4f6c-cf72-bcd94e06ae8d","executionInfo":{"status":"ok","timestamp":1589240628508,"user_tz":420,"elapsed":431,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["np.exp(-np.inf), np.exp(0)"],"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.0, 1.0)"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"markdown","metadata":{"id":"C4AyIzGgIKOE","colab_type":"text"},"source":["- now let's use our softmax activation function and see our neural network thus far: "]},{"cell_type":"code","metadata":{"id":"0ofcax2kIQHP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"5403fb25-5986-46c4-bac9-ef15e6e8bcc5","executionInfo":{"status":"ok","timestamp":1589240816161,"user_tz":420,"elapsed":415,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["# Create dataset\n","X, y = create_data(100, 3)\n","\n","# Create Dense layer with 2 input features and 3 output values\n","dense1 = Layer_Dense(2, 3)  # first dense layer, 2 inputs (each sample has 2 features), 3 outputs\n","\n","# Create ReLU activation (to be used with Dense layer):\n","activation1 = Activation_ReLU()\n","\n","# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values (output values)\n","dense2 = Layer_Dense(3, 3)  # second dense layer, 3 inputs, 3 outputs\n","\n","# Create Softmax activation (to be used with Dense layer):\n","activation2 = Activation_Softmax()\n","\n","#===========================================================================#\n","\n","# Make a forward pass of our training data through this layer\n","dense1.forward(X)\n","\n","# Make a forward pass through activation function - we take output of previous layer here\n","activation1.forward(dense1.output)\n","\n","# Make a forward pass through second Dense layer - it takes outputs of activation function of first layer as inputs\n","dense2.forward(activation1.output)\n","\n","# Make a forward pass through activation function - we take output of previous layer here\n","activation2.forward(dense2.output)\n","\n","# Let's see output of few first samples:\n","print(activation2.output[:5])"],"execution_count":61,"outputs":[{"output_type":"stream","text":["[[0.33333333 0.33333333 0.33333333]\n"," [0.33333333 0.33333333 0.33333333]\n"," [0.33333333 0.33333333 0.33333333]\n"," [0.33333333 0.33333333 0.33333333]\n"," [0.33333333 0.33333333 0.33333333]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dKV_3QKKJIFZ","colab_type":"text"},"source":["- in order to determine which classification the model has chosen to be the prediction, we usually perform an `argmax`, which simply checks which of the classes in the output distribution have the highest confidence"]},{"cell_type":"markdown","metadata":{"id":"lzeJEUHKJokH","colab_type":"text"},"source":["### Full Code"]},{"cell_type":"code","metadata":{"id":"tF1Dd4u-JkcC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"a2898dbd-ead9-4a77-df65-dcfa0098538f","executionInfo":{"status":"ok","timestamp":1589241093930,"user_tz":420,"elapsed":760,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["import numpy as np\n","\n","\n","np.random.seed(0)\n","\n","\n","# Our sample dataset\n","def create_data(n, k):\n","    X = np.zeros((n*k, 2))  # data matrix (each row = single example)\n","    y = np.zeros(n*k, dtype='uint8')  # class labels\n","    for j in range(k):\n","        ix = range(n*j, n*(j+1))\n","        r = np.linspace(0.0, 1, n)  # radius\n","        t = np.linspace(j*4, (j+1)*4, n) + np.random.randn(n)*0.2  # theta\n","        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n","        y[ix] = j\n","    return X, y\n","\n","\n","# Dense layer\n","class Layer_Dense:\n","\n","    # Layer initialization\n","    def __init__(self, inputs, neurons):\n","        # Initialize weights and biases\n","        self.weights = 0.01 * np.random.randn(inputs, neurons)\n","        self.biases = np.zeros((1, neurons))\n","\n","    # Forward pass\n","    def forward(self, inputs):\n","        # Calculate output values from input ones, weights and biases\n","        self.output = np.dot(inputs, self.weights) + self.biases\n","\n","\n","# ReLU activation\n","class Activation_ReLU:\n","\n","    # Forward pass\n","    def forward(self, inputs):\n","        self.output = np.maximum(0, inputs)\n","\n","\n","# Softmax activation\n","class Activation_Softmax:\n","\n","    # Forward pass\n","    def forward(self, inputs):\n","\n","        # get unnormalized probabilities\n","        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n","        # normalize them for each sample\n","        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n","\n","        self.output = probabilities\n","\n","\n","# Create dataset\n","X, y = create_data(100, 3)\n","\n","# Create Dense layer with 2 input features and 3 output values\n","dense1 = Layer_Dense(2, 3)  # first dense layer, 2 inputs (each sample has 2 features), 3 outputs\n","\n","# Create ReLU activation (to be used with Dense layer):\n","activation1 = Activation_ReLU()\n","\n","# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values (output values)\n","dense2 = Layer_Dense(3, 3)  # second dense layer, 3 inputs, 3 outputs\n","\n","# Create Softmax activation (to be used with Dense layer):\n","activation2 = Activation_Softmax()\n","\n","# Make a forward pass of our training data thru this layer\n","dense1.forward(X)\n","\n","# Make a forward pass thru activation function - we take output of previous layer here\n","activation1.forward(dense1.output)\n","\n","# Make a forward pass thru second Dense layer - it takes outputs of activation function of first layer as inputs\n","dense2.forward(activation1.output)\n","\n","# Make a forward pass thru activation function - we take output of previous layer here\n","activation2.forward(dense2.output)\n","\n","# Let's see output of few first samples:\n","print(activation2.output[:5])"],"execution_count":62,"outputs":[{"output_type":"stream","text":["[[0.33333333 0.33333333 0.33333333]\n"," [0.33333317 0.33333318 0.33333364]\n"," [0.33333289 0.33333292 0.3333342 ]\n"," [0.33333259 0.33333264 0.33333477]\n"," [0.33333233 0.33333239 0.33333528]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"up8dRgGQJzZS","colab_type":"text"},"source":["- at this point, we've completed what we need for forward-passing data through our model\n","- we used the rectified linear activation function (ReLU) for the hidden layer, which works on a pre-neuron basis\n","- additionally, we used the softmax activation function for the output layer because it accepts non-normalized values as inputs and outputs a probability distribution, which we'll use as confidence scores for each class\n","---\n","- next, we need to calculate how wrong the neural network is and begin adjusting weights and biases to decrease error over time\n","- thus, our next step is to quantify how wrong the model is through a *loss function*"]}],"metadata":{"colab":{"name":"NNFS Chapter 4.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMahOlmJkghyW9t24H2/yiN"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}