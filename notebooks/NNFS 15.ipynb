{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15: Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- another regularization technique for neural networks is **dropout** regularization\n",
    "- a dropout layer disables some neurons to prevent a neural network from becoming too dependent on any specific neuron\n",
    "- dropout also helps alleviate **co-adoption**, which occurs when neurons become too dependent on the output values of other neurons and fail to learn the underlying function on their own \n",
    "- dropout can also help with **noise** and other perturbations in the training data\n",
    "---\n",
    "- with a dropout layer, neurons are disabled randomly at a given rate during every forward pass, forcing the network to learn how to make accurate predictions with only the random selection of remaining neurons\n",
    "- to make an important clarification, the dropout layer does not truly disable neurons, but instead zeroes their outputs\n",
    "- in other words, dropout does not actually decrease the number of neurons used, nor does it speed up the training time\n",
    "---\n",
    "- in code, we'll “turn off” neurons with a filter that is an array with the same shape as the weight array, but filled with numbers from a **Bernoulli distribution** \n",
    "- a Bernoulli distribution is a binary (also called discrete) probability distribution where we can get a value of 1 with probability of $p$ and value of 0 with probability of $q$\n",
    "---\n",
    "- we'll have one hyperparameter for a dropout layer that dictates the percentage of neurons to disable in that layer (0.10 = 10% of the neurons will randomly be disabled during each forward pass) \n",
    "- before we use NumPy, we’ll show a raw Python example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0, -1.03, 0.67, 0, 0, 0, -2.01, 0, -0.07, 0.73]"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "dropout_rate = 0.5\n",
    "example_output = [0.27, -1.03, 0.67, 0.99, 0.05, -0.37, -2.01, 1.13, -0.07, 0.73]  # example output containing 10 values\n",
    "\n",
    "# Repeat as long as necessary\n",
    "while True:\n",
    "    \n",
    "    # Randomly choose index and set value to 0\n",
    "    index = random.randint(0, len(example_output) - 1)\n",
    "    example_output[index] = 0\n",
    "    \n",
    "    # We might set an index that already is zeroed\n",
    "    # There are different ways of overcoming this problem, for simplicity we count values that are exactly 0\n",
    "    # as it's extremely rare in real model that weights are exactly 0, it's not the best method for sure\n",
    "    dropped_out = 0\n",
    "    for value in example_output:\n",
    "        if value == 0:\n",
    "            dropped_out += 1\n",
    "    \n",
    "    # If required number of outputs is zeroed - leave a loop\n",
    "    if dropped_out / len(example_output) >= dropout_rate:\n",
    "        break\n",
    "\n",
    "example_output # outputs vary due to randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the idea is to just keep disabling neuron outputs (setting them to 0) at random until we’ve disabled our target % of neurons \n",
    "- a Bernoulli distribution is a special variation of a **binomial distribution** with $n=1$ \n",
    "- therefore, we can use `np.random.binomial()` \n",
    "- a binomial distribution differs from a Bernoulli distribution as it adds a parameter, $n$, which dictates the number of concurrent experiments (instead of just one) and returns the number of successes from these $n$ experiments\n",
    "- `np.random.binomial()` has three parameters: $n$ (number of experiments), $p$ (probability of true value of experiment), and parameter size (`np.random.binomial(n, p, size)`)\n",
    "---\n",
    "- think of this function like a coin toss, where the result will either be 0 or 1\n",
    "- the $n$ is how many tosses of the coin do you want to perform\n",
    "- the $p$ is the probability for the toss to result in a 1 and the overall result is the sum of all the results\n",
    "- the $size$ is our desired amount of “tests” we want to perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0, 1, 0, 1, 0, 1, 1, 2, 2, 1])"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "np.random.binomial(2, 0.5, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the code above will produce an array that is of size 10, where each element will be the sum of 2 coin tosses, where the probability of 1 will be 0.5, or 50%\n",
    "---\n",
    "- we can use this code to create our dropout layer\n",
    "- our goal is to create a filter where the intended dropout % is represented as 0 and otherwise represented as 1\n",
    "- suppose we have a dropout layer of 5 neurons that we’ll add after a layer and we wish to have a 20% dropout\n",
    "- an example of a dropout layer might look like: `[1, 0, 1, 1, 1]` (as you can see, 20% of that list is 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 1, 1, 1, 0])"
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "np.random.binomial(1, 0.8, size=5) # my own attempt to yield the array above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 1, 1, 1, 0])"
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "dropout_rate = 0.20 # actual way (same thing, but more explanatory)\n",
    "np.random.binomial(1, 1-dropout_rate, size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- on a realistically sized layer, you will find that the probability more consistently matches your intended value\n",
    "- assume a neural network layer’s output is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_output = np.array([0.27, -1.03, 0.67, 0.99, 0.05, -0.37, -2.01, 1.13, -0.07, 0.73])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- next, let’s assume our target dropout rate is 0.3, or 30%\n",
    "- we apply a dropout layer like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ 0.   -1.03  0.    0.    0.05 -0.37 -2.01  0.   -0.07  0.  ]\n"
    }
   ],
   "source": [
    "dropout_rate = 0.3\n",
    "\n",
    "example_output = np.array([0.27, -1.03, 0.67, 0.99, 0.05, -0.37, -2.01, 1.13, -0.07, 0.73]) \n",
    "example_output *= np.random.binomial(1, 1-dropout_rate, example_output.shape)\n",
    "\n",
    "print(example_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- note that our dropout rate is the amount of neurons we intend to disable ($q$), but sometimes the implementation of dropout will include a rate parameter that instead dictates the amount of neurons you intend to keep ($p$) \n",
    "---\n",
    "- while dropout helps a neural network generalize and is helpful for training, it’s not something to utilize when predicting\n",
    "---\n",
    "- in any specific example, you will find that scaling doesn’t equal the exact same sum as before because we’re randomly dropping neurons, but after enough samples, the scaling will eventually average out\n",
    "- let's see a supporting example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "sum initial 0.36000000000000015\nmean sum: 0.37619250000000015\n"
    }
   ],
   "source": [
    "dropout_rate = 0.2\n",
    "example_output = np.array([0.27, -1.03, 0.67, 0.99, 0.05, -0.37, -2.01, 1.13, -0.07, 0.73]) \n",
    "print(f\"sum initial {sum(example_output)}\")\n",
    "\n",
    "sums = []\n",
    "for i in range(10000):\n",
    "\n",
    "    example_output2 = example_output * np.random.binomial(1, 1-dropout_rate, example_output.shape) / (1-dropout_rate)\n",
    "    sums.append(sum(example_output2))\n",
    "    \n",
    "print(f\"mean sum: {np.mean(sums)}\") \n",
    "# not exact, but you should get the idea (\"the scaling will eventually average out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- our final task is to implement a `backward()` method \n",
    "- like before, we need to calculate the partial derivative of the dropout operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "class Layer_Dropout:\n",
    "\n",
    "    # Init\n",
    "    def __init__(self, rate):\n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, values):\n",
    "        # Save input values\n",
    "        self.input = values\n",
    "       \n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=values.shape) / self.rate\n",
    "        # Apply mask to output values\n",
    "        self.output = values * self.binary_mask\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradient on values\n",
    "        self.dvalues = dvalues * self.binary_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let’s take this new dropout layer and add it between our two dense layers (first defining it)\n",
    "- add dropout regularization in the forward and backward passes\n",
    "- let's take a look out our final code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 0 acc: 0.324 loss: 1.099 (data_loss: 1.099 reg_loss: 0.000) ) lr: 0.05\nepoch: 100 acc: 0.580 loss: 0.928 (data_loss: 0.885 reg_loss: 0.043) ) lr: 0.04999752506207612\nepoch: 200 acc: 0.627 loss: 0.883 (data_loss: 0.831 reg_loss: 0.052) ) lr: 0.049990050996574775\nepoch: 300 acc: 0.670 loss: 0.826 (data_loss: 0.767 reg_loss: 0.059) ) lr: 0.04997758005043209\nepoch: 400 acc: 0.665 loss: 0.808 (data_loss: 0.752 reg_loss: 0.056) ) lr: 0.04996011596895705\nepoch: 500 acc: 0.672 loss: 0.780 (data_loss: 0.723 reg_loss: 0.057) ) lr: 0.04993766399395728\nepoch: 600 acc: 0.692 loss: 0.785 (data_loss: 0.730 reg_loss: 0.056) ) lr: 0.04991023086111661\nepoch: 700 acc: 0.684 loss: 0.779 (data_loss: 0.726 reg_loss: 0.053) ) lr: 0.049877824796627425\nepoch: 800 acc: 0.687 loss: 0.764 (data_loss: 0.712 reg_loss: 0.052) ) lr: 0.0498404555130797\nepoch: 900 acc: 0.690 loss: 0.769 (data_loss: 0.719 reg_loss: 0.050) ) lr: 0.04979813420460921\nepoch: 1000 acc: 0.695 loss: 0.751 (data_loss: 0.701 reg_loss: 0.050) ) lr: 0.04975087354130951\nepoch: 1100 acc: 0.698 loss: 0.765 (data_loss: 0.716 reg_loss: 0.049) ) lr: 0.04969868766290968\nepoch: 1200 acc: 0.687 loss: 0.730 (data_loss: 0.682 reg_loss: 0.049) ) lr: 0.04964159217172425\nepoch: 1300 acc: 0.690 loss: 0.739 (data_loss: 0.691 reg_loss: 0.047) ) lr: 0.04957960412487896\nepoch: 1400 acc: 0.705 loss: 0.730 (data_loss: 0.683 reg_loss: 0.047) ) lr: 0.04951274202581829\nepoch: 1500 acc: 0.692 loss: 0.741 (data_loss: 0.694 reg_loss: 0.047) ) lr: 0.049441025815100244\nepoch: 1600 acc: 0.690 loss: 0.751 (data_loss: 0.703 reg_loss: 0.048) ) lr: 0.049364476860485375\nepoch: 1700 acc: 0.713 loss: 0.737 (data_loss: 0.690 reg_loss: 0.047) ) lr: 0.04928311794632677\nepoch: 1800 acc: 0.707 loss: 0.743 (data_loss: 0.697 reg_loss: 0.046) ) lr: 0.04919697326226773\nepoch: 1900 acc: 0.715 loss: 0.733 (data_loss: 0.688 reg_loss: 0.045) ) lr: 0.04910606839125617\nepoch: 2000 acc: 0.711 loss: 0.737 (data_loss: 0.693 reg_loss: 0.044) ) lr: 0.049010430296883054\nepoch: 2100 acc: 0.696 loss: 0.746 (data_loss: 0.703 reg_loss: 0.043) ) lr: 0.048910087310054\nepoch: 2200 acc: 0.703 loss: 0.727 (data_loss: 0.683 reg_loss: 0.044) ) lr: 0.04880506911500336\nepoch: 2300 acc: 0.719 loss: 0.730 (data_loss: 0.688 reg_loss: 0.043) ) lr: 0.048695406734660295\nepoch: 2400 acc: 0.709 loss: 0.737 (data_loss: 0.695 reg_loss: 0.043) ) lr: 0.048581132515377004\nepoch: 2500 acc: 0.723 loss: 0.709 (data_loss: 0.666 reg_loss: 0.042) ) lr: 0.048462280111029786\nepoch: 2600 acc: 0.700 loss: 0.748 (data_loss: 0.706 reg_loss: 0.042) ) lr: 0.04833888446650344\nepoch: 2700 acc: 0.711 loss: 0.730 (data_loss: 0.689 reg_loss: 0.041) ) lr: 0.0482109818005708\nepoch: 2800 acc: 0.717 loss: 0.742 (data_loss: 0.702 reg_loss: 0.040) ) lr: 0.048078609588178944\nepoch: 2900 acc: 0.719 loss: 0.722 (data_loss: 0.682 reg_loss: 0.040) ) lr: 0.04794180654215414\nepoch: 3000 acc: 0.719 loss: 0.712 (data_loss: 0.673 reg_loss: 0.039) ) lr: 0.047800612594338286\nepoch: 3100 acc: 0.712 loss: 0.725 (data_loss: 0.685 reg_loss: 0.040) ) lr: 0.0476550688761695\nepoch: 3200 acc: 0.712 loss: 0.706 (data_loss: 0.666 reg_loss: 0.039) ) lr: 0.0475052176987199\nepoch: 3300 acc: 0.700 loss: 0.733 (data_loss: 0.695 reg_loss: 0.038) ) lr: 0.0473511025322044\nepoch: 3400 acc: 0.703 loss: 0.709 (data_loss: 0.671 reg_loss: 0.038) ) lr: 0.047192767984974744\nepoch: 3500 acc: 0.701 loss: 0.723 (data_loss: 0.686 reg_loss: 0.037) ) lr: 0.047030259782012335\nepoch: 3600 acc: 0.700 loss: 0.720 (data_loss: 0.683 reg_loss: 0.037) ) lr: 0.04686362474293419\nepoch: 3700 acc: 0.705 loss: 0.760 (data_loss: 0.723 reg_loss: 0.037) ) lr: 0.046692910759528077\nepoch: 3800 acc: 0.711 loss: 0.701 (data_loss: 0.664 reg_loss: 0.037) ) lr: 0.04651816677283049\nepoch: 3900 acc: 0.720 loss: 0.702 (data_loss: 0.666 reg_loss: 0.036) ) lr: 0.046339442749763586\nepoch: 4000 acc: 0.699 loss: 0.732 (data_loss: 0.697 reg_loss: 0.036) ) lr: 0.04615678965934673\nepoch: 4100 acc: 0.685 loss: 0.743 (data_loss: 0.707 reg_loss: 0.036) ) lr: 0.04597025944849779\nepoch: 4200 acc: 0.706 loss: 0.715 (data_loss: 0.679 reg_loss: 0.036) ) lr: 0.045779905017441204\nepoch: 4300 acc: 0.717 loss: 0.718 (data_loss: 0.683 reg_loss: 0.035) ) lr: 0.04558578019473819\nepoch: 4400 acc: 0.712 loss: 0.708 (data_loss: 0.673 reg_loss: 0.036) ) lr: 0.04538793971195641\nepoch: 4500 acc: 0.722 loss: 0.722 (data_loss: 0.687 reg_loss: 0.035) ) lr: 0.04518643917799511\nepoch: 4600 acc: 0.724 loss: 0.710 (data_loss: 0.676 reg_loss: 0.035) ) lr: 0.04498133505308287\nepoch: 4700 acc: 0.716 loss: 0.717 (data_loss: 0.682 reg_loss: 0.035) ) lr: 0.044772684622464705\nepoch: 4800 acc: 0.717 loss: 0.730 (data_loss: 0.695 reg_loss: 0.034) ) lr: 0.04456054596979586\nepoch: 4900 acc: 0.715 loss: 0.709 (data_loss: 0.674 reg_loss: 0.035) ) lr: 0.04434497795025991\nepoch: 5000 acc: 0.717 loss: 0.706 (data_loss: 0.672 reg_loss: 0.035) ) lr: 0.04412604016342762\nepoch: 5100 acc: 0.729 loss: 0.712 (data_loss: 0.678 reg_loss: 0.034) ) lr: 0.04390379292587474\nepoch: 5200 acc: 0.713 loss: 0.713 (data_loss: 0.680 reg_loss: 0.034) ) lr: 0.043678297243576227\nepoch: 5300 acc: 0.721 loss: 0.697 (data_loss: 0.663 reg_loss: 0.034) ) lr: 0.04344961478409441\nepoch: 5400 acc: 0.725 loss: 0.674 (data_loss: 0.641 reg_loss: 0.033) ) lr: 0.04321780784857837\nepoch: 5500 acc: 0.729 loss: 0.713 (data_loss: 0.679 reg_loss: 0.033) ) lr: 0.042982939343593214\nepoch: 5600 acc: 0.706 loss: 0.719 (data_loss: 0.686 reg_loss: 0.033) ) lr: 0.042745072752796036\nepoch: 5700 acc: 0.716 loss: 0.686 (data_loss: 0.653 reg_loss: 0.033) ) lr: 0.04250427210847695\nepoch: 5800 acc: 0.712 loss: 0.702 (data_loss: 0.670 reg_loss: 0.032) ) lr: 0.04226060196298256\nepoch: 5900 acc: 0.705 loss: 0.726 (data_loss: 0.693 reg_loss: 0.032) ) lr: 0.04201412736003991\nepoch: 6000 acc: 0.706 loss: 0.729 (data_loss: 0.696 reg_loss: 0.033) ) lr: 0.041764913805998484\nepoch: 6100 acc: 0.716 loss: 0.710 (data_loss: 0.677 reg_loss: 0.032) ) lr: 0.041513027241007916\nepoch: 6200 acc: 0.721 loss: 0.697 (data_loss: 0.665 reg_loss: 0.032) ) lr: 0.04125853401014897\nepoch: 6300 acc: 0.724 loss: 0.721 (data_loss: 0.689 reg_loss: 0.032) ) lr: 0.04100150083453588\nepoch: 6400 acc: 0.699 loss: 0.728 (data_loss: 0.696 reg_loss: 0.032) ) lr: 0.040741994782406296\nepoch: 6500 acc: 0.711 loss: 0.709 (data_loss: 0.678 reg_loss: 0.031) ) lr: 0.040480083240217575\nepoch: 6600 acc: 0.701 loss: 0.705 (data_loss: 0.673 reg_loss: 0.031) ) lr: 0.04021583388376586\nepoch: 6700 acc: 0.716 loss: 0.731 (data_loss: 0.700 reg_loss: 0.031) ) lr: 0.03994931464934522\nepoch: 6800 acc: 0.725 loss: 0.714 (data_loss: 0.683 reg_loss: 0.031) ) lr: 0.03968059370496389\nepoch: 6900 acc: 0.721 loss: 0.702 (data_loss: 0.671 reg_loss: 0.031) ) lr: 0.03940973942163448\nepoch: 7000 acc: 0.708 loss: 0.725 (data_loss: 0.695 reg_loss: 0.031) ) lr: 0.03913682034475463\nepoch: 7100 acc: 0.709 loss: 0.700 (data_loss: 0.669 reg_loss: 0.031) ) lr: 0.03886190516559515\nepoch: 7200 acc: 0.722 loss: 0.710 (data_loss: 0.679 reg_loss: 0.030) ) lr: 0.03858506269291113\nepoch: 7300 acc: 0.717 loss: 0.688 (data_loss: 0.658 reg_loss: 0.030) ) lr: 0.03830636182469295\nepoch: 7400 acc: 0.710 loss: 0.713 (data_loss: 0.683 reg_loss: 0.030) ) lr: 0.03802587152007248\nepoch: 7500 acc: 0.718 loss: 0.719 (data_loss: 0.689 reg_loss: 0.030) ) lr: 0.03774366077140049\nepoch: 7600 acc: 0.725 loss: 0.704 (data_loss: 0.674 reg_loss: 0.030) ) lr: 0.037459798576510786\nepoch: 7700 acc: 0.729 loss: 0.693 (data_loss: 0.663 reg_loss: 0.030) ) lr: 0.03717435391118595\nepoch: 7800 acc: 0.708 loss: 0.735 (data_loss: 0.706 reg_loss: 0.030) ) lr: 0.03688739570184001\nepoch: 7900 acc: 0.726 loss: 0.696 (data_loss: 0.667 reg_loss: 0.029) ) lr: 0.0365989927984322\nepoch: 8000 acc: 0.716 loss: 0.681 (data_loss: 0.651 reg_loss: 0.029) ) lr: 0.03630921394762722\nepoch: 8100 acc: 0.713 loss: 0.705 (data_loss: 0.675 reg_loss: 0.029) ) lr: 0.0360181277662148\nepoch: 8200 acc: 0.709 loss: 0.742 (data_loss: 0.713 reg_loss: 0.029) ) lr: 0.03572580271480377\nepoch: 8300 acc: 0.733 loss: 0.700 (data_loss: 0.671 reg_loss: 0.029) ) lr: 0.035432307071803025\nepoch: 8400 acc: 0.709 loss: 0.704 (data_loss: 0.675 reg_loss: 0.029) ) lr: 0.03513770890770358\nepoch: 8500 acc: 0.719 loss: 0.711 (data_loss: 0.683 reg_loss: 0.028) ) lr: 0.034842076059673724\nepoch: 8600 acc: 0.704 loss: 0.714 (data_loss: 0.685 reg_loss: 0.029) ) lr: 0.034545476106480955\nepoch: 8700 acc: 0.716 loss: 0.679 (data_loss: 0.651 reg_loss: 0.028) ) lr: 0.03424797634375167\nepoch: 8800 acc: 0.732 loss: 0.707 (data_loss: 0.679 reg_loss: 0.028) ) lr: 0.03394964375958191\nepoch: 8900 acc: 0.716 loss: 0.718 (data_loss: 0.690 reg_loss: 0.028) ) lr: 0.0336505450105096\nepoch: 9000 acc: 0.715 loss: 0.685 (data_loss: 0.658 reg_loss: 0.028) ) lr: 0.03335074639786025\nepoch: 9100 acc: 0.718 loss: 0.694 (data_loss: 0.666 reg_loss: 0.028) ) lr: 0.033050313844476605\nepoch: 9200 acc: 0.719 loss: 0.685 (data_loss: 0.657 reg_loss: 0.028) ) lr: 0.03274931287184286\nepoch: 9300 acc: 0.717 loss: 0.700 (data_loss: 0.673 reg_loss: 0.027) ) lr: 0.03244780857761376\nepoch: 9400 acc: 0.709 loss: 0.706 (data_loss: 0.679 reg_loss: 0.027) ) lr: 0.03214586561355804\nepoch: 9500 acc: 0.717 loss: 0.706 (data_loss: 0.678 reg_loss: 0.027) ) lr: 0.03184354816392586\nepoch: 9600 acc: 0.725 loss: 0.682 (data_loss: 0.655 reg_loss: 0.027) ) lr: 0.03154091992424932\nepoch: 9700 acc: 0.711 loss: 0.701 (data_loss: 0.674 reg_loss: 0.027) ) lr: 0.031238044080584084\nepoch: 9800 acc: 0.720 loss: 0.724 (data_loss: 0.697 reg_loss: 0.027) ) lr: 0.03093498328920125\nepoch: 9900 acc: 0.724 loss: 0.702 (data_loss: 0.675 reg_loss: 0.027) ) lr: 0.030631799656736635\nepoch: 10000 acc: 0.710 loss: 0.703 (data_loss: 0.676 reg_loss: 0.027) ) lr: 0.030328554720805104\nvalidation, acc: 0.800, loss: 0.640\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Our sample dataset\n",
    "def create_data(n, k):\n",
    "    X = np.zeros((n*k, 2))  # data matrix (each row = single example)\n",
    "    y = np.zeros(n*k, dtype='uint8')  # class labels\n",
    "    for j in range(k):\n",
    "        ix = range(n*j, n*(j+1))\n",
    "        r = np.linspace(0.0, 1, n)  # radius\n",
    "        t = np.linspace(j*4, (j+1)*4, n) + np.random.randn(n)*0.2  # theta\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = j\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, inputs, neurons, weight_regularizer_l1=0, weight_regularizer_l2=0, bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(inputs, neurons)\n",
    "        self.biases = np.zeros((1, neurons))\n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from input ones, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "        # Gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = self.weights.copy()\n",
    "            dL1[dL1 >= 0] = 1\n",
    "            dL1[dL1 < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = self.biases.copy()\n",
    "            dL1[dL1 >= 0] = 1\n",
    "            dL1[dL1 < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "\n",
    "        # Gradient on values\n",
    "        self.dvalues = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# Dropout\n",
    "class Layer_Dropout:\n",
    "\n",
    "    # Init\n",
    "    def __init__(self, rate):\n",
    "        # Store rate, we invert it as for example for dropout of 0.1 we need success rate of 0.9\n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, values):\n",
    "        # Save input values\n",
    "        self.input = values\n",
    "        # Generate and save scaled mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=values.shape) / self.rate\n",
    "        # Apply mask to output values\n",
    "        self.output = values * self.binary_mask\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradient on values\n",
    "        self.dvalues = dvalues * self.binary_mask\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs \n",
    "        # Calculate output values from input ones\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        dvalues = dvalues.copy()  # Since we need to modify original variable, let;s make a copy of values first\n",
    "        dvalues[self.inputs <= 0] = 0  # Zero gradient where input values were negative\n",
    "        self.dvalues = dvalues\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        self.dvalues = dvalues\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Regularization loss calculation\n",
    "    def regularization_loss(self, layer):\n",
    "\n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "\n",
    "        # L1 regularization - weights\n",
    "        if layer.weight_regularizer_l1 > 0:  # only calculate when factor greaten than 0\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "\n",
    "        # L2 regularization - weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "\n",
    "        # L1 regularization - biases\n",
    "        if layer.bias_regularizer_l1 > 0:  # only calculate when factor greater than 0\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "\n",
    "        # L2 regularization - biases\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = y_pred.shape[0]\n",
    "\n",
    "        # Probabilities for target values - only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_pred = y_pred[range(samples), y_true]\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(y_pred)\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        if len(y_true.shape) == 2:\n",
    "            negative_log_likelihoods *= y_true\n",
    "\n",
    "        # Overall loss\n",
    "        data_loss = np.sum(negative_log_likelihoods) / samples\n",
    "        return data_loss\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        samples = dvalues.shape[0]\n",
    "\n",
    "        dvalues = dvalues.copy()  # We need to modify variable directly, make a copy first then\n",
    "        dvalues[range(samples), y_true] -= 1\n",
    "        dvalues = dvalues / samples\n",
    "\n",
    "        self.dvalues = dvalues\n",
    "\n",
    "\n",
    "# SGD Optimizer\n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0., nesterov=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        self.nesterov = nesterov\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.current_learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain momentum arrays, create ones filled with zeros\n",
    "        if not hasattr(layer, 'weight_momentums'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "\n",
    "            # Build weight updates with momentum - take previous updates multiplied by retain factor and update with current gradients\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "            # Apply Nesterov as well?\n",
    "            if self.nesterov:\n",
    "                weight_updates = self.momentum * weight_updates - self.current_learning_rate * layer.dweights\n",
    "                bias_updates = self.momentum * bias_updates - self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        # Update weights with updates which are either vanilla, momentum or momentum+nesterov updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Adagrad Optimizer\n",
    "class Optimizer_Adagrad:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.current_learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays, create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# RMSprop Optimizer\n",
    "class Optimizer_RMSprop:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.current_learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays, create ones filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Adam Optimizer\n",
    "class Optimizer_Adam:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.current_learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays, create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum  with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "        # Get corrected momentum\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))  # self.iteration is 0 at first pass ans we need to start with 1 here\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "        # Get corrected bias\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "#============================================================================================================================================================================#\n",
    "\n",
    "# Create dataset\n",
    "X, y = create_data(1000, 3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)  # first dense layer, 2 inputs (each sample has 2 features), 3 outputs\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create dropout layer\n",
    "dropout1 = Layer_Dropout(0.1)\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "#optimizer = Optimizer_SGD(decay=1e-8, momentum=0.9)\n",
    "#optimizer = Optimizer_Adagrad(decay=1e-8)\n",
    "#optimizer = Optimizer_RMSprop(learning_rate=0.05, decay=4e-8, rho=0.999)\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, decay=1e-8)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Make a forward pass of our training data thru this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Make a forward pass thru Dropout layer \n",
    "    dropout1.forward(activation1.output)\n",
    "\n",
    "    # Make a forward pass thru second Dense layer - it takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(dropout1.output)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Calculate loss from output of activation2 so softmax activation\n",
    "    data_loss = loss_function.forward(activation2.output, y)\n",
    "\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = loss_function.regularization_loss(dense1) + loss_function.regularization_loss(dense2)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    predictions = np.argmax(activation2.output, axis=1)  # calculate values along first axis\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print('epoch:', epoch, 'acc:', f'{accuracy:.3f}', 'loss:', f'{loss:.3f}', '(data_loss:', f'{data_loss:.3f}', 'reg_loss:', f'{regularization_loss:.3f})', ')', 'lr:', optimizer.current_learning_rate)\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dvalues)\n",
    "    dense2.backward(activation2.dvalues)\n",
    "    dropout1.backward(dense2.dvalues)\n",
    "    activation1.backward(dropout1.dvalues)\n",
    "    dense1.backward(activation1.dvalues)\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "#============================================================================================================================================================================#\n",
    "\n",
    "# Validate model\n",
    "\n",
    "# Create test dataset\n",
    "X_test, y_test = create_data(100, 3)\n",
    "\n",
    "# Make a forward pass of our training data thru this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# Make a forward pass thru activation function - we take output of previous layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Make a forward pass thru second Dense layer - it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Make a forward pass thru activation function - we take output of previous layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Calculate loss from output of activation2 so softmax activation\n",
    "loss = loss_function.forward(activation2.output, y_test)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "predictions = np.argmax(activation2.output, axis=1)  # calculate values along first axis\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- while our accuracy and loss suffered considerably, we’ve found a scenario where our validation set actually performs better than our in-sample dataset, which is due to removing dropout when testing\n",
    "- further tweaking would likely fix the accuracy issue; for example, due to our regularization tactics, we can change our layer sizes to 512: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 0 acc: 0.298 loss: 1.099 (data_loss: 1.099 reg_loss: 0.000) ) lr: 0.05\nepoch: 100 acc: 0.714 loss: 0.756 (data_loss: 0.661 reg_loss: 0.096) ) lr: 0.04999752506207612\nepoch: 200 acc: 0.796 loss: 0.677 (data_loss: 0.554 reg_loss: 0.124) ) lr: 0.049990050996574775\nepoch: 300 acc: 0.812 loss: 0.612 (data_loss: 0.484 reg_loss: 0.128) ) lr: 0.04997758005043209\nepoch: 400 acc: 0.804 loss: 0.620 (data_loss: 0.494 reg_loss: 0.126) ) lr: 0.04996011596895705\nepoch: 500 acc: 0.834 loss: 0.580 (data_loss: 0.456 reg_loss: 0.124) ) lr: 0.04993766399395728\nepoch: 600 acc: 0.836 loss: 0.561 (data_loss: 0.438 reg_loss: 0.123) ) lr: 0.04991023086111661\nepoch: 700 acc: 0.832 loss: 0.562 (data_loss: 0.443 reg_loss: 0.119) ) lr: 0.049877824796627425\nepoch: 800 acc: 0.818 loss: 0.588 (data_loss: 0.470 reg_loss: 0.118) ) lr: 0.0498404555130797\nepoch: 900 acc: 0.845 loss: 0.540 (data_loss: 0.427 reg_loss: 0.113) ) lr: 0.04979813420460921\nepoch: 1000 acc: 0.824 loss: 0.557 (data_loss: 0.447 reg_loss: 0.111) ) lr: 0.04975087354130951\nepoch: 1100 acc: 0.834 loss: 0.544 (data_loss: 0.434 reg_loss: 0.110) ) lr: 0.04969868766290968\nepoch: 1200 acc: 0.828 loss: 0.547 (data_loss: 0.440 reg_loss: 0.107) ) lr: 0.04964159217172425\nepoch: 1300 acc: 0.856 loss: 0.523 (data_loss: 0.418 reg_loss: 0.105) ) lr: 0.04957960412487896\nepoch: 1400 acc: 0.843 loss: 0.528 (data_loss: 0.424 reg_loss: 0.104) ) lr: 0.04951274202581829\nepoch: 1500 acc: 0.851 loss: 0.510 (data_loss: 0.408 reg_loss: 0.101) ) lr: 0.049441025815100244\nepoch: 1600 acc: 0.857 loss: 0.504 (data_loss: 0.404 reg_loss: 0.100) ) lr: 0.049364476860485375\nepoch: 1700 acc: 0.851 loss: 0.523 (data_loss: 0.423 reg_loss: 0.100) ) lr: 0.04928311794632677\nepoch: 1800 acc: 0.853 loss: 0.477 (data_loss: 0.378 reg_loss: 0.099) ) lr: 0.04919697326226773\nepoch: 1900 acc: 0.832 loss: 0.536 (data_loss: 0.441 reg_loss: 0.095) ) lr: 0.04910606839125617\nepoch: 2000 acc: 0.843 loss: 0.495 (data_loss: 0.400 reg_loss: 0.095) ) lr: 0.049010430296883054\nepoch: 2100 acc: 0.838 loss: 0.539 (data_loss: 0.444 reg_loss: 0.095) ) lr: 0.048910087310054\nepoch: 2200 acc: 0.866 loss: 0.500 (data_loss: 0.406 reg_loss: 0.094) ) lr: 0.04880506911500336\nepoch: 2300 acc: 0.843 loss: 0.511 (data_loss: 0.418 reg_loss: 0.093) ) lr: 0.048695406734660295\nepoch: 2400 acc: 0.863 loss: 0.493 (data_loss: 0.395 reg_loss: 0.098) ) lr: 0.048581132515377004\nepoch: 2500 acc: 0.867 loss: 0.496 (data_loss: 0.401 reg_loss: 0.095) ) lr: 0.048462280111029786\nepoch: 2600 acc: 0.856 loss: 0.477 (data_loss: 0.385 reg_loss: 0.091) ) lr: 0.04833888446650344\nepoch: 2700 acc: 0.857 loss: 0.469 (data_loss: 0.380 reg_loss: 0.090) ) lr: 0.0482109818005708\nepoch: 2800 acc: 0.865 loss: 0.480 (data_loss: 0.387 reg_loss: 0.094) ) lr: 0.048078609588178944\nepoch: 2900 acc: 0.862 loss: 0.470 (data_loss: 0.376 reg_loss: 0.093) ) lr: 0.04794180654215414\nepoch: 3000 acc: 0.859 loss: 0.461 (data_loss: 0.369 reg_loss: 0.092) ) lr: 0.047800612594338286\nepoch: 3100 acc: 0.862 loss: 0.493 (data_loss: 0.404 reg_loss: 0.089) ) lr: 0.0476550688761695\nepoch: 3200 acc: 0.853 loss: 0.476 (data_loss: 0.387 reg_loss: 0.089) ) lr: 0.0475052176987199\nepoch: 3300 acc: 0.862 loss: 0.474 (data_loss: 0.385 reg_loss: 0.088) ) lr: 0.0473511025322044\nepoch: 3400 acc: 0.851 loss: 0.495 (data_loss: 0.407 reg_loss: 0.087) ) lr: 0.047192767984974744\nepoch: 3500 acc: 0.862 loss: 0.473 (data_loss: 0.387 reg_loss: 0.086) ) lr: 0.047030259782012335\nepoch: 3600 acc: 0.853 loss: 0.497 (data_loss: 0.407 reg_loss: 0.090) ) lr: 0.04686362474293419\nepoch: 3700 acc: 0.864 loss: 0.469 (data_loss: 0.381 reg_loss: 0.088) ) lr: 0.046692910759528077\nepoch: 3800 acc: 0.858 loss: 0.464 (data_loss: 0.373 reg_loss: 0.091) ) lr: 0.04651816677283049\nepoch: 3900 acc: 0.844 loss: 0.492 (data_loss: 0.403 reg_loss: 0.089) ) lr: 0.046339442749763586\nepoch: 4000 acc: 0.857 loss: 0.490 (data_loss: 0.402 reg_loss: 0.089) ) lr: 0.04615678965934673\nepoch: 4100 acc: 0.865 loss: 0.476 (data_loss: 0.385 reg_loss: 0.091) ) lr: 0.04597025944849779\nepoch: 4200 acc: 0.856 loss: 0.469 (data_loss: 0.381 reg_loss: 0.087) ) lr: 0.045779905017441204\nepoch: 4300 acc: 0.847 loss: 0.482 (data_loss: 0.396 reg_loss: 0.086) ) lr: 0.04558578019473819\nepoch: 4400 acc: 0.851 loss: 0.482 (data_loss: 0.396 reg_loss: 0.087) ) lr: 0.04538793971195641\nepoch: 4500 acc: 0.866 loss: 0.461 (data_loss: 0.376 reg_loss: 0.085) ) lr: 0.04518643917799511\nepoch: 4600 acc: 0.854 loss: 0.463 (data_loss: 0.380 reg_loss: 0.083) ) lr: 0.04498133505308287\nepoch: 4700 acc: 0.863 loss: 0.478 (data_loss: 0.391 reg_loss: 0.087) ) lr: 0.044772684622464705\nepoch: 4800 acc: 0.865 loss: 0.465 (data_loss: 0.381 reg_loss: 0.084) ) lr: 0.04456054596979586\nepoch: 4900 acc: 0.844 loss: 0.517 (data_loss: 0.434 reg_loss: 0.083) ) lr: 0.04434497795025991\nepoch: 5000 acc: 0.843 loss: 0.515 (data_loss: 0.403 reg_loss: 0.112) ) lr: 0.04412604016342762\nepoch: 5100 acc: 0.862 loss: 0.502 (data_loss: 0.399 reg_loss: 0.104) ) lr: 0.04390379292587474\nepoch: 5200 acc: 0.861 loss: 0.483 (data_loss: 0.385 reg_loss: 0.098) ) lr: 0.043678297243576227\nepoch: 5300 acc: 0.854 loss: 0.481 (data_loss: 0.388 reg_loss: 0.093) ) lr: 0.04344961478409441\nepoch: 5400 acc: 0.865 loss: 0.471 (data_loss: 0.382 reg_loss: 0.089) ) lr: 0.04321780784857837\nepoch: 5500 acc: 0.854 loss: 0.460 (data_loss: 0.373 reg_loss: 0.087) ) lr: 0.042982939343593214\nepoch: 5600 acc: 0.868 loss: 0.475 (data_loss: 0.389 reg_loss: 0.086) ) lr: 0.042745072752796036\nepoch: 5700 acc: 0.850 loss: 0.474 (data_loss: 0.391 reg_loss: 0.083) ) lr: 0.04250427210847695\nepoch: 5800 acc: 0.864 loss: 0.465 (data_loss: 0.384 reg_loss: 0.081) ) lr: 0.04226060196298256\nepoch: 5900 acc: 0.863 loss: 0.493 (data_loss: 0.412 reg_loss: 0.081) ) lr: 0.04201412736003991\nepoch: 6000 acc: 0.858 loss: 0.474 (data_loss: 0.395 reg_loss: 0.079) ) lr: 0.041764913805998484\nepoch: 6100 acc: 0.851 loss: 0.457 (data_loss: 0.376 reg_loss: 0.081) ) lr: 0.041513027241007916\nepoch: 6200 acc: 0.867 loss: 0.464 (data_loss: 0.384 reg_loss: 0.080) ) lr: 0.04125853401014897\nepoch: 6300 acc: 0.858 loss: 0.464 (data_loss: 0.384 reg_loss: 0.080) ) lr: 0.04100150083453588\nepoch: 6400 acc: 0.867 loss: 0.451 (data_loss: 0.373 reg_loss: 0.078) ) lr: 0.040741994782406296\nepoch: 6500 acc: 0.863 loss: 0.469 (data_loss: 0.392 reg_loss: 0.077) ) lr: 0.040480083240217575\nepoch: 6600 acc: 0.856 loss: 0.455 (data_loss: 0.379 reg_loss: 0.076) ) lr: 0.04021583388376586\nepoch: 6700 acc: 0.869 loss: 0.456 (data_loss: 0.376 reg_loss: 0.079) ) lr: 0.03994931464934522\nepoch: 6800 acc: 0.841 loss: 0.526 (data_loss: 0.425 reg_loss: 0.101) ) lr: 0.03968059370496389\nepoch: 6900 acc: 0.843 loss: 0.503 (data_loss: 0.409 reg_loss: 0.094) ) lr: 0.03940973942163448\nepoch: 7000 acc: 0.857 loss: 0.494 (data_loss: 0.404 reg_loss: 0.090) ) lr: 0.03913682034475463\nepoch: 7100 acc: 0.857 loss: 0.486 (data_loss: 0.400 reg_loss: 0.085) ) lr: 0.03886190516559515\nepoch: 7200 acc: 0.859 loss: 0.498 (data_loss: 0.415 reg_loss: 0.083) ) lr: 0.03858506269291113\nepoch: 7300 acc: 0.860 loss: 0.476 (data_loss: 0.396 reg_loss: 0.080) ) lr: 0.03830636182469295\nepoch: 7400 acc: 0.859 loss: 0.487 (data_loss: 0.408 reg_loss: 0.079) ) lr: 0.03802587152007248\nepoch: 7500 acc: 0.855 loss: 0.462 (data_loss: 0.385 reg_loss: 0.077) ) lr: 0.03774366077140049\nepoch: 7600 acc: 0.858 loss: 0.493 (data_loss: 0.417 reg_loss: 0.076) ) lr: 0.037459798576510786\nepoch: 7700 acc: 0.856 loss: 0.486 (data_loss: 0.410 reg_loss: 0.075) ) lr: 0.03717435391118595\nepoch: 7800 acc: 0.860 loss: 0.451 (data_loss: 0.377 reg_loss: 0.074) ) lr: 0.03688739570184001\nepoch: 7900 acc: 0.857 loss: 0.473 (data_loss: 0.399 reg_loss: 0.074) ) lr: 0.0365989927984322\nepoch: 8000 acc: 0.858 loss: 0.462 (data_loss: 0.388 reg_loss: 0.074) ) lr: 0.03630921394762722\nepoch: 8100 acc: 0.851 loss: 0.518 (data_loss: 0.443 reg_loss: 0.075) ) lr: 0.0360181277662148\nepoch: 8200 acc: 0.858 loss: 0.496 (data_loss: 0.419 reg_loss: 0.076) ) lr: 0.03572580271480377\nepoch: 8300 acc: 0.862 loss: 0.479 (data_loss: 0.403 reg_loss: 0.075) ) lr: 0.035432307071803025\nepoch: 8400 acc: 0.867 loss: 0.464 (data_loss: 0.390 reg_loss: 0.074) ) lr: 0.03513770890770358\nepoch: 8500 acc: 0.856 loss: 0.438 (data_loss: 0.365 reg_loss: 0.073) ) lr: 0.034842076059673724\nepoch: 8600 acc: 0.852 loss: 0.458 (data_loss: 0.384 reg_loss: 0.073) ) lr: 0.034545476106480955\nepoch: 8700 acc: 0.856 loss: 0.458 (data_loss: 0.385 reg_loss: 0.073) ) lr: 0.03424797634375167\nepoch: 8800 acc: 0.865 loss: 0.465 (data_loss: 0.393 reg_loss: 0.072) ) lr: 0.03394964375958191\nepoch: 8900 acc: 0.855 loss: 0.461 (data_loss: 0.390 reg_loss: 0.072) ) lr: 0.0336505450105096\nepoch: 9000 acc: 0.860 loss: 0.446 (data_loss: 0.375 reg_loss: 0.071) ) lr: 0.03335074639786025\nepoch: 9100 acc: 0.859 loss: 0.498 (data_loss: 0.427 reg_loss: 0.071) ) lr: 0.033050313844476605\nepoch: 9200 acc: 0.860 loss: 0.461 (data_loss: 0.390 reg_loss: 0.071) ) lr: 0.03274931287184286\nepoch: 9300 acc: 0.858 loss: 0.477 (data_loss: 0.406 reg_loss: 0.071) ) lr: 0.03244780857761376\nepoch: 9400 acc: 0.852 loss: 0.463 (data_loss: 0.391 reg_loss: 0.072) ) lr: 0.03214586561355804\nepoch: 9500 acc: 0.865 loss: 0.440 (data_loss: 0.369 reg_loss: 0.071) ) lr: 0.03184354816392586\nepoch: 9600 acc: 0.868 loss: 0.451 (data_loss: 0.380 reg_loss: 0.071) ) lr: 0.03154091992424932\nepoch: 9700 acc: 0.869 loss: 0.442 (data_loss: 0.372 reg_loss: 0.070) ) lr: 0.031238044080584084\nepoch: 9800 acc: 0.858 loss: 0.471 (data_loss: 0.400 reg_loss: 0.070) ) lr: 0.03093498328920125\nepoch: 9900 acc: 0.867 loss: 0.441 (data_loss: 0.371 reg_loss: 0.070) ) lr: 0.030631799656736635\nepoch: 10000 acc: 0.865 loss: 0.462 (data_loss: 0.393 reg_loss: 0.069) ) lr: 0.030328554720805104\nvalidation, acc: 0.847, loss: 0.975\n"
    }
   ],
   "source": [
    "# Create dataset ---> changed number of neurons per layer to 512 (warning: this will take a very long time)\n",
    "X, y = create_data(1000, 3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 512, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)  # first dense layer, 2 inputs (each sample has 2 features), 3 outputs\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create dropout layer\n",
    "dropout1 = Layer_Dropout(0.1)\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(512, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "#optimizer = Optimizer_SGD(decay=1e-8, momentum=0.9)\n",
    "#optimizer = Optimizer_Adagrad(decay=1e-8)\n",
    "#optimizer = Optimizer_RMSprop(learning_rate=0.05, decay=4e-8, rho=0.999)\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, decay=1e-8)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Make a forward pass of our training data thru this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Make a forward pass thru Dropout layer \n",
    "    dropout1.forward(activation1.output)\n",
    "\n",
    "    # Make a forward pass thru second Dense layer - it takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(dropout1.output)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Calculate loss from output of activation2 so softmax activation\n",
    "    data_loss = loss_function.forward(activation2.output, y)\n",
    "\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = loss_function.regularization_loss(dense1) + loss_function.regularization_loss(dense2)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    predictions = np.argmax(activation2.output, axis=1)  # calculate values along first axis\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print('epoch:', epoch, 'acc:', f'{accuracy:.3f}', 'loss:', f'{loss:.3f}', '(data_loss:', f'{data_loss:.3f}', 'reg_loss:', f'{regularization_loss:.3f})', ')', 'lr:', optimizer.current_learning_rate)\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dvalues)\n",
    "    dense2.backward(activation2.dvalues)\n",
    "    dropout1.backward(dense2.dvalues)\n",
    "    activation1.backward(dropout1.dvalues)\n",
    "    dense1.backward(activation1.dvalues)\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "# Validate model\n",
    "\n",
    "# Create test dataset\n",
    "X_test, y_test = create_data(100, 3)\n",
    "\n",
    "# Make a forward pass of our training data thru this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# Make a forward pass thru activation function - we take output of previous layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Make a forward pass thru second Dense layer - it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Make a forward pass thru activation function - we take output of previous layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Calculate loss from output of activation2 so softmax activation\n",
    "loss = loss_function.forward(activation2.output, y_test)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "predictions = np.argmax(activation2.output, axis=1)  # calculate values along first axis\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}