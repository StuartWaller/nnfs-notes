{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.33333333 0.33333333 0.33333333]\n [0.33333317 0.33333318 0.33333364]\n [0.33333289 0.33333292 0.3333342 ]\n [0.33333259 0.33333264 0.33333477]\n [0.33333233 0.33333239 0.33333528]]\nloss: 1.0986104615465142\nacc: 0.34\n[[ 1.57663575e-04  7.83685868e-05  4.73243939e-05]\n [ 1.81610390e-04  1.10455707e-05 -3.30962973e-05]]\n[[-3.60553684e-04  9.66122221e-05 -1.03671511e-04]]\n[[ 5.44109554e-05  1.07411413e-04 -1.61822369e-04]\n [-4.07913528e-05 -7.16780945e-05  1.12469447e-04]\n [-5.30112970e-05  8.58172904e-05 -3.28059934e-05]]\n[[-1.06521079e-05 -9.44490453e-06  2.00970125e-05]]\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "# Our sample dataset\n",
    "def create_data(n, k):\n",
    "    X = np.zeros((n*k, 2))  # data matrix (each row = single example)\n",
    "    y = np.zeros(n*k, dtype='uint8')  # class labels\n",
    "    for j in range(k):\n",
    "        ix = range(n*j, n*(j+1))\n",
    "        r = np.linspace(0.0, 1, n)  # radius\n",
    "        t = np.linspace(j*4, (j+1)*4, n) + np.random.randn(n)*0.2  # theta\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = j\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, inputs, neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(inputs, neurons)\n",
    "        self.biases = np.zeros((1, neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from input ones, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dvalues = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from input ones\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable, \n",
    "        # let's make a copy of values first\n",
    "        self.dvalues = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative \n",
    "        self.dvalues[self.inputs <= 0] = 0 \n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        self.dvalues = dvalues.copy()\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = y_pred.shape[0]\n",
    "\n",
    "        # Probabilities for target values - only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_pred = y_pred[range(samples), y_true]\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(y_pred)\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        if len(y_true.shape) == 2:\n",
    "            negative_log_likelihoods *= y_true\n",
    "\n",
    "        # Overall loss\n",
    "        data_loss = np.sum(negative_log_likelihoods) / samples\n",
    "        return data_loss\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        samples = dvalues.shape[0]\n",
    "\n",
    "        self.dvalues = dvalues.copy()  # Copy so we can safely modify\n",
    "        self.dvalues[range(samples), y_true] -= 1\n",
    "        self.dvalues = self.dvalues / samples\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "X, y = create_data(100, 3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)  # first dense layer, 2 inputs (each sample has 2 features), 3 outputs\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(3, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Make a forward pass of our training data thru this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Make a forward pass thru activation function - we take output of previous layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Make a forward pass thru second Dense layer - it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Make a forward pass thru activation function - we take output of previous layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Let's see output of the first few samples:\n",
    "print(activation2.output[:5])\n",
    "\n",
    "# Calculate loss from output of activation2 (softmax activation)\n",
    "loss = loss_function.forward(activation2.output, y)\n",
    "\n",
    "# Print loss value\n",
    "print('loss:', loss)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "predictions = np.argmax(activation2.output, axis=1)  # calculate values along first axis\n",
    "accuracy = np.mean(predictions==y)\n",
    "\n",
    "print('acc:', accuracy)\n",
    "\n",
    "# Backward pass\n",
    "loss_function.backward(activation2.output, y)\n",
    "activation2.backward(loss_function.dvalues)\n",
    "dense2.backward(activation2.dvalues)\n",
    "activation1.backward(dense2.dvalues)\n",
    "dense1.backward(activation1.dvalues)\n",
    "\n",
    "# Print gradients\n",
    "print(dense1.dweights)\n",
    "print(dense1.dbiases)\n",
    "print(dense2.dweights)\n",
    "print(dense2.dbiases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- once we have calculated the gradient, we can use this to adjust weights and biases to decrease the loss\n",
    "- in a previous example, we showed how we could successfully decrease a neuron’s activation function’s (ReLU) output in this manner\n",
    "- recall that we subtracted a fraction of the gradient for each weight and bias parameter\n",
    "- while very rudimentary, this is still a common optimizer, which is called **Stochastic Gradient Descent** (SGD)\n",
    "- as you will soon discover, most optimizers are just variants of SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)\n",
    "- we choose a learning rate of 1.0\n",
    "- we then subtract `learning_rate * parameter_gradients` from the actual parameter values\n",
    "- because our learning rate is 1, we're subtracting the full gradient value from our parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings, \n",
    "    # learning rate of 1.0 is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights -= self.learning_rate * layer.dweights\n",
    "        layer.biases -= self.learning_rate * layer.dbiases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Optimizer_SGD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- after calculating the gradient, use this to update our layer's parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.update_params(dense1)\n",
    "optimizer.update_params(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in our main neural network code, we employ this update after backpropagation\n",
    "---\n",
    "- step 1): make a 2x64 (2 layers, 64 neurons each) densely-connected neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y = create_data(100, 3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values (64 neurons)\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3) \n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create Optimizer\n",
    "optimizer = Optimizer_SGD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- step 2): perform a forward pass of our sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a forward pass of our training data thru this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Make a forward pass thru activation function\n",
    "# it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Make a forward pass thru second Dense layer \n",
    "# it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Make a forward pass thru activation function\n",
    "# it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- step 3): calculate the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loss: 1.0986057583966125\nacc: 0.3466666666666667\n"
    }
   ],
   "source": [
    "# Calculate loss from output of activation2 (softmax activation)\n",
    "loss = loss_function.forward(activation2.output, y)\n",
    "\n",
    "# Let's print loss value\n",
    "print('loss:', loss)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "predictions = np.argmax(activation2.output, axis=1)  # calculate values along first axis\n",
    "accuracy = np.mean(predictions==y)\n",
    "\n",
    "print('acc:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- step 4): perform a backwards pass (i.e., backpropagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "loss_function.backward(activation2.output, y)\n",
    "activation2.backward(loss_function.dvalues)\n",
    "dense2.backward(activation2.dvalues)\n",
    "activation1.backward(dense2.dvalues)\n",
    "dense1.backward(activation1.dvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 5): use optimizer to update weights using the calculated gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update weights and biases (which are in the layers)\n",
    "optimizer.update_params(dense1)\n",
    "optimizer.update_params(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this is everything we need to train our model, but let's perform optimization multiple times using Python's looping capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for Multiple Epochs\n",
    "- we will repeatedly perform a forward pass, backward pass, and optimize our weights and biases until we reach a stopping point\n",
    "- each full pass through all of the training data is called an **epoch**\n",
    "- in most deep learning tasks, a neural network will be trained for multiple epochs, though the ideal scenario would be to have a perfect model with ideal weights and biases after only one epoch\n",
    "- to add multiple epochs, we will initialize our model and run a loop around all the code performing the forward pass, backward pass, and optimization calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 0 acc: 0.36666666666666664 loss: 1.0985882449466124\nepoch: 100 acc: 0.42 loss: 1.0720765463829551\nepoch: 200 acc: 0.41 loss: 1.0624655989595677\nepoch: 300 acc: 0.41 loss: 1.0616153723467232\nepoch: 400 acc: 0.4166666666666667 loss: 1.0606483462374505\nepoch: 500 acc: 0.42 loss: 1.0595095194010553\nepoch: 600 acc: 0.42 loss: 1.05807753621963\nepoch: 700 acc: 0.41333333333333333 loss: 1.0555091262523664\nepoch: 800 acc: 0.43 loss: 1.050181932524032\nepoch: 900 acc: 0.4533333333333333 loss: 1.0397715979508741\nepoch: 1000 acc: 0.47333333333333333 loss: 1.0230670004571563\nepoch: 1100 acc: 0.42 loss: 1.0214345839016705\nepoch: 1200 acc: 0.39666666666666667 loss: 1.014260785726662\nepoch: 1300 acc: 0.39 loss: 1.0067441796699017\nepoch: 1400 acc: 0.4 loss: 1.0034592795149848\nepoch: 1500 acc: 0.49333333333333335 loss: 0.9926242584545484\nepoch: 1600 acc: 0.4066666666666667 loss: 1.0551332809720675\nepoch: 1700 acc: 0.4066666666666667 loss: 1.0360131977649636\nepoch: 1800 acc: 0.45 loss: 1.012606419761076\nepoch: 1900 acc: 0.43333333333333335 loss: 0.9861035066577404\nepoch: 2000 acc: 0.4066666666666667 loss: 0.9957380619704976\nepoch: 2100 acc: 0.52 loss: 0.9756760410340287\nepoch: 2200 acc: 0.4766666666666667 loss: 0.9652003638953491\nepoch: 2300 acc: 0.4666666666666667 loss: 1.0024288526942455\nepoch: 2400 acc: 0.46 loss: 0.9528957760990956\nepoch: 2500 acc: 0.43666666666666665 loss: 0.9455384759283153\nepoch: 2600 acc: 0.49666666666666665 loss: 0.9347657062397783\nepoch: 2700 acc: 0.4666666666666667 loss: 0.974253940270025\nepoch: 2800 acc: 0.49666666666666665 loss: 0.9225541989622105\nepoch: 2900 acc: 0.5133333333333333 loss: 0.9366305289364203\nepoch: 3000 acc: 0.52 loss: 0.9278532249245515\nepoch: 3100 acc: 0.52 loss: 0.8932536471522623\nepoch: 3200 acc: 0.5766666666666667 loss: 0.8904980163030128\nepoch: 3300 acc: 0.5766666666666667 loss: 0.8519401186143011\nepoch: 3400 acc: 0.6 loss: 0.8274160351013353\nepoch: 3500 acc: 0.6233333333333333 loss: 0.8193085339869824\nepoch: 3600 acc: 0.59 loss: 0.841671652140223\nepoch: 3700 acc: 0.5866666666666667 loss: 0.7764390257238367\nepoch: 3800 acc: 0.5966666666666667 loss: 0.7731071557061392\nepoch: 3900 acc: 0.6533333333333333 loss: 0.7385732161520739\nepoch: 4000 acc: 0.6633333333333333 loss: 0.7743772957016264\nepoch: 4100 acc: 0.6233333333333333 loss: 0.7880219348800727\nepoch: 4200 acc: 0.6266666666666667 loss: 0.7422667173087415\nepoch: 4300 acc: 0.64 loss: 0.8370068656770066\nepoch: 4400 acc: 0.6166666666666667 loss: 0.7598038403490195\nepoch: 4500 acc: 0.6533333333333333 loss: 0.7213347965636165\nepoch: 4600 acc: 0.72 loss: 0.6735018704060408\nepoch: 4700 acc: 0.6666666666666666 loss: 0.7299350113695592\nepoch: 4800 acc: 0.6633333333333333 loss: 0.6978424869185941\nepoch: 4900 acc: 0.6833333333333333 loss: 0.7231383751312893\nepoch: 5000 acc: 0.6233333333333333 loss: 0.7738699713410239\nepoch: 5100 acc: 0.7266666666666667 loss: 0.677163140100367\nepoch: 5200 acc: 0.68 loss: 0.7260403657554386\nepoch: 5300 acc: 0.66 loss: 0.6931233047626211\nepoch: 5400 acc: 0.71 loss: 0.6679988298249427\nepoch: 5500 acc: 0.7166666666666667 loss: 0.6268121574745479\nepoch: 5600 acc: 0.7033333333333334 loss: 0.6771470294273978\nepoch: 5700 acc: 0.7 loss: 0.6741399122398296\nepoch: 5800 acc: 0.7166666666666667 loss: 0.6562982077776156\nepoch: 5900 acc: 0.7166666666666667 loss: 0.662863396531191\nepoch: 6000 acc: 0.73 loss: 0.6341784602607692\nepoch: 6100 acc: 0.7433333333333333 loss: 0.6339993668200619\nepoch: 6200 acc: 0.6166666666666667 loss: 0.8192445424295391\nepoch: 6300 acc: 0.7033333333333334 loss: 0.719222581278236\nepoch: 6400 acc: 0.7366666666666667 loss: 0.6404524605737231\nepoch: 6500 acc: 0.6733333333333333 loss: 0.722945620926518\nepoch: 6600 acc: 0.7366666666666667 loss: 0.6088559411595763\nepoch: 6700 acc: 0.7466666666666667 loss: 0.6142379669061903\nepoch: 6800 acc: 0.64 loss: 0.9404859183002352\nepoch: 6900 acc: 0.6833333333333333 loss: 0.6625184380707447\nepoch: 7000 acc: 0.5766666666666667 loss: 1.1910537911363734\nepoch: 7100 acc: 0.7133333333333334 loss: 0.604463174090016\nepoch: 7200 acc: 0.7533333333333333 loss: 0.5644657263745575\nepoch: 7300 acc: 0.7566666666666667 loss: 0.5508945397187556\nepoch: 7400 acc: 0.7566666666666667 loss: 0.5425683817560626\nepoch: 7500 acc: 0.7533333333333333 loss: 0.5390620769568856\nepoch: 7600 acc: 0.7666666666666667 loss: 0.5384554815886329\nepoch: 7700 acc: 0.76 loss: 0.5319739639160361\nepoch: 7800 acc: 0.7766666666666666 loss: 0.5242642561658085\nepoch: 7900 acc: 0.6166666666666667 loss: 1.1861533725477313\nepoch: 8000 acc: 0.7766666666666666 loss: 0.5044594057661357\nepoch: 8100 acc: 0.6033333333333334 loss: 1.1647771956332544\nepoch: 8200 acc: 0.8066666666666666 loss: 0.480672372163781\nepoch: 8300 acc: 0.7733333333333333 loss: 0.4869189888409398\nepoch: 8400 acc: 0.79 loss: 0.48143108113959565\nepoch: 8500 acc: 0.8166666666666667 loss: 0.4549475720374526\nepoch: 8600 acc: 0.8033333333333333 loss: 0.47604917042134587\nepoch: 8700 acc: 0.8 loss: 0.45701976656757853\nepoch: 8800 acc: 0.8066666666666666 loss: 0.4700935475725914\nepoch: 8900 acc: 0.7833333333333333 loss: 0.5566396868676623\nepoch: 9000 acc: 0.81 loss: 0.4668361664405666\nepoch: 9100 acc: 0.7966666666666666 loss: 0.4614565699413955\nepoch: 9200 acc: 0.83 loss: 0.4723721050655416\nepoch: 9300 acc: 0.8133333333333334 loss: 0.457057238260158\nepoch: 9400 acc: 0.8133333333333334 loss: 0.49591852304114703\nepoch: 9500 acc: 0.8166666666666667 loss: 0.4792985824940801\nepoch: 9600 acc: 0.8333333333333334 loss: 0.47244855963520666\nepoch: 9700 acc: 0.8333333333333334 loss: 0.4645052686251883\nepoch: 9800 acc: 0.7733333333333333 loss: 0.4514949705735866\nepoch: 9900 acc: 0.78 loss: 0.4312553680263477\nepoch: 10000 acc: 0.78 loss: 0.4189660575219176\n"
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = create_data(100, 3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 64)  # first dense layer, 2 inputs (each sample has 2 features), 64 outputs\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD()\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Make a forward pass of our training data thru this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Make a forward pass thru activation function\n",
    "    # we take output of previous layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Make a forward pass thru second Dense layer \n",
    "    # outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Make a forward pass thru activation function\n",
    "    # we take output of previous layer here\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Calculate loss from output of activation2 so softmax activation\n",
    "    loss = loss_function.forward(activation2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(activation2.output, axis=1) \n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print('epoch:', epoch, 'acc:', accuracy, 'loss:', loss)\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dvalues)\n",
    "    dense2.backward(activation2.dvalues)\n",
    "    activation1.backward(dense2.dvalues)\n",
    "    dense1.backward(activation1.dvalues)\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- our neural network stays stuck at around a loss of 1 and an accuracy around 0.33\n",
    "- iterating over more epochs doesn’t seem to be helpful at this point, which tells us that we’re likely stuck with our optimization\n",
    "- recall that we’re adjusting our weights and biases by applying some fraction, in this case 1.0, to the gradient and subtracting this from the weights and biases\n",
    "- this fraction is called the learning rate (LR) and is the primary adjustable parameter for the optimizer as it decreases loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate \n",
    "- knowing exactly what the learning rate should be to get the most out of your training process isn’t possible, but a good rule is that your initial training will benefit from a larger learning rate to take those initial steps faster\n",
    "- if you start with steps that are too small, you might get stuck in a local minimum, which you won't be able to escape due to not making large enough updates to the parameters\n",
    "- for example, let's change SGD optimizer to have a learning rate of 0.5 instead of 1.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 0 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 100 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 200 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 300 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 400 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 500 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 600 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 700 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 800 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 900 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 1000 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 1100 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 1200 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 1300 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 1400 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 1500 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 1600 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 1700 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 1800 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 1900 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 2000 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 2100 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 2200 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 2300 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 2400 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 2500 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 2600 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 2700 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 2800 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 2900 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 3000 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 3100 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 3200 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 3300 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 3400 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 3500 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 3600 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 3700 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 3800 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 3900 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 4000 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 4100 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 4200 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 4300 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 4400 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 4500 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 4600 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 4700 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 4800 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 4900 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 5000 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 5100 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 5200 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 5300 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 5400 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 5500 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 5600 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 5700 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 5800 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 5900 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 6000 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 6100 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 6200 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 6300 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 6400 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 6500 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 6600 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 6700 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 6800 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 6900 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 7000 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 7100 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 7200 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 7300 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 7400 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 7500 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 7600 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 7700 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 7800 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 7900 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 8000 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 8100 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 8200 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 8300 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 8400 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 8500 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 8600 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 8700 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 8800 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 8900 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 9000 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 9100 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 9200 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 9300 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 9400 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 9500 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 9600 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 9700 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 9800 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 9900 acc: 0.8366666666666667 loss: 0.4451942603476483\nepoch: 10000 acc: 0.8366666666666667 loss: 0.4451942603476483\n"
    }
   ],
   "source": [
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(learning_rate=.5) # adjusted learning rate\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    loss = loss_function.forward(activation2.output, y)\n",
    "\n",
    "    predictions = np.argmax(activation2.output, axis=1)\n",
    "\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print('epoch:', epoch, 'acc:', accuracy, 'loss:', loss)\n",
    "\n",
    "\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dvalues)\n",
    "    dense2.backward(activation2.dvalues)\n",
    "    activation1.backward(dense2.dvalues)\n",
    "    dense1.backward(activation1.dvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lower loss is not always associated with higher accuracy\n",
    "- remember, even if we desire the best accuracy out of our model, the optimizer’s task is to decrease loss\n",
    "- a common solution to keep initial updates large, but also explore a variety of learning rates during training is to implement a **learning rate decay**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Decay\n",
    "- the idea behind learning rate decay is to start with a large learning rate, say 1.0, and then decrease it throughout training\n",
    "- one option is to decrease the learning rate in response to the behavior of the loss across epochs\n",
    "- another option, which we are going to implement, is to program a **Decay Rate**, which decays the learning rate constantly per batch or epoch\n",
    "- let’s plan to decay per step, which can also be referred to as **1/t decaying** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.9090909090909091\n"
    }
   ],
   "source": [
    "starting_learning_rate = 1.\n",
    "learning_rate_decay = 0.1\n",
    "step = 1\n",
    "\n",
    "learning_rate = starting_learning_rate * (1. / (1 + learning_rate_decay * step))\n",
    "print(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in practice, 0.1 is an aggressive decay rate, but this should give you a sense of the concept\n",
    "- this is what step 20 looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.3333333333333333\n"
    }
   ],
   "source": [
    "starting_learning_rate = 1.\n",
    "learning_rate_decay = 0.1\n",
    "step = 20\n",
    "\n",
    "learning_rate = starting_learning_rate * (1. / (1 + learning_rate_decay * step))\n",
    "print(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can also apply a loop, which is more comparable to our future application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1.0\n0.9090909090909091\n0.8333333333333334\n0.7692307692307692\n0.7142857142857143\n0.6666666666666666\n0.625\n0.588235294117647\n0.5555555555555556\n0.5263157894736842\n0.5\n0.47619047619047616\n0.45454545454545453\n0.4347826086956522\n0.41666666666666663\n0.4\n0.3846153846153846\n0.37037037037037035\n0.35714285714285715\n0.3448275862068965\n"
    }
   ],
   "source": [
    "starting_learning_rate = 1.\n",
    "learning_rate_decay = 0.1\n",
    "\n",
    "for step in range (20):\n",
    "    learning_rate = starting_learning_rate * (1. / (1 + learning_rate_decay * step))\n",
    "    print(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- initially, the learning rate drops fast, but the change in the learning rate lowers at each step\n",
    "- this lets the model get as close at possible to the minimum\n",
    "- we can now update our `Optimizer_SGD()` class by implementing a learning rate decay: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "\n",
    "    def __init__(self, learning_rate=1., decay=0.1):\n",
    "        self.learning_rate = learning_rate # initial learning rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0 # keeps track of the number of iterations the optimizer has completed (works with post_update_params)\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self): # updates learning rate using the formula we covered earlier\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.current_learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer): # this method remains unchanged\n",
    "        layer.weights -= self.current_learning_rate * layer.dweights\n",
    "        layer.biases -= self.current_learning_rate * layer.dbiases\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self): # adds to our self.iterations tracking\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in the `__init__()` method, we added handling for the current learning rate, and `self.learning_rate` is the initial learning rate\n",
    "- we also added attributes to track the decay rate as well as the number of iterations that the optimizer has completed\n",
    "- next, we added a new method called `pre_update_params()`, which will update our `self.current_learning_rate` using the formula we covered earlier\n",
    "- the `update_params()` method remains unchanged, but we do have a new `post_update_params()` method which adds to our `self.iterations` tracking\n",
    "- with our updated SGD optimizer class, let’s use a decay rate of 1e-6 (0.000001) and train our model again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 0, acc: 0.360, loss: 1.099, lr: 1.0\nepoch: 100, acc: 0.397, loss: 1.087, lr: 0.995062394416656\nepoch: 200, acc: 0.417, loss: 1.077, lr: 0.9802979952223477\nepoch: 300, acc: 0.417, loss: 1.076, lr: 0.9561451727242277\nepoch: 400, acc: 0.403, loss: 1.074, lr: 0.923310797854788\nepoch: 500, acc: 0.397, loss: 1.072, lr: 0.8827358848191256\nepoch: 600, acc: 0.417, loss: 1.069, lr: 0.8355508252084564\nepoch: 700, acc: 0.427, loss: 1.065, lr: 0.7830231828383335\nepoch: 800, acc: 0.440, loss: 1.060, lr: 0.7265014060259195\nepoch: 900, acc: 0.437, loss: 1.055, lr: 0.6673579255709609\nepoch: 1000, acc: 0.423, loss: 1.049, lr: 0.6069349461705745\nepoch: 1100, acc: 0.403, loss: 1.038, lr: 0.5464958360051433\nepoch: 1200, acc: 0.400, loss: 1.028, lr: 0.4871844246103457\nepoch: 1300, acc: 0.420, loss: 1.019, lr: 0.4299937982030348\nepoch: 1400, acc: 0.453, loss: 1.010, lr: 0.3757454062555259\nepoch: 1500, acc: 0.483, loss: 1.000, lr: 0.3250785338236289\nepoch: 1600, acc: 0.503, loss: 0.991, lr: 0.2784495129408349\nepoch: 1700, acc: 0.517, loss: 0.983, lr: 0.23613949135416537\nepoch: 1800, acc: 0.513, loss: 0.976, lr: 0.19826917878634304\nepoch: 1900, acc: 0.527, loss: 0.970, lr: 0.16481876226269923\nepoch: 2000, acc: 0.530, loss: 0.965, lr: 0.1356511181638607\nepoch: 2100, acc: 0.543, loss: 0.961, lr: 0.11053653055281436\nepoch: 2200, acc: 0.540, loss: 0.958, lr: 0.089177323645256\nepoch: 2300, acc: 0.543, loss: 0.955, lr: 0.07123109589948101\nepoch: 2400, acc: 0.540, loss: 0.953, lr: 0.056331567558959926\nepoch: 2500, acc: 0.547, loss: 0.951, lr: 0.0441063884874588\nepoch: 2600, acc: 0.550, loss: 0.950, lr: 0.0341915700272081\nepoch: 2700, acc: 0.550, loss: 0.949, lr: 0.02624248174937513\nepoch: 2800, acc: 0.547, loss: 0.948, lr: 0.019941577544805203\nepoch: 2900, acc: 0.547, loss: 0.948, lr: 0.015003179275444308\nepoch: 3000, acc: 0.543, loss: 0.947, lr: 0.01117575048620662\nepoch: 3100, acc: 0.543, loss: 0.947, lr: 0.008242142865759196\nepoch: 3200, acc: 0.543, loss: 0.947, lr: 0.006018303064661818\nepoch: 3300, acc: 0.543, loss: 0.946, lr: 0.004350897734611063\nepoch: 3400, acc: 0.543, loss: 0.946, lr: 0.003114261199530007\nepoch: 3500, acc: 0.543, loss: 0.946, lr: 0.002207003202249269\nepoch: 3600, acc: 0.543, loss: 0.946, lr: 0.0015485423674865093\nepoch: 3700, acc: 0.543, loss: 0.946, lr: 0.0010757611402198023\nepoch: 3800, acc: 0.543, loss: 0.946, lr: 0.0007399147331724246\nepoch: 3900, acc: 0.543, loss: 0.946, lr: 0.0005038728890344654\nepoch: 4000, acc: 0.543, loss: 0.946, lr: 0.00033973026119931657\nepoch: 4100, acc: 0.543, loss: 0.946, lr: 0.0002267889184820343\nepoch: 4200, acc: 0.543, loss: 0.946, lr: 0.00014989398761309564\nepoch: 4300, acc: 0.543, loss: 0.946, lr: 9.808934028869505e-05\nepoch: 4400, acc: 0.543, loss: 0.946, lr: 6.355285449252535e-05\nepoch: 4500, acc: 0.543, loss: 0.946, lr: 4.0768467658015205e-05\nepoch: 4600, acc: 0.543, loss: 0.946, lr: 2.5893468144679178e-05\nepoch: 4700, acc: 0.543, loss: 0.946, lr: 1.628294655854071e-05\nepoch: 4800, acc: 0.543, loss: 0.946, lr: 1.0138019940186175e-05\nepoch: 4900, acc: 0.543, loss: 0.946, lr: 6.249583633212575e-06\nepoch: 5000, acc: 0.543, loss: 0.946, lr: 3.8144089883259635e-06\nepoch: 5100, acc: 0.543, loss: 0.946, lr: 2.305059231691406e-06\nepoch: 5200, acc: 0.543, loss: 0.946, lr: 1.3791643161310815e-06\nepoch: 5300, acc: 0.543, loss: 0.946, lr: 8.170139156114397e-07\nepoch: 5400, acc: 0.543, loss: 0.946, lr: 4.792066593401758e-07\nepoch: 5500, acc: 0.543, loss: 0.946, lr: 2.782893680764022e-07\nepoch: 5600, acc: 0.543, loss: 0.946, lr: 1.6001149879166383e-07\nepoch: 5700, acc: 0.543, loss: 0.946, lr: 9.109342048721564e-08\nepoch: 5800, acc: 0.543, loss: 0.946, lr: 5.134574905726243e-08\nepoch: 5900, acc: 0.543, loss: 0.946, lr: 2.8655241631372176e-08\nepoch: 6000, acc: 0.543, loss: 0.946, lr: 1.583383744557663e-08\nepoch: 6100, acc: 0.543, loss: 0.946, lr: 8.662659507027728e-09\nepoch: 6200, acc: 0.543, loss: 0.946, lr: 4.692450404761907e-09\nepoch: 6300, acc: 0.543, loss: 0.946, lr: 2.51670291025561e-09\nepoch: 6400, acc: 0.543, loss: 0.946, lr: 1.3364368859267267e-09\nepoch: 6500, acc: 0.543, loss: 0.946, lr: 7.026671123615271e-10\nepoch: 6600, acc: 0.543, loss: 0.946, lr: 3.657934298602095e-10\nepoch: 6700, acc: 0.543, loss: 0.946, lr: 1.8854182330595555e-10\nepoch: 6800, acc: 0.543, loss: 0.946, lr: 9.622002417496261e-11\nepoch: 6900, acc: 0.543, loss: 0.946, lr: 4.8619401265033695e-11\nepoch: 7000, acc: 0.543, loss: 0.946, lr: 2.4324311979177358e-11\nepoch: 7100, acc: 0.543, loss: 0.946, lr: 1.2049215336857844e-11\nepoch: 7200, acc: 0.543, loss: 0.946, lr: 5.909689148197751e-12\nepoch: 7300, acc: 0.543, loss: 0.946, lr: 2.8698461001469168e-12\nepoch: 7400, acc: 0.543, loss: 0.946, lr: 1.3798793157142183e-12\nepoch: 7500, acc: 0.543, loss: 0.946, lr: 6.569201091988038e-13\nepoch: 7600, acc: 0.543, loss: 0.946, lr: 3.0965164449689895e-13\nepoch: 7700, acc: 0.543, loss: 0.946, lr: 1.4451869646839734e-13\nepoch: 7800, acc: 0.543, loss: 0.946, lr: 6.678284535570419e-14\nepoch: 7900, acc: 0.543, loss: 0.946, lr: 3.055599762284006e-14\nepoch: 8000, acc: 0.543, loss: 0.946, lr: 1.3842645906161382e-14\nepoch: 8100, acc: 0.543, loss: 0.946, lr: 6.209166031459825e-15\nepoch: 8200, acc: 0.543, loss: 0.946, lr: 2.7576515203336005e-15\nepoch: 8300, acc: 0.543, loss: 0.946, lr: 1.2126567014845225e-15\nepoch: 8400, acc: 0.543, loss: 0.946, lr: 5.279942108634617e-16\nepoch: 8500, acc: 0.543, loss: 0.946, lr: 2.2762170930437706e-16\nepoch: 8600, acc: 0.543, loss: 0.946, lr: 9.716097332766565e-17\nepoch: 8700, acc: 0.543, loss: 0.946, lr: 4.106427098048629e-17\nepoch: 8800, acc: 0.543, loss: 0.946, lr: 1.7184262108469076e-17\nepoch: 8900, acc: 0.543, loss: 0.946, lr: 7.12020662763002e-18\nepoch: 9000, acc: 0.543, loss: 0.946, lr: 2.921121828000155e-18\nepoch: 9100, acc: 0.543, loss: 0.946, lr: 1.186595054330777e-18\nepoch: 9200, acc: 0.543, loss: 0.946, lr: 4.7725623982151e-19\nepoch: 9300, acc: 0.543, loss: 0.946, lr: 1.9006289517443693e-19\nepoch: 9400, acc: 0.543, loss: 0.946, lr: 7.494456375703395e-20\nepoch: 9500, acc: 0.543, loss: 0.946, lr: 2.926041140271441e-20\nepoch: 9600, acc: 0.543, loss: 0.946, lr: 1.13114594471042e-20\nepoch: 9700, acc: 0.543, loss: 0.946, lr: 4.329674088528229e-21\nepoch: 9800, acc: 0.543, loss: 0.946, lr: 1.640931851962078e-21\nepoch: 9900, acc: 0.543, loss: 0.946, lr: 6.157792985816496e-22\nepoch: 10000, acc: 0.543, loss: 0.946, lr: 2.2880170201279647e-22\n"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "# Create dataset\n",
    "X, y = create_data(100, 3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 64)  # first dense layer, 2 inputs (each sample has 2 features), 3 outputs\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(decay=1e-6)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Make a forward pass of our training data thru this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Make a forward pass thru second Dense layer - it takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Calculate loss from output of activation2 so softmax activation\n",
    "    loss = loss_function.forward(activation2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    predictions = np.argmax(activation2.output, axis=1)  # calculate values along first axis\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f}, lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dvalues)\n",
    "    dense2.backward(activation2.dvalues)\n",
    "    activation1.backward(dense2.dvalues)\n",
    "    dense1.backward(activation1.dvalues)\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- as you can clearly see, this model got stuck somewhere in the first few thousand epochs\n",
    "- the model got stuck because the learning rate decayed far too quickly, which made it too small to escape local minima\n",
    "- let's trying making our decay rate smaller (now 5e-8):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 0, acc: 0.360, loss: 1.099, lr: 1.0\nepoch: 100, acc: 0.400, loss: 1.087, lr: 0.9997525310359338\nepoch: 200, acc: 0.417, loss: 1.077, lr: 0.9990054981534312\nepoch: 300, acc: 0.420, loss: 1.076, lr: 0.997760023693318\nepoch: 400, acc: 0.403, loss: 1.074, lr: 0.9960179759343823\nepoch: 500, acc: 0.403, loss: 1.071, lr: 0.9937819644233231\nepoch: 600, acc: 0.417, loss: 1.067, lr: 0.9910553334608369\nepoch: 700, acc: 0.437, loss: 1.063, lr: 0.9878421537665991\nepoch: 800, acc: 0.453, loss: 1.055, lr: 0.9841472123522141\nepoch: 900, acc: 0.407, loss: 1.062, lr: 0.9799760006374122\nepoch: 1000, acc: 0.410, loss: 1.058, lr: 0.9753347008507738\nepoch: 1100, acc: 0.397, loss: 1.056, lr: 0.9702301707621199\nepoch: 1200, acc: 0.427, loss: 1.068, lr: 0.9646699267993539\nepoch: 1300, acc: 0.413, loss: 1.055, lr: 0.9586621256079148\nepoch: 1400, acc: 0.390, loss: 1.078, lr: 0.9522155441161774\nepoch: 1500, acc: 0.390, loss: 1.047, lr: 0.9453395581749566\nepoch: 1600, acc: 0.417, loss: 1.043, lr: 0.9380441198438796\nepoch: 1700, acc: 0.483, loss: 1.050, lr: 0.9303397334016029\nepoch: 1800, acc: 0.487, loss: 1.046, lr: 0.9222374301607807\nepoch: 1900, acc: 0.487, loss: 1.055, lr: 0.9137487421722358\nepoch: 2000, acc: 0.433, loss: 1.041, lr: 0.9048856749059981\nepoch: 2100, acc: 0.450, loss: 1.010, lr: 0.8956606789996725\nepoch: 2200, acc: 0.480, loss: 1.016, lr: 0.8860866211670557\nepoch: 2300, acc: 0.467, loss: 0.990, lr: 0.8761767543619547\nepoch: 2400, acc: 0.470, loss: 1.015, lr: 0.8659446872938026\nepoch: 2500, acc: 0.523, loss: 0.994, lr: 0.8554043533929261\nepoch: 2600, acc: 0.493, loss: 0.974, lr: 0.844569979324141\nepoch: 2700, acc: 0.517, loss: 0.998, lr: 0.833456053147818\nepoch: 2800, acc: 0.560, loss: 0.978, lr: 0.822077292227582\nepoch: 2900, acc: 0.483, loss: 0.963, lr: 0.8104486109834639\nepoch: 3000, acc: 0.480, loss: 0.962, lr: 0.7985850885885959\nepoch: 3100, acc: 0.520, loss: 0.948, lr: 0.7865019367063895\nepoch: 3200, acc: 0.520, loss: 0.957, lr: 0.7742144673636858\nepoch: 3300, acc: 0.517, loss: 0.941, lr: 0.7617380610534867\nepoch: 3400, acc: 0.520, loss: 0.936, lr: 0.7490881351587024\nepoch: 3500, acc: 0.503, loss: 0.937, lr: 0.7362801127858304\nepoch: 3600, acc: 0.497, loss: 0.940, lr: 0.7233293920946249\nepoch: 3700, acc: 0.577, loss: 0.938, lr: 0.7102513162067197\nepoch: 3800, acc: 0.527, loss: 0.942, lr: 0.6970611437727328\nepoch: 3900, acc: 0.567, loss: 0.946, lr: 0.6837740202737295\nepoch: 4000, acc: 0.580, loss: 0.941, lr: 0.6704049501290217\nepoch: 4100, acc: 0.567, loss: 0.913, lr: 0.6569687696781638\nepoch: 4200, acc: 0.523, loss: 0.916, lr: 0.6434801211006969\nepoch: 4300, acc: 0.533, loss: 0.922, lr: 0.6299534273327249\nepoch: 4400, acc: 0.583, loss: 0.932, lr: 0.6164028680347811\nepoch: 4500, acc: 0.577, loss: 0.932, lr: 0.6028423566606949\nepoch: 4600, acc: 0.557, loss: 0.910, lr: 0.589285518672321\nepoch: 4700, acc: 0.603, loss: 0.917, lr: 0.5757456709400948\nepoch: 4800, acc: 0.573, loss: 0.897, lr: 0.5622358023643838\nepoch: 4900, acc: 0.583, loss: 0.898, lr: 0.5487685557476036\nepoch: 5000, acc: 0.547, loss: 0.898, lr: 0.5353562109420904\nepoch: 5100, acc: 0.567, loss: 0.888, lr: 0.5220106692936887\nepoch: 5200, acc: 0.563, loss: 0.898, lr: 0.5087434393960976\nepoch: 5300, acc: 0.577, loss: 0.879, lr: 0.49556562416610983\nepoch: 5400, acc: 0.580, loss: 0.879, lr: 0.48248790924506524\nepoch: 5500, acc: 0.617, loss: 0.885, lr: 0.46952055272713383\nepoch: 5600, acc: 0.587, loss: 0.876, lr: 0.456673376210437\nepoch: 5700, acc: 0.590, loss: 0.860, lr: 0.4439557571625653\nepoch: 5800, acc: 0.597, loss: 0.857, lr: 0.43137662258773346\nepoch: 5900, acc: 0.577, loss: 0.862, lr: 0.4189444439786708\nepoch: 6000, acc: 0.623, loss: 0.854, lr: 0.40666723353238227\nepoch: 6100, acc: 0.583, loss: 0.857, lr: 0.3945525416051419\nepoch: 6200, acc: 0.610, loss: 0.840, lr: 0.3826074553785173\nepoch: 6300, acc: 0.640, loss: 0.832, lr: 0.37083859870486113\nepoch: 6400, acc: 0.603, loss: 0.835, lr: 0.35925213309759024\nepoch: 6500, acc: 0.633, loss: 0.826, lr: 0.34785375982865246\nepoch: 6600, acc: 0.650, loss: 0.817, lr: 0.3366487230929319\nepoch: 6700, acc: 0.653, loss: 0.817, lr: 0.32564181419691024\nepoch: 6800, acc: 0.640, loss: 0.797, lr: 0.31483737672672496\nepoch: 6900, acc: 0.663, loss: 0.794, lr: 0.304239312648824\nepoch: 7000, acc: 0.660, loss: 0.786, lr: 0.29385108929473275\nepoch: 7100, acc: 0.670, loss: 0.775, lr: 0.28367574718000566\nepoch: 7200, acc: 0.683, loss: 0.774, lr: 0.2737159086062279\nepoch: 7300, acc: 0.683, loss: 0.764, lr: 0.26397378699399016\nepoch: 7400, acc: 0.683, loss: 0.755, lr: 0.2544511968940157\nepoch: 7500, acc: 0.693, loss: 0.750, lr: 0.24514956462315737\nepoch: 7600, acc: 0.700, loss: 0.746, lr: 0.23606993947169932\nepoch: 7700, acc: 0.707, loss: 0.742, lr: 0.22721300542836217\nepoch: 7800, acc: 0.710, loss: 0.738, lr: 0.2185790933695863\nepoch: 7900, acc: 0.717, loss: 0.734, lr: 0.21016819366003006\nepoch: 8000, acc: 0.720, loss: 0.732, lr: 0.20197996911179583\nepoch: 8100, acc: 0.717, loss: 0.729, lr: 0.19401376825064212\nepoch: 8200, acc: 0.717, loss: 0.726, lr: 0.18626863883837028\nepoch: 8300, acc: 0.720, loss: 0.724, lr: 0.17874334160165903\nepoch: 8400, acc: 0.717, loss: 0.722, lr: 0.17143636411886728\nepoch: 8500, acc: 0.710, loss: 0.719, lr: 0.16434593481770504\nepoch: 8600, acc: 0.727, loss: 0.717, lr: 0.1574700370381852\nepoch: 8700, acc: 0.723, loss: 0.715, lr: 0.15080642311689468\nepoch: 8800, acc: 0.727, loss: 0.713, lr: 0.14435262845036528\nepoch: 8900, acc: 0.723, loss: 0.711, lr: 0.13810598549714745\nepoch: 9000, acc: 0.723, loss: 0.710, lr: 0.1320636376801004\nepoch: 9100, acc: 0.727, loss: 0.708, lr: 0.1262225531523929\nepoch: 9200, acc: 0.727, loss: 0.706, lr: 0.1205795383927474\nepoch: 9300, acc: 0.730, loss: 0.704, lr: 0.11513125159754022\nepoch: 9400, acc: 0.727, loss: 0.703, lr: 0.10987421583949403\nepoch: 9500, acc: 0.733, loss: 0.701, lr: 0.10480483196484092\nepoch: 9600, acc: 0.733, loss: 0.700, lr: 0.09991939120299322\nepoch: 9700, acc: 0.737, loss: 0.699, lr: 0.09521408746491831\nepoch: 9800, acc: 0.737, loss: 0.698, lr: 0.09068502930857424\nepoch: 9900, acc: 0.737, loss: 0.697, lr: 0.08632825155189956\nepoch: 10000, acc: 0.737, loss: 0.695, lr: 0.08213972651597043\n"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "# Create dataset\n",
    "X, y = create_data(100, 3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 64)  # first dense layer, 2 inputs (each sample has 2 features), 3 outputs\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(decay=5e-8) # adjusted decay rate <----------\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    loss = loss_function.forward(activation2.output, y)\n",
    "\n",
    "    predictions = np.argmax(activation2.output, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f}, lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dvalues)\n",
    "    dense2.backward(activation2.dvalues)\n",
    "    activation1.backward(dense2.dvalues)\n",
    "    dense1.backward(activation1.dvalues)\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- decreasing the decay rate has yieled our highest accuracy and lowest loss thus far\n",
    "---\n",
    "- Stochastic Gradient Descent with learning rate decay can do fairly well, but remains **a basic method of optimization that only follows a gradient without any additional logic that could potentially help the model find the global minimum to the loss function**\n",
    "- one option for improving the SGD optimizer is to introduce **momentum** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent with Momentum\n",
    "- the idea behind momentum is to make a rolling average of gradients over some number of updates and use this average rather than the unique gradient at each step\n",
    "- imagine a ball rolling down a hill: even if it runs into a small hole or incline, momentum will allow it to **escape** such obstacles and keep rolling towards a lower minimum\n",
    "---\n",
    "- remember that the gradient points towards the current steepest descent for that step, which may not necessarily follow descent towards the global minimum, but point towards a local minimum instead\n",
    "- therefore, this step may decrease loss for that update, but may not get us out of the local minimum\n",
    "- we could end up with a gradient that points in one direction, and then the opposite direction in the next update\n",
    "- the gradient could continue to bounce back and forth around a local minimum, keeping the optimization of the loss stuck\n",
    "- SGD with momentum, however, **uses the previous update’s direction to influence the next update’s direction**, minimizing the chances of getting stuck around a local minimum\n",
    "---\n",
    "- we effectively using a portion of the gradient from preceding steps as our momentum (direction of previous changes) and a portion of the current gradient\n",
    "- together, these portions form the change to our parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.current_learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain momentum arrays, create ones \n",
    "        #filled with zeros\n",
    "        if not hasattr(layer, 'weight_momentums'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            # If there is no momentum array for weights\n",
    "            # The array doesn’t exist for biases yet either.\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "\n",
    "            # Build weight updates with momentum - take previous \n",
    "            # updates multiplied by retain factor and update with \n",
    "            # current gradients\n",
    "            weight_updates = (\n",
    "                (self.momentum * layer.weight_momentums) - \n",
    "                (self.current_learning_rate * layer.dweights)\n",
    "            )\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = (\n",
    "                (self.momentum * layer.bias_momentums) - \n",
    "                (self.current_learning_rate * layer.dbiases)\n",
    "            )\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = (-self.current_learning_rate *                      \n",
    "                              layer.dweights)\n",
    "            bias_updates = (-self.current_learning_rate * \n",
    "                            layer.dbiases)\n",
    "\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's demonstrate how adding momentum changes the learning process\n",
    "- we will keep the same initial **learning rate** (1) and **decay** (5e-8) from the previous training attempt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 0, acc: 0.360, loss: 1.099, lr: 1.0\nepoch: 100, acc: 0.477, loss: 1.044, lr: 0.9997525310359338\nepoch: 200, acc: 0.503, loss: 0.970, lr: 0.9990054981534312\nepoch: 300, acc: 0.570, loss: 0.855, lr: 0.997760023693318\nepoch: 400, acc: 0.663, loss: 0.717, lr: 0.9960179759343823\nepoch: 500, acc: 0.677, loss: 0.616, lr: 0.9937819644233231\nepoch: 600, acc: 0.727, loss: 0.542, lr: 0.9910553334608369\nepoch: 700, acc: 0.747, loss: 0.501, lr: 0.9878421537665991\nepoch: 800, acc: 0.743, loss: 0.484, lr: 0.9841472123522141\nepoch: 900, acc: 0.773, loss: 0.468, lr: 0.9799760006374122\nepoch: 1000, acc: 0.803, loss: 0.455, lr: 0.9753347008507738\nepoch: 1100, acc: 0.777, loss: 0.502, lr: 0.9702301707621199\nepoch: 1200, acc: 0.793, loss: 0.458, lr: 0.9646699267993539\nepoch: 1300, acc: 0.730, loss: 0.496, lr: 0.9586621256079148\nepoch: 1400, acc: 0.743, loss: 0.465, lr: 0.9522155441161774\nepoch: 1500, acc: 0.813, loss: 0.432, lr: 0.9453395581749566\nepoch: 1600, acc: 0.767, loss: 0.537, lr: 0.9380441198438796\nepoch: 1700, acc: 0.807, loss: 0.434, lr: 0.9303397334016029\nepoch: 1800, acc: 0.830, loss: 0.408, lr: 0.9222374301607807\nepoch: 1900, acc: 0.810, loss: 0.410, lr: 0.9137487421722358\nepoch: 2000, acc: 0.833, loss: 0.408, lr: 0.9048856749059981\nepoch: 2100, acc: 0.833, loss: 0.398, lr: 0.8956606789996725\nepoch: 2200, acc: 0.827, loss: 0.393, lr: 0.8860866211670557\nepoch: 2300, acc: 0.830, loss: 0.393, lr: 0.8761767543619547\nepoch: 2400, acc: 0.770, loss: 0.436, lr: 0.8659446872938026\nepoch: 2500, acc: 0.813, loss: 0.396, lr: 0.8554043533929261\nepoch: 2600, acc: 0.827, loss: 0.391, lr: 0.844569979324141\nepoch: 2700, acc: 0.823, loss: 0.389, lr: 0.833456053147818\nepoch: 2800, acc: 0.823, loss: 0.388, lr: 0.822077292227582\nepoch: 2900, acc: 0.823, loss: 0.387, lr: 0.8104486109834639\nepoch: 3000, acc: 0.823, loss: 0.387, lr: 0.7985850885885959\nepoch: 3100, acc: 0.823, loss: 0.386, lr: 0.7865019367063895\nepoch: 3200, acc: 0.823, loss: 0.386, lr: 0.7742144673636858\nepoch: 3300, acc: 0.823, loss: 0.385, lr: 0.7617380610534867\nepoch: 3400, acc: 0.823, loss: 0.385, lr: 0.7490881351587024\nepoch: 3500, acc: 0.827, loss: 0.385, lr: 0.7362801127858304\nepoch: 3600, acc: 0.823, loss: 0.384, lr: 0.7233293920946249\nepoch: 3700, acc: 0.823, loss: 0.384, lr: 0.7102513162067197\nepoch: 3800, acc: 0.823, loss: 0.384, lr: 0.6970611437727328\nepoch: 3900, acc: 0.827, loss: 0.384, lr: 0.6837740202737295\nepoch: 4000, acc: 0.823, loss: 0.383, lr: 0.6704049501290217\nepoch: 4100, acc: 0.823, loss: 0.383, lr: 0.6569687696781638\nepoch: 4200, acc: 0.823, loss: 0.383, lr: 0.6434801211006969\nepoch: 4300, acc: 0.823, loss: 0.383, lr: 0.6299534273327249\nepoch: 4400, acc: 0.823, loss: 0.383, lr: 0.6164028680347811\nepoch: 4500, acc: 0.823, loss: 0.383, lr: 0.6028423566606949\nepoch: 4600, acc: 0.823, loss: 0.382, lr: 0.589285518672321\nepoch: 4700, acc: 0.823, loss: 0.382, lr: 0.5757456709400948\nepoch: 4800, acc: 0.827, loss: 0.382, lr: 0.5622358023643838\nepoch: 4900, acc: 0.827, loss: 0.381, lr: 0.5487685557476036\nepoch: 5000, acc: 0.827, loss: 0.381, lr: 0.5353562109420904\nepoch: 5100, acc: 0.827, loss: 0.381, lr: 0.5220106692936887\nepoch: 5200, acc: 0.827, loss: 0.381, lr: 0.5087434393960976\nepoch: 5300, acc: 0.827, loss: 0.381, lr: 0.49556562416610983\nepoch: 5400, acc: 0.827, loss: 0.381, lr: 0.48248790924506524\nepoch: 5500, acc: 0.827, loss: 0.380, lr: 0.46952055272713383\nepoch: 5600, acc: 0.827, loss: 0.380, lr: 0.456673376210437\nepoch: 5700, acc: 0.827, loss: 0.380, lr: 0.4439557571625653\nepoch: 5800, acc: 0.827, loss: 0.380, lr: 0.43137662258773346\nepoch: 5900, acc: 0.827, loss: 0.380, lr: 0.4189444439786708\nepoch: 6000, acc: 0.827, loss: 0.380, lr: 0.40666723353238227\nepoch: 6100, acc: 0.827, loss: 0.380, lr: 0.3945525416051419\nepoch: 6200, acc: 0.827, loss: 0.380, lr: 0.3826074553785173\nepoch: 6300, acc: 0.827, loss: 0.380, lr: 0.37083859870486113\nepoch: 6400, acc: 0.827, loss: 0.380, lr: 0.35925213309759024\nepoch: 6500, acc: 0.827, loss: 0.380, lr: 0.34785375982865246\nepoch: 6600, acc: 0.827, loss: 0.380, lr: 0.3366487230929319\nepoch: 6700, acc: 0.827, loss: 0.380, lr: 0.32564181419691024\nepoch: 6800, acc: 0.827, loss: 0.380, lr: 0.31483737672672496\nepoch: 6900, acc: 0.827, loss: 0.380, lr: 0.304239312648824\nepoch: 7000, acc: 0.827, loss: 0.380, lr: 0.29385108929473275\nepoch: 7100, acc: 0.833, loss: 0.379, lr: 0.28367574718000566\nepoch: 7200, acc: 0.833, loss: 0.379, lr: 0.2737159086062279\nepoch: 7300, acc: 0.833, loss: 0.379, lr: 0.26397378699399016\nepoch: 7400, acc: 0.830, loss: 0.379, lr: 0.2544511968940157\nepoch: 7500, acc: 0.830, loss: 0.379, lr: 0.24514956462315737\nepoch: 7600, acc: 0.830, loss: 0.379, lr: 0.23606993947169932\nepoch: 7700, acc: 0.830, loss: 0.379, lr: 0.22721300542836217\nepoch: 7800, acc: 0.830, loss: 0.379, lr: 0.2185790933695863\nepoch: 7900, acc: 0.833, loss: 0.379, lr: 0.21016819366003006\nepoch: 8000, acc: 0.830, loss: 0.379, lr: 0.20197996911179583\nepoch: 8100, acc: 0.830, loss: 0.379, lr: 0.19401376825064212\nepoch: 8200, acc: 0.833, loss: 0.379, lr: 0.18626863883837028\nepoch: 8300, acc: 0.830, loss: 0.379, lr: 0.17874334160165903\nepoch: 8400, acc: 0.830, loss: 0.378, lr: 0.17143636411886728\nepoch: 8500, acc: 0.830, loss: 0.378, lr: 0.16434593481770504\nepoch: 8600, acc: 0.833, loss: 0.378, lr: 0.1574700370381852\nepoch: 8700, acc: 0.833, loss: 0.378, lr: 0.15080642311689468\nepoch: 8800, acc: 0.833, loss: 0.378, lr: 0.14435262845036528\nepoch: 8900, acc: 0.833, loss: 0.378, lr: 0.13810598549714745\nepoch: 9000, acc: 0.833, loss: 0.378, lr: 0.1320636376801004\nepoch: 9100, acc: 0.833, loss: 0.378, lr: 0.1262225531523929\nepoch: 9200, acc: 0.833, loss: 0.378, lr: 0.1205795383927474\nepoch: 9300, acc: 0.833, loss: 0.378, lr: 0.11513125159754022\nepoch: 9400, acc: 0.833, loss: 0.378, lr: 0.10987421583949403\nepoch: 9500, acc: 0.833, loss: 0.378, lr: 0.10480483196484092\nepoch: 9600, acc: 0.833, loss: 0.378, lr: 0.09991939120299322\nepoch: 9700, acc: 0.833, loss: 0.378, lr: 0.09521408746491831\nepoch: 9800, acc: 0.833, loss: 0.378, lr: 0.09068502930857424\nepoch: 9900, acc: 0.833, loss: 0.378, lr: 0.08632825155189956\nepoch: 10000, acc: 0.833, loss: 0.378, lr: 0.08213972651597043\n"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "# Create dataset\n",
    "X, y = create_data(100, 3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 64)  # first dense layer, 2 inputs (each sample has 2 features), 3 outputs\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(decay=5e-8, momentum=0.9)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Make a forward pass of our training data thru this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Make a forward pass thru second Dense layer - it takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Calculate loss from output of activation2 so softmax activation\n",
    "    loss = loss_function.forward(activation2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    predictions = np.argmax(activation2.output, axis=1)  # calculate values along first axis\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f}, lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dvalues)\n",
    "    dense2.backward(activation2.dvalues)\n",
    "    activation1.backward(dense2.dvalues)\n",
    "    dense1.backward(activation1.dvalues)\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- by epoch 2000, the neural network reached ~83% accuracy, at which we should try a slower decay as change was stagnant from that point forward\n",
    "- without momentum, a decay of 1e-8 performed worse than 5e-8, but let's try 1e-8 with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 0, acc: 0.360, loss: 1.099, lr: 1.0\nepoch: 100, acc: 0.453, loss: 1.044, lr: 0.9999505012415218\nepoch: 200, acc: 0.533, loss: 0.948, lr: 0.9998010199314954\nepoch: 300, acc: 0.627, loss: 0.786, lr: 0.9995516010086423\nepoch: 400, acc: 0.550, loss: 0.901, lr: 0.9992023193791428\nepoch: 500, acc: 0.670, loss: 0.711, lr: 0.9987532798791473\nepoch: 600, acc: 0.850, loss: 0.431, lr: 0.998204617222333\nepoch: 700, acc: 0.877, loss: 0.344, lr: 0.99755649593255\nepoch: 800, acc: 0.767, loss: 0.504, lr: 0.9968091102615951\nepoch: 900, acc: 0.853, loss: 0.397, lr: 0.9959626840921862\nepoch: 1000, acc: 0.850, loss: 0.374, lr: 0.9950174708261916\nepoch: 1100, acc: 0.847, loss: 0.364, lr: 0.993973753258195\nepoch: 1200, acc: 0.857, loss: 0.358, lr: 0.9928318434344862\nepoch: 1300, acc: 0.847, loss: 0.353, lr: 0.9915920824975807\nepoch: 1400, acc: 0.847, loss: 0.347, lr: 0.9902548405163678\nepoch: 1500, acc: 0.850, loss: 0.344, lr: 0.9888205163020066\nepoch: 1600, acc: 0.847, loss: 0.342, lr: 0.9872895372097089\nepoch: 1700, acc: 0.847, loss: 0.339, lr: 0.985662358926536\nepoch: 1800, acc: 0.850, loss: 0.338, lr: 0.983939465245355\nepoch: 1900, acc: 0.850, loss: 0.336, lr: 0.9821213678251238\nepoch: 2000, acc: 0.850, loss: 0.335, lr: 0.9802086059376619\nepoch: 2100, acc: 0.850, loss: 0.334, lr: 0.9782017462010811\nepoch: 2200, acc: 0.853, loss: 0.332, lr: 0.976101382300068\nepoch: 2300, acc: 0.850, loss: 0.331, lr: 0.9739081346932065\nepoch: 2400, acc: 0.850, loss: 0.331, lr: 0.9716226503075411\nepoch: 2500, acc: 0.853, loss: 0.329, lr: 0.9692456022205971\nepoch: 2600, acc: 0.853, loss: 0.328, lr: 0.9667776893300709\nepoch: 2700, acc: 0.853, loss: 0.326, lr: 0.9642196360114177\nepoch: 2800, acc: 0.847, loss: 0.325, lr: 0.9615721917635801\nepoch: 2900, acc: 0.847, loss: 0.323, lr: 0.9588361308430841\nepoch: 3000, acc: 0.853, loss: 0.322, lr: 0.9560122518867674\nepoch: 3100, acc: 0.850, loss: 0.320, lr: 0.9531013775233921\nepoch: 3200, acc: 0.853, loss: 0.319, lr: 0.9501043539743993\nepoch: 3300, acc: 0.857, loss: 0.317, lr: 0.9470220506440895\nepoch: 3400, acc: 0.867, loss: 0.315, lr: 0.9438553596994974\nepoch: 3500, acc: 0.863, loss: 0.314, lr: 0.9406051956402495\nepoch: 3600, acc: 0.863, loss: 0.313, lr: 0.9372724948586864\nepoch: 3700, acc: 0.863, loss: 0.313, lr: 0.9338582151905644\nepoch: 3800, acc: 0.860, loss: 0.312, lr: 0.9303633354566125\nepoch: 3900, acc: 0.867, loss: 0.312, lr: 0.9267888549952745\nepoch: 4000, acc: 0.863, loss: 0.312, lr: 0.9231357931869363\nepoch: 4100, acc: 0.867, loss: 0.311, lr: 0.9194051889699576\nepoch: 4200, acc: 0.860, loss: 0.312, lr: 0.915598100348825\nepoch: 4300, acc: 0.860, loss: 0.310, lr: 0.9117156038947642\nepoch: 4400, acc: 0.867, loss: 0.310, lr: 0.9077587942391291\nepoch: 4500, acc: 0.870, loss: 0.309, lr: 0.9037287835599044\nepoch: 4600, acc: 0.860, loss: 0.310, lr: 0.8996267010616601\nepoch: 4700, acc: 0.863, loss: 0.309, lr: 0.8954536924492963\nepoch: 4800, acc: 0.863, loss: 0.308, lr: 0.8912109193959191\nepoch: 4900, acc: 0.860, loss: 0.308, lr: 0.8868995590051997\nepoch: 5000, acc: 0.867, loss: 0.307, lr: 0.8825208032685533\nepoch: 5100, acc: 0.860, loss: 0.307, lr: 0.8780758585174951\nepoch: 5200, acc: 0.867, loss: 0.307, lr: 0.8735659448715251\nepoch: 5300, acc: 0.867, loss: 0.307, lr: 0.8689922956818875\nepoch: 5400, acc: 0.860, loss: 0.306, lr: 0.8643561569715662\nepoch: 5500, acc: 0.863, loss: 0.306, lr: 0.8596587868718629\nepoch: 5600, acc: 0.860, loss: 0.306, lr: 0.85490145505592\nepoch: 5700, acc: 0.863, loss: 0.305, lr: 0.8500854421695384\nepoch: 5800, acc: 0.863, loss: 0.305, lr: 0.8452120392596508\nepoch: 5900, acc: 0.863, loss: 0.305, lr: 0.8402825472007978\nepoch: 6000, acc: 0.863, loss: 0.305, lr: 0.8352982761199695\nepoch: 6100, acc: 0.863, loss: 0.305, lr: 0.8302605448201584\nepoch: 6200, acc: 0.863, loss: 0.304, lr: 0.82517068020298\nepoch: 6300, acc: 0.863, loss: 0.305, lr: 0.8200300166907178\nepoch: 6400, acc: 0.863, loss: 0.304, lr: 0.8148398956481256\nepoch: 6500, acc: 0.860, loss: 0.304, lr: 0.8096016648043511\nepoch: 6600, acc: 0.863, loss: 0.304, lr: 0.8043166776753177\nepoch: 6700, acc: 0.863, loss: 0.303, lr: 0.7989862929869058\nepoch: 6800, acc: 0.860, loss: 0.303, lr: 0.7936118740992796\nepoch: 6900, acc: 0.860, loss: 0.303, lr: 0.788194788432691\nepoch: 7000, acc: 0.860, loss: 0.303, lr: 0.7827364068950939\nepoch: 7100, acc: 0.860, loss: 0.303, lr: 0.7772381033119045\nepoch: 7200, acc: 0.863, loss: 0.302, lr: 0.7717012538582239\nepoch: 7300, acc: 0.860, loss: 0.302, lr: 0.7661272364938597\nepoch: 7400, acc: 0.860, loss: 0.302, lr: 0.7605174304014501\nepoch: 7500, acc: 0.863, loss: 0.302, lr: 0.7548732154280102\nepoch: 7600, acc: 0.860, loss: 0.302, lr: 0.7491959715302158\nepoch: 7700, acc: 0.860, loss: 0.302, lr: 0.7434870782237198\nepoch: 7800, acc: 0.860, loss: 0.302, lr: 0.7377479140368003\nepoch: 7900, acc: 0.860, loss: 0.302, lr: 0.7319798559686449\nepoch: 8000, acc: 0.860, loss: 0.301, lr: 0.7261842789525443\nepoch: 8100, acc: 0.860, loss: 0.301, lr: 0.7203625553242963\nepoch: 8200, acc: 0.860, loss: 0.301, lr: 0.7145160542960759\nepoch: 8300, acc: 0.863, loss: 0.301, lr: 0.7086461414360616\nepoch: 8400, acc: 0.863, loss: 0.301, lr: 0.7027541781540725\nepoch: 8500, acc: 0.863, loss: 0.301, lr: 0.6968415211934755\nepoch: 8600, acc: 0.863, loss: 0.301, lr: 0.6909095221296203\nepoch: 8700, acc: 0.860, loss: 0.301, lr: 0.6849595268750338\nepoch: 8800, acc: 0.863, loss: 0.301, lr: 0.6789928751916383\nepoch: 8900, acc: 0.863, loss: 0.301, lr: 0.6730109002101919\nepoch: 9000, acc: 0.863, loss: 0.300, lr: 0.6670149279572052\nepoch: 9100, acc: 0.860, loss: 0.300, lr: 0.6610062768895324\nepoch: 9200, acc: 0.860, loss: 0.300, lr: 0.6549862574368576\nepoch: 9300, acc: 0.863, loss: 0.300, lr: 0.6489561715522751\nepoch: 9400, acc: 0.863, loss: 0.300, lr: 0.6429173122711604\nepoch: 9500, acc: 0.863, loss: 0.300, lr: 0.6368709632785172\nepoch: 9600, acc: 0.863, loss: 0.300, lr: 0.6308183984849866\nepoch: 9700, acc: 0.863, loss: 0.300, lr: 0.6247608816116815\nepoch: 9800, acc: 0.863, loss: 0.301, lr: 0.6186996657840249\nepoch: 9900, acc: 0.863, loss: 0.301, lr: 0.6126359931347328\nepoch: 10000, acc: 0.863, loss: 0.300, lr: 0.6065710944161026\n"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "# Create dataset\n",
    "X, y = create_data(100, 3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 64)  # first dense layer, 2 inputs (each sample has 2 features), 3 outputs\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(decay=1e-8, momentum=0.9)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Make a forward pass of our training data thru this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Make a forward pass thru second Dense layer - it takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Calculate loss from output of activation2 so softmax activation\n",
    "    loss = loss_function.forward(activation2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    predictions = np.argmax(activation2.output, axis=1)  # calculate values along first axis\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f}, lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dvalues)\n",
    "    dense2.backward(activation2.dvalues)\n",
    "    activation1.backward(dense2.dvalues)\n",
    "    dense1.backward(activation1.dvalues)\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can derive better results with more tweaking, but this is a decent example of how momentum can be useful by allowing us to use a slower rate, but still decreasing loss over local minimas\n",
    "- the next modification to Stochasic Gradient Descent is **AdaGrad**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad (adaptive learning rate)\n",
    "- short for adaptive gradient\n",
    "- AdaGrad **introduces a per-parameter learning rate** instead of a globally-shared learning rate\n",
    "- with AdaGrad, we allow the model to value certain features higher than others\n",
    "- with our generated dataset, all features hold roughly the same value, but AdaGrad can provide significant improvements for feature sets where certain features have varying degrees of rarity or are more informative than others\n",
    "- the concept of AdaGrad can be depicted in short, simple code:\n",
    "---\n",
    "`cache += parm_gradient ** 2`\n",
    "\n",
    "`parm_updates = learning_rate * parm_gradient / (sqrt(cache) + eps)`\n",
    "\n",
    "---\n",
    "- `cache` holds a history of squared gradinets and `parm_updates` is a function of the learning rate multiplied by the `parm_gradient` (basic SGD at the moment), then we divide this by the square root of the `cache` plus some epsilon value\n",
    "- epsilon is a hyperparameter (pre-training control knob setting), which we'll default to 1e-7 (usually a small number)\n",
    "---\n",
    "- **parameters** in a neural network are weights and biases that are being \"learned\" by the network during training\n",
    "- **hyperparameters** are all the settings, like the number of neurons per layer, the learning rate, the decaying factor, the activation function(s), $l1/l2$ normalization amounts, and more\n",
    "---\n",
    "- overall, the impact is the learning rates for more **sparse** parameters are increased, while the less sparse parameters are decreased\n",
    "- to implement AdaGrad, we'll start by copying and pasting our `Optimizer_SGD()` class\n",
    "- then we'll change the name\n",
    "- then we'll add `self.epsilon` to the `__init__()` method\n",
    "- then we'll remove the momentum code from the `update_params()` method and replace it with AdaGrad code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adagrad:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.current_learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create ones filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now let's see the AdaGrad optimizer in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 0, acc: 0.360, loss: 1.099, lr: 1.0\nepoch: 100, acc: 0.493, loss: 1.012, lr: 0.9999505012415218\nepoch: 200, acc: 0.510, loss: 0.939, lr: 0.9998010199314954\nepoch: 300, acc: 0.573, loss: 0.892, lr: 0.9995516010086423\nepoch: 400, acc: 0.600, loss: 0.855, lr: 0.9992023193791428\nepoch: 500, acc: 0.617, loss: 0.802, lr: 0.9987532798791473\nepoch: 600, acc: 0.650, loss: 0.778, lr: 0.998204617222333\nepoch: 700, acc: 0.653, loss: 0.745, lr: 0.99755649593255\nepoch: 800, acc: 0.660, loss: 0.729, lr: 0.9968091102615951\nepoch: 900, acc: 0.670, loss: 0.703, lr: 0.9959626840921862\nepoch: 1000, acc: 0.690, loss: 0.665, lr: 0.9950174708261916\nepoch: 1100, acc: 0.707, loss: 0.663, lr: 0.993973753258195\nepoch: 1200, acc: 0.693, loss: 0.642, lr: 0.9928318434344862\nepoch: 1300, acc: 0.750, loss: 0.605, lr: 0.9915920824975807\nepoch: 1400, acc: 0.750, loss: 0.606, lr: 0.9902548405163678\nepoch: 1500, acc: 0.717, loss: 0.598, lr: 0.9888205163020066\nepoch: 1600, acc: 0.780, loss: 0.567, lr: 0.9872895372097089\nepoch: 1700, acc: 0.757, loss: 0.570, lr: 0.985662358926536\nepoch: 1800, acc: 0.763, loss: 0.564, lr: 0.983939465245355\nepoch: 1900, acc: 0.770, loss: 0.556, lr: 0.9821213678251238\nepoch: 2000, acc: 0.777, loss: 0.550, lr: 0.9802086059376619\nepoch: 2100, acc: 0.783, loss: 0.541, lr: 0.9782017462010811\nepoch: 2200, acc: 0.790, loss: 0.537, lr: 0.976101382300068\nepoch: 2300, acc: 0.793, loss: 0.530, lr: 0.9739081346932065\nepoch: 2400, acc: 0.793, loss: 0.527, lr: 0.9716226503075411\nepoch: 2500, acc: 0.800, loss: 0.520, lr: 0.9692456022205971\nepoch: 2600, acc: 0.807, loss: 0.511, lr: 0.9667776893300709\nepoch: 2700, acc: 0.807, loss: 0.507, lr: 0.9642196360114177\nepoch: 2800, acc: 0.807, loss: 0.504, lr: 0.9615721917635801\nepoch: 2900, acc: 0.807, loss: 0.498, lr: 0.9588361308430841\nepoch: 3000, acc: 0.807, loss: 0.493, lr: 0.9560122518867674\nepoch: 3100, acc: 0.803, loss: 0.490, lr: 0.9531013775233921\nepoch: 3200, acc: 0.803, loss: 0.485, lr: 0.9501043539743993\nepoch: 3300, acc: 0.803, loss: 0.480, lr: 0.9470220506440895\nepoch: 3400, acc: 0.807, loss: 0.476, lr: 0.9438553596994974\nepoch: 3500, acc: 0.807, loss: 0.472, lr: 0.9406051956402495\nepoch: 3600, acc: 0.810, loss: 0.469, lr: 0.9372724948586864\nepoch: 3700, acc: 0.813, loss: 0.465, lr: 0.9338582151905644\nepoch: 3800, acc: 0.817, loss: 0.461, lr: 0.9303633354566125\nepoch: 3900, acc: 0.817, loss: 0.458, lr: 0.9267888549952745\nepoch: 4000, acc: 0.827, loss: 0.455, lr: 0.9231357931869363\nepoch: 4100, acc: 0.833, loss: 0.452, lr: 0.9194051889699576\nepoch: 4200, acc: 0.830, loss: 0.452, lr: 0.915598100348825\nepoch: 4300, acc: 0.830, loss: 0.447, lr: 0.9117156038947642\nepoch: 4400, acc: 0.833, loss: 0.443, lr: 0.9077587942391291\nepoch: 4500, acc: 0.830, loss: 0.439, lr: 0.9037287835599044\nepoch: 4600, acc: 0.833, loss: 0.435, lr: 0.8996267010616601\nepoch: 4700, acc: 0.830, loss: 0.432, lr: 0.8954536924492963\nepoch: 4800, acc: 0.830, loss: 0.430, lr: 0.8912109193959191\nepoch: 4900, acc: 0.827, loss: 0.428, lr: 0.8868995590051997\nepoch: 5000, acc: 0.827, loss: 0.425, lr: 0.8825208032685533\nepoch: 5100, acc: 0.833, loss: 0.423, lr: 0.8780758585174951\nepoch: 5200, acc: 0.837, loss: 0.420, lr: 0.8735659448715251\nepoch: 5300, acc: 0.833, loss: 0.418, lr: 0.8689922956818875\nepoch: 5400, acc: 0.830, loss: 0.416, lr: 0.8643561569715662\nepoch: 5500, acc: 0.837, loss: 0.414, lr: 0.8596587868718629\nepoch: 5600, acc: 0.837, loss: 0.410, lr: 0.85490145505592\nepoch: 5700, acc: 0.837, loss: 0.407, lr: 0.8500854421695384\nepoch: 5800, acc: 0.840, loss: 0.405, lr: 0.8452120392596508\nepoch: 5900, acc: 0.843, loss: 0.403, lr: 0.8402825472007978\nepoch: 6000, acc: 0.843, loss: 0.401, lr: 0.8352982761199695\nepoch: 6100, acc: 0.820, loss: 0.390, lr: 0.8302605448201584\nepoch: 6200, acc: 0.827, loss: 0.381, lr: 0.82517068020298\nepoch: 6300, acc: 0.823, loss: 0.375, lr: 0.8200300166907178\nepoch: 6400, acc: 0.830, loss: 0.371, lr: 0.8148398956481256\nepoch: 6500, acc: 0.833, loss: 0.368, lr: 0.8096016648043511\nepoch: 6600, acc: 0.837, loss: 0.366, lr: 0.8043166776753177\nepoch: 6700, acc: 0.840, loss: 0.364, lr: 0.7989862929869058\nepoch: 6800, acc: 0.840, loss: 0.362, lr: 0.7936118740992796\nepoch: 6900, acc: 0.843, loss: 0.361, lr: 0.788194788432691\nepoch: 7000, acc: 0.847, loss: 0.359, lr: 0.7827364068950939\nepoch: 7100, acc: 0.847, loss: 0.358, lr: 0.7772381033119045\nepoch: 7200, acc: 0.863, loss: 0.356, lr: 0.7717012538582239\nepoch: 7300, acc: 0.863, loss: 0.354, lr: 0.7661272364938597\nepoch: 7400, acc: 0.863, loss: 0.352, lr: 0.7605174304014501\nepoch: 7500, acc: 0.863, loss: 0.351, lr: 0.7548732154280102\nepoch: 7600, acc: 0.863, loss: 0.349, lr: 0.7491959715302158\nepoch: 7700, acc: 0.863, loss: 0.347, lr: 0.7434870782237198\nepoch: 7800, acc: 0.863, loss: 0.346, lr: 0.7377479140368003\nepoch: 7900, acc: 0.867, loss: 0.345, lr: 0.7319798559686449\nepoch: 8000, acc: 0.867, loss: 0.343, lr: 0.7261842789525443\nepoch: 8100, acc: 0.870, loss: 0.342, lr: 0.7203625553242963\nepoch: 8200, acc: 0.870, loss: 0.341, lr: 0.7145160542960759\nepoch: 8300, acc: 0.870, loss: 0.340, lr: 0.7086461414360616\nepoch: 8400, acc: 0.867, loss: 0.341, lr: 0.7027541781540725\nepoch: 8500, acc: 0.873, loss: 0.339, lr: 0.6968415211934755\nepoch: 8600, acc: 0.873, loss: 0.336, lr: 0.6909095221296203\nepoch: 8700, acc: 0.873, loss: 0.335, lr: 0.6849595268750338\nepoch: 8800, acc: 0.873, loss: 0.334, lr: 0.6789928751916383\nepoch: 8900, acc: 0.873, loss: 0.333, lr: 0.6730109002101919\nepoch: 9000, acc: 0.880, loss: 0.332, lr: 0.6670149279572052\nepoch: 9100, acc: 0.880, loss: 0.331, lr: 0.6610062768895324\nepoch: 9200, acc: 0.880, loss: 0.329, lr: 0.6549862574368576\nepoch: 9300, acc: 0.880, loss: 0.328, lr: 0.6489561715522751\nepoch: 9400, acc: 0.880, loss: 0.326, lr: 0.6429173122711604\nepoch: 9500, acc: 0.880, loss: 0.324, lr: 0.6368709632785172\nepoch: 9600, acc: 0.877, loss: 0.322, lr: 0.6308183984849866\nepoch: 9700, acc: 0.873, loss: 0.318, lr: 0.6247608816116815\nepoch: 9800, acc: 0.873, loss: 0.315, lr: 0.6186996657840249\nepoch: 9900, acc: 0.877, loss: 0.311, lr: 0.6126359931347328\nepoch: 10000, acc: 0.880, loss: 0.308, lr: 0.6065710944161026\n"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "# Create dataset\n",
    "X, y = create_data(100, 3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 64)  # first dense layer, 2 inputs (each sample has 2 features), 3 outputs\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "#optimizer = Optimizer_SGD(decay=1e-8, momentum=0.9)\n",
    "optimizer = Optimizer_Adagrad(decay=1e-8)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Make a forward pass of our training data thru this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Make a forward pass thru second Dense layer - it takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Calculate loss from output of activation2 so softmax activation\n",
    "    loss = loss_function.forward(activation2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    predictions = np.argmax(activation2.output, axis=1)  # calculate values along first axis\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f}, lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dvalues)\n",
    "    dense2.backward(activation2.dvalues)\n",
    "    activation1.backward(dense2.dvalues)\n",
    "    dense1.backward(activation1.dvalues)\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AdaGrad worked well, and we can see that loss consistently fell throughout the entire training process\n",
    "- interestingly, SGD with momentum got near its final results in 1700 epochs, whereas AdaGrad took 3000 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp (adaptive learning rate)\n",
    "- continuing with modifcations of Stochastic Gradient Descent, let's meet RMSProp; short for Root Mean Squared Propagation\n",
    "- similar to AdaGrad, RMSProp calculates an adaptive learning rate per parameter, but it's calculated in a different way\n",
    "- AdaGrad calculates the cache (which holds a history of gradients) as: `cache += gradient ** 2`\n",
    "- RMSProp calculates the cache as: `cache += rho * cache + (1- rho) * gradient ** 2`\n",
    "- RMSProp is **similar to SGD with momentum**: RMSProp adds a mechanism similar to momentum, but also adds a per-parameter adaptive learning rate so the learning rate changes are smoother\n",
    "- instead of constantly adding to a cache (like in AdaGrad), RMSProp uses a moving average of the cache\n",
    "---\n",
    "- `rho` is the cache memory decay rate\n",
    "- a low learning rate of 0.001 (common default) is highly recommended for RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_RMSprop:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.current_learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create ones filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's change the optimizer used in our main neural network code and begin training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 0, acc: 0.360, loss: 1.099, lr: 0.001\nepoch: 100, acc: 0.417, loss: 1.077, lr: 0.0009999505012415217\nepoch: 200, acc: 0.457, loss: 1.072, lr: 0.0009998010199314946\nepoch: 300, acc: 0.473, loss: 1.062, lr: 0.0009995516010086409\nepoch: 400, acc: 0.483, loss: 1.050, lr: 0.0009992023193791406\nepoch: 500, acc: 0.483, loss: 1.035, lr: 0.0009987532798791455\nepoch: 600, acc: 0.497, loss: 1.019, lr: 0.0009982046172223313\nepoch: 700, acc: 0.540, loss: 1.003, lr: 0.0009975564959325478\nepoch: 800, acc: 0.580, loss: 0.987, lr: 0.0009968091102615926\nepoch: 900, acc: 0.597, loss: 0.972, lr: 0.0009959626840921837\nepoch: 1000, acc: 0.597, loss: 0.958, lr: 0.0009950174708261896\nepoch: 1100, acc: 0.597, loss: 0.945, lr: 0.000993973753258193\nepoch: 1200, acc: 0.613, loss: 0.933, lr: 0.0009928318434344848\nepoch: 1300, acc: 0.633, loss: 0.922, lr: 0.0009915920824975796\nepoch: 1400, acc: 0.633, loss: 0.912, lr: 0.0009902548405163651\nepoch: 1500, acc: 0.643, loss: 0.903, lr: 0.0009888205163020038\nepoch: 1600, acc: 0.630, loss: 0.895, lr: 0.0009872895372097059\nepoch: 1700, acc: 0.630, loss: 0.888, lr: 0.0009856623589265339\nepoch: 1800, acc: 0.637, loss: 0.881, lr: 0.0009839394652453532\nepoch: 1900, acc: 0.627, loss: 0.875, lr: 0.0009821213678251224\nepoch: 2000, acc: 0.633, loss: 0.870, lr: 0.0009802086059376594\nepoch: 2100, acc: 0.623, loss: 0.863, lr: 0.0009782017462010787\nepoch: 2200, acc: 0.637, loss: 0.857, lr: 0.0009761013823000658\nepoch: 2300, acc: 0.647, loss: 0.849, lr: 0.0009739081346932043\nepoch: 2400, acc: 0.667, loss: 0.842, lr: 0.0009716226503075388\nepoch: 2500, acc: 0.670, loss: 0.836, lr: 0.0009692456022205949\nepoch: 2600, acc: 0.680, loss: 0.830, lr: 0.0009667776893300685\nepoch: 2700, acc: 0.680, loss: 0.825, lr: 0.0009642196360114158\nepoch: 2800, acc: 0.673, loss: 0.820, lr: 0.0009615721917635784\nepoch: 2900, acc: 0.673, loss: 0.815, lr: 0.000958836130843082\nepoch: 3000, acc: 0.673, loss: 0.811, lr: 0.0009560122518867658\nepoch: 3100, acc: 0.673, loss: 0.808, lr: 0.0009531013775233906\nepoch: 3200, acc: 0.680, loss: 0.804, lr: 0.000950104353974398\nepoch: 3300, acc: 0.687, loss: 0.800, lr: 0.000947022050644088\nepoch: 3400, acc: 0.687, loss: 0.796, lr: 0.0009438553596994956\nepoch: 3500, acc: 0.673, loss: 0.792, lr: 0.0009406051956402466\nepoch: 3600, acc: 0.680, loss: 0.789, lr: 0.000937272494858684\nepoch: 3700, acc: 0.680, loss: 0.786, lr: 0.0009338582151905614\nepoch: 3800, acc: 0.687, loss: 0.783, lr: 0.0009303633354566098\nepoch: 3900, acc: 0.707, loss: 0.778, lr: 0.0009267888549952716\nepoch: 4000, acc: 0.700, loss: 0.775, lr: 0.000923135793186934\nepoch: 4100, acc: 0.693, loss: 0.773, lr: 0.0009194051889699553\nepoch: 4200, acc: 0.697, loss: 0.769, lr: 0.0009155981003488233\nepoch: 4300, acc: 0.687, loss: 0.767, lr: 0.0009117156038947624\nepoch: 4400, acc: 0.693, loss: 0.763, lr: 0.0009077587942391271\nepoch: 4500, acc: 0.690, loss: 0.761, lr: 0.0009037287835599016\nepoch: 4600, acc: 0.687, loss: 0.758, lr: 0.0008996267010616575\nepoch: 4700, acc: 0.703, loss: 0.754, lr: 0.0008954536924492935\nepoch: 4800, acc: 0.713, loss: 0.752, lr: 0.000891210919395916\nepoch: 4900, acc: 0.717, loss: 0.749, lr: 0.0008868995590051961\nepoch: 5000, acc: 0.707, loss: 0.746, lr: 0.0008825208032685497\nepoch: 5100, acc: 0.717, loss: 0.743, lr: 0.000878075858517491\nepoch: 5200, acc: 0.713, loss: 0.741, lr: 0.0008735659448715212\nepoch: 5300, acc: 0.713, loss: 0.738, lr: 0.0008689922956818844\nepoch: 5400, acc: 0.720, loss: 0.736, lr: 0.0008643561569715636\nepoch: 5500, acc: 0.730, loss: 0.733, lr: 0.0008596587868718603\nepoch: 5600, acc: 0.730, loss: 0.731, lr: 0.0008549014550559176\nepoch: 5700, acc: 0.730, loss: 0.729, lr: 0.0008500854421695354\nepoch: 5800, acc: 0.720, loss: 0.726, lr: 0.0008452120392596475\nepoch: 5900, acc: 0.710, loss: 0.724, lr: 0.0008402825472007941\nepoch: 6000, acc: 0.710, loss: 0.722, lr: 0.0008352982761199653\nepoch: 6100, acc: 0.717, loss: 0.719, lr: 0.0008302605448201535\nepoch: 6200, acc: 0.723, loss: 0.717, lr: 0.000825170680202975\nepoch: 6300, acc: 0.717, loss: 0.715, lr: 0.0008200300166907126\nepoch: 6400, acc: 0.720, loss: 0.712, lr: 0.0008148398956481206\nepoch: 6500, acc: 0.720, loss: 0.710, lr: 0.0008096016648043457\nepoch: 6600, acc: 0.710, loss: 0.708, lr: 0.0008043166776753116\nepoch: 6700, acc: 0.710, loss: 0.705, lr: 0.0007989862929868997\nepoch: 6800, acc: 0.713, loss: 0.702, lr: 0.0007936118740992737\nepoch: 6900, acc: 0.717, loss: 0.700, lr: 0.0007881947884326854\nepoch: 7000, acc: 0.717, loss: 0.698, lr: 0.0007827364068950887\nepoch: 7100, acc: 0.720, loss: 0.695, lr: 0.0007772381033118986\nepoch: 7200, acc: 0.717, loss: 0.693, lr: 0.0007717012538582183\nepoch: 7300, acc: 0.727, loss: 0.691, lr: 0.0007661272364938547\nepoch: 7400, acc: 0.737, loss: 0.689, lr: 0.0007605174304014453\nepoch: 7500, acc: 0.733, loss: 0.686, lr: 0.0007548732154280046\nepoch: 7600, acc: 0.733, loss: 0.684, lr: 0.0007491959715302105\nepoch: 7700, acc: 0.737, loss: 0.682, lr: 0.0007434870782237143\nepoch: 7800, acc: 0.733, loss: 0.680, lr: 0.000737747914036795\nepoch: 7900, acc: 0.730, loss: 0.678, lr: 0.0007319798559686387\nepoch: 8000, acc: 0.733, loss: 0.676, lr: 0.0007261842789525386\nepoch: 8100, acc: 0.730, loss: 0.674, lr: 0.0007203625553242909\nepoch: 8200, acc: 0.730, loss: 0.672, lr: 0.000714516054296071\nepoch: 8300, acc: 0.737, loss: 0.671, lr: 0.0007086461414360566\nepoch: 8400, acc: 0.730, loss: 0.669, lr: 0.0007027541781540674\nepoch: 8500, acc: 0.737, loss: 0.667, lr: 0.0006968415211934707\nepoch: 8600, acc: 0.730, loss: 0.665, lr: 0.0006909095221296151\nepoch: 8700, acc: 0.730, loss: 0.664, lr: 0.0006849595268750291\nepoch: 8800, acc: 0.733, loss: 0.662, lr: 0.0006789928751916338\nepoch: 8900, acc: 0.730, loss: 0.660, lr: 0.0006730109002101874\nepoch: 9000, acc: 0.730, loss: 0.658, lr: 0.0006670149279572009\nepoch: 9100, acc: 0.733, loss: 0.657, lr: 0.0006610062768895281\nepoch: 9200, acc: 0.727, loss: 0.655, lr: 0.0006549862574368526\nepoch: 9300, acc: 0.733, loss: 0.654, lr: 0.0006489561715522696\nepoch: 9400, acc: 0.730, loss: 0.652, lr: 0.000642917312271155\nepoch: 9500, acc: 0.737, loss: 0.651, lr: 0.0006368709632785117\nepoch: 9600, acc: 0.730, loss: 0.649, lr: 0.0006308183984849817\nepoch: 9700, acc: 0.733, loss: 0.648, lr: 0.0006247608816116766\nepoch: 9800, acc: 0.737, loss: 0.646, lr: 0.0006186996657840194\nepoch: 9900, acc: 0.733, loss: 0.645, lr: 0.0006126359931347275\nepoch: 10000, acc: 0.733, loss: 0.643, lr: 0.0006065710944160969\n"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "# Create dataset\n",
    "X, y = create_data(100, 3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 64)  # first dense layer, 2 inputs (each sample has 2 features), 3 outputs\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "#optimizer = Optimizer_SGD(decay=1e-8, momentum=0.9)\n",
    "optimizer = Optimizer_RMSprop(decay=1e-8) # decay is 0 by default, unlike the other hyperparameters, so we needed to set it here\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Make a forward pass of our training data thru this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Make a forward pass thru second Dense layer - it takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Calculate loss from output of activation2 so softmax activation\n",
    "    loss = loss_function.forward(activation2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    predictions = np.argmax(activation2.output, axis=1)  # calculate values along first axis\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f}, lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dvalues)\n",
    "    dense2.backward(activation2.dvalues)\n",
    "    activation1.backward(dense2.dvalues)\n",
    "    dense1.backward(activation1.dvalues)\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- these results are not great, but we can tweak the hyperparameters:\n",
    "- we'll increase `learning_rate` to 0.05, decrease `decay` to 4e-8, and decrease `rho` to 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 0, acc: 0.360, loss: 1.099, lr: 0.05\nepoch: 100, acc: 0.410, loss: 1.024, lr: 0.04999010099316666\nepoch: 200, acc: 0.427, loss: 0.982, lr: 0.04996021594198117\nepoch: 300, acc: 0.477, loss: 0.952, lr: 0.049910380770363756\nepoch: 400, acc: 0.537, loss: 0.899, lr: 0.04984065529821222\nepoch: 500, acc: 0.517, loss: 0.900, lr: 0.04975112312174363\nepoch: 600, acc: 0.563, loss: 0.849, lr: 0.04964189144647378\nepoch: 700, acc: 0.537, loss: 0.814, lr: 0.04951309087330241\nepoch: 800, acc: 0.557, loss: 0.798, lr: 0.04936487513830117\nepoch: 900, acc: 0.577, loss: 0.778, lr: 0.049197420806929945\nepoch: 1000, acc: 0.607, loss: 0.771, lr: 0.049010926923533135\nepoch: 1100, acc: 0.583, loss: 0.776, lr: 0.04880561461708983\nepoch: 1200, acc: 0.570, loss: 0.755, lr: 0.04858172666431087\nepoch: 1300, acc: 0.580, loss: 0.757, lr: 0.04833952701129078\nepoch: 1400, acc: 0.660, loss: 0.752, lr: 0.048079300255034645\nepoch: 1500, acc: 0.600, loss: 0.731, lr: 0.047801351086283965\nepoch: 1600, acc: 0.660, loss: 0.733, lr: 0.04750600369516955\nepoch: 1700, acc: 0.623, loss: 0.710, lr: 0.04719360114131276\nepoch: 1800, acc: 0.627, loss: 0.719, lr: 0.04686450469008847\nepoch: 1900, acc: 0.710, loss: 0.642, lr: 0.04651909311684583\nepoch: 2000, acc: 0.633, loss: 0.766, lr: 0.04615776198096219\nepoch: 2100, acc: 0.690, loss: 0.626, lr: 0.04578092287167548\nepoch: 2200, acc: 0.713, loss: 0.610, lr: 0.04538900262770674\nepoch: 2300, acc: 0.707, loss: 0.601, lr: 0.044982442532739884\nepoch: 2400, acc: 0.697, loss: 0.647, lr: 0.044561697488880066\nepoch: 2500, acc: 0.723, loss: 0.590, lr: 0.04412723517025177\nepoch: 2600, acc: 0.710, loss: 0.568, lr: 0.04367953515893745\nepoch: 2700, acc: 0.700, loss: 0.654, lr: 0.04321908806548398\nepoch: 2800, acc: 0.720, loss: 0.568, lr: 0.04274639463622801\nepoch: 2900, acc: 0.780, loss: 0.508, lr: 0.04226196484970326\nepoch: 3000, acc: 0.760, loss: 0.552, lr: 0.041766317004402864\nepoch: 3100, acc: 0.760, loss: 0.507, lr: 0.04125997680016567\nepoch: 3200, acc: 0.773, loss: 0.532, lr: 0.040743476415451944\nepoch: 3300, acc: 0.773, loss: 0.544, lr: 0.04021735358275496\nepoch: 3400, acc: 0.750, loss: 0.513, lr: 0.039682150664377865\nepoch: 3500, acc: 0.757, loss: 0.513, lr: 0.03913841373077229\nepoch: 3600, acc: 0.763, loss: 0.496, lr: 0.0385866916436036\nepoch: 3700, acc: 0.760, loss: 0.490, lr: 0.038027535145663746\nepoch: 3800, acc: 0.773, loss: 0.479, lr: 0.03746149595970692\nepoch: 3900, acc: 0.777, loss: 0.475, lr: 0.036889125898227404\nepoch: 4000, acc: 0.780, loss: 0.470, lr: 0.036310975986141955\nepoch: 4100, acc: 0.787, loss: 0.461, lr: 0.035727595598272634\nepoch: 4200, acc: 0.807, loss: 0.437, lr: 0.035139531613457196\nepoch: 4300, acc: 0.780, loss: 0.452, lr: 0.03454732758703956\nepoch: 4400, acc: 0.780, loss: 0.452, lr: 0.03395152294341337\nepoch: 4500, acc: 0.783, loss: 0.445, lr: 0.033352652190210344\nepoch: 4600, acc: 0.793, loss: 0.437, lr: 0.03275124415563605\nepoch: 4700, acc: 0.730, loss: 0.518, lr: 0.03214782125036875\nepoch: 4800, acc: 0.787, loss: 0.435, lr: 0.031542898755341726\nepoch: 4900, acc: 0.797, loss: 0.429, lr: 0.03093698413663571\nepoch: 5000, acc: 0.813, loss: 0.422, lr: 0.03033057638860928\nepoch: 5100, acc: 0.813, loss: 0.406, lr: 0.02972416540629706\nepoch: 5200, acc: 0.810, loss: 0.396, lr: 0.029118231388003988\nepoch: 5300, acc: 0.817, loss: 0.385, lr: 0.02851324426892212\nepoch: 5400, acc: 0.830, loss: 0.376, lr: 0.027909663186494987\nepoch: 5500, acc: 0.827, loss: 0.366, lr: 0.02730793597815105\nepoch: 5600, acc: 0.817, loss: 0.365, lr: 0.026708498711925766\nepoch: 5700, acc: 0.837, loss: 0.352, lr: 0.02611177525038971\nepoch: 5800, acc: 0.837, loss: 0.349, lr: 0.025518176848199268\nepoch: 5900, acc: 0.843, loss: 0.340, lr: 0.024928101783486208\nepoch: 6000, acc: 0.853, loss: 0.336, lr: 0.024341935023204163\nepoch: 6100, acc: 0.867, loss: 0.327, lr: 0.023760047922453176\nepoch: 6200, acc: 0.860, loss: 0.320, lr: 0.023182797957709732\nepoch: 6300, acc: 0.863, loss: 0.315, lr: 0.02261052849379656\nepoch: 6400, acc: 0.860, loss: 0.313, lr: 0.022043568584337918\nepoch: 6500, acc: 0.870, loss: 0.308, lr: 0.02148223280535957\nepoch: 6600, acc: 0.873, loss: 0.304, lr: 0.020926821121608864\nepoch: 6700, acc: 0.877, loss: 0.300, lr: 0.020377618785091218\nepoch: 6800, acc: 0.880, loss: 0.296, lr: 0.01983489626524244\nepoch: 6900, acc: 0.880, loss: 0.293, lr: 0.019298909210084057\nepoch: 7000, acc: 0.887, loss: 0.288, lr: 0.018769898437640452\nepoch: 7100, acc: 0.887, loss: 0.285, lr: 0.018248089956831925\nepoch: 7200, acc: 0.887, loss: 0.282, lr: 0.017733695016997276\nepoch: 7300, acc: 0.890, loss: 0.279, lr: 0.0172269101851442\nepoch: 7400, acc: 0.890, loss: 0.274, lr: 0.01672791744997348\nepoch: 7500, acc: 0.893, loss: 0.270, lr: 0.01623688435167627\nepoch: 7600, acc: 0.897, loss: 0.267, lr: 0.015753964136460814\nepoch: 7700, acc: 0.897, loss: 0.264, lr: 0.015279295934726943\nepoch: 7800, acc: 0.897, loss: 0.263, lr: 0.014813004961773056\nepoch: 7900, acc: 0.897, loss: 0.258, lr: 0.014355202739890894\nepoch: 8000, acc: 0.893, loss: 0.259, lr: 0.013905987340679144\nepoch: 8100, acc: 0.903, loss: 0.252, lr: 0.01346544364638616\nepoch: 8200, acc: 0.903, loss: 0.250, lr: 0.013033643629076799\nepoch: 8300, acc: 0.900, loss: 0.247, lr: 0.012610646646406166\nepoch: 8400, acc: 0.900, loss: 0.251, lr: 0.012196499752776046\nepoch: 8500, acc: 0.903, loss: 0.244, lr: 0.011791238024646385\nepoch: 8600, acc: 0.907, loss: 0.242, lr: 0.011394884898774778\nepoch: 8700, acc: 0.903, loss: 0.241, lr: 0.011007452522161381\nepoch: 8800, acc: 0.903, loss: 0.240, lr: 0.010628942112485084\nepoch: 8900, acc: 0.907, loss: 0.241, lr: 0.010259344327828086\nepoch: 9000, acc: 0.907, loss: 0.232, lr: 0.009898639644501571\nepoch: 9100, acc: 0.910, loss: 0.231, lr: 0.009546798741803203\nepoch: 9200, acc: 0.907, loss: 0.233, lr: 0.009203782892558783\nepoch: 9300, acc: 0.910, loss: 0.230, lr: 0.008869544358324344\nepoch: 9400, acc: 0.913, loss: 0.229, lr: 0.008544026788152198\nepoch: 9500, acc: 0.910, loss: 0.227, lr: 0.008227165619853703\nepoch: 9600, acc: 0.910, loss: 0.225, lr: 0.007918888482723102\nepoch: 9700, acc: 0.907, loss: 0.224, lr: 0.007619115600720805\nepoch: 9800, acc: 0.910, loss: 0.223, lr: 0.007327760195150098\nepoch: 9900, acc: 0.910, loss: 0.223, lr: 0.007044728885898562\nepoch: 10000, acc: 0.913, loss: 0.221, lr: 0.0067699220903545895\n"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "# Create dataset\n",
    "X, y = create_data(100, 3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 64)  # first dense layer, 2 inputs (each sample has 2 features), 3 outputs\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "#optimizer = Optimizer_SGD(decay=1e-8, momentum=0.9)\n",
    "optimizer = Optimizer_RMSprop(learning_rate=0.05, decay=4e-8, rho=0.999) # adjusted hyperparameters\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Make a forward pass of our training data thru this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Make a forward pass thru second Dense layer - it takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Calculate loss from output of activation2 so softmax activation\n",
    "    loss = loss_function.forward(activation2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    predictions = np.argmax(activation2.output, axis=1)  # calculate values along first axis\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f}, lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dvalues)\n",
    "    dense2.backward(activation2.dvalues)\n",
    "    activation1.backward(dense2.dvalues)\n",
    "    dense1.backward(activation1.dvalues)\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- by adjusting our hyperparameter values for the RMSProp optimizer, our neural network just yielded its best results\n",
    "- we still have one more SGD modification to cover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam (adaptive learning rate)\n",
    "- short for Adaptive Moment\n",
    "- Adam is currently the most popular optimizer, and is built atop RMSProp with momentum added back in\n",
    "- with Adam, the `rho` hyperparameter becomes `beta_2` and the `momentum` hyperparameter becomes `beta_1`\n",
    "- to get parameter updates, we divide scaled momentum by scaled cache\n",
    "- this has the effect of significantly boosting weight updates during the first training steps, speeding up the whole process, then quickly returning its weight updates back to their more-typical values for the later training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adam:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.current_learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum  with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums /  (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        # Update cache with squared current gradients --> this is where the RMSProp flavor to Adam comes into play\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache +  (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "        # Get corrected cachebias\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    " \n",
    "        # Vanilla SGD parameter update + normalization with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's see how the Adam optimizer performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 0, acc: 0.360, loss: 1.099, lr: 0.001\nepoch: 100, acc: 0.427, loss: 1.077, lr: 0.0009998020198633338\nepoch: 200, acc: 0.430, loss: 1.071, lr: 0.0009992043188396232\nepoch: 300, acc: 0.463, loss: 1.060, lr: 0.0009982076154072756\nepoch: 400, acc: 0.480, loss: 1.043, lr: 0.0009968131059642454\nepoch: 500, acc: 0.513, loss: 1.018, lr: 0.0009950224624348744\nepoch: 600, acc: 0.583, loss: 0.988, lr: 0.0009928378289294775\nepoch: 700, acc: 0.617, loss: 0.957, lr: 0.0009902618174660494\nepoch: 800, acc: 0.643, loss: 0.926, lr: 0.0009872975027660248\nepoch: 900, acc: 0.650, loss: 0.896, lr: 0.0009839484161386\nepoch: 1000, acc: 0.670, loss: 0.866, lr: 0.000980218538470663\nepoch: 1100, acc: 0.677, loss: 0.837, lr: 0.0009761122923417968\nepoch: 1200, acc: 0.690, loss: 0.808, lr: 0.0009716345332862163\nepoch: 1300, acc: 0.683, loss: 0.781, lr: 0.0009667905402258149\nepoch: 1400, acc: 0.693, loss: 0.756, lr: 0.0009615860051006909\nepoch: 1500, acc: 0.697, loss: 0.734, lr: 0.0009560270217256772\nepoch: 1600, acc: 0.707, loss: 0.714, lr: 0.0009501200739033891\nepoch: 1700, acc: 0.713, loss: 0.697, lr: 0.0009438720228262535\nepoch: 1800, acc: 0.730, loss: 0.681, lr: 0.0009372900938017678\nepoch: 1900, acc: 0.737, loss: 0.665, lr: 0.0009303818623369144\nepoch: 2000, acc: 0.743, loss: 0.648, lr: 0.0009231552396192416\nepoch: 2100, acc: 0.743, loss: 0.634, lr: 0.000915618457433508\nepoch: 2200, acc: 0.750, loss: 0.620, lr: 0.0009077800525541336\nepoch: 2300, acc: 0.763, loss: 0.606, lr: 0.0008996488506547967\nepoch: 2400, acc: 0.780, loss: 0.594, lr: 0.0008912339497776001\nepoch: 2500, acc: 0.780, loss: 0.583, lr: 0.000882544703405034\nepoch: 2600, acc: 0.787, loss: 0.571, lr: 0.0008735907031787477\nepoch: 2700, acc: 0.783, loss: 0.559, lr: 0.0008643817613096789\nepoch: 2800, acc: 0.793, loss: 0.547, lr: 0.0008549278927245598\nepoch: 2900, acc: 0.797, loss: 0.537, lr: 0.0008452392969940644\nepoch: 3000, acc: 0.800, loss: 0.528, lr: 0.0008353263400880564\nepoch: 3100, acc: 0.797, loss: 0.519, lr: 0.0008251995360033131\nepoch: 3200, acc: 0.807, loss: 0.511, lr: 0.0008148695283090374\nepoch: 3300, acc: 0.807, loss: 0.503, lr: 0.0008043470716550972\nepoch: 3400, acc: 0.810, loss: 0.496, lr: 0.000793643013287555\nepoch: 3500, acc: 0.820, loss: 0.488, lr: 0.000782768274615444\nepoch: 3600, acc: 0.820, loss: 0.481, lr: 0.0007717338328720701\nepoch: 3700, acc: 0.820, loss: 0.473, lr: 0.000760550702913273\nepoch: 3800, acc: 0.820, loss: 0.467, lr: 0.0007492299191941366\nepoch: 3900, acc: 0.830, loss: 0.460, lr: 0.0007377825179645462\nepoch: 4000, acc: 0.833, loss: 0.454, lr: 0.0007262195197228369\nepoch: 4100, acc: 0.837, loss: 0.449, lr: 0.0007145519119654511\nepoch: 4200, acc: 0.833, loss: 0.443, lr: 0.0007027906322691422\nepoch: 4300, acc: 0.837, loss: 0.438, lr: 0.0006909465517407895\nepoch: 4400, acc: 0.837, loss: 0.433, lr: 0.0006790304588682663\nepoch: 4500, acc: 0.840, loss: 0.429, lr: 0.0006670530438042058\nepoch: 4600, acc: 0.840, loss: 0.424, lr: 0.0006550248831127193\nepoch: 4700, acc: 0.847, loss: 0.418, lr: 0.0006429564250073736\nepoch: 4800, acc: 0.847, loss: 0.413, lr: 0.0006308579751068334\nepoch: 4900, acc: 0.847, loss: 0.409, lr: 0.0006187396827327136\nepoch: 5000, acc: 0.843, loss: 0.404, lr: 0.0006066115277721854\nepoch: 5100, acc: 0.843, loss: 0.400, lr: 0.0005944833081259407\nepoch: 5200, acc: 0.843, loss: 0.396, lr: 0.000582364627760079\nepoch: 5300, acc: 0.847, loss: 0.393, lr: 0.0005702648853784415\nepoch: 5400, acc: 0.847, loss: 0.389, lr: 0.0005581932637298994\nepoch: 5500, acc: 0.843, loss: 0.385, lr: 0.0005461587195630211\nepoch: 5600, acc: 0.843, loss: 0.382, lr: 0.0005341699742385153\nepoch: 5700, acc: 0.850, loss: 0.378, lr: 0.000522235505007794\nepoch: 5800, acc: 0.850, loss: 0.375, lr: 0.0005103635369639854\nepoch: 5900, acc: 0.853, loss: 0.372, lr: 0.0004985620356697248\nepoch: 6000, acc: 0.853, loss: 0.369, lr: 0.00048683870046408405\nepoch: 6100, acc: 0.857, loss: 0.367, lr: 0.0004752009584490646\nepoch: 6200, acc: 0.857, loss: 0.364, lr: 0.00046365595915419573\nepoch: 6300, acc: 0.857, loss: 0.362, lr: 0.0004522105698759322\nepoch: 6400, acc: 0.860, loss: 0.359, lr: 0.0004408713716867598\nepoch: 6500, acc: 0.863, loss: 0.357, lr: 0.0004296446561071928\nepoch: 6600, acc: 0.863, loss: 0.354, lr: 0.0004185364224321787\nepoch: 6700, acc: 0.860, loss: 0.352, lr: 0.00040755237570182614\nepoch: 6800, acc: 0.860, loss: 0.350, lr: 0.0003966979253048506\nepoch: 6900, acc: 0.863, loss: 0.348, lr: 0.0003859781842016828\nepoch: 7000, acc: 0.863, loss: 0.347, lr: 0.0003753979687528109\nepoch: 7100, acc: 0.867, loss: 0.345, lr: 0.00036496179913664023\nepoch: 7200, acc: 0.867, loss: 0.343, lr: 0.0003546739003399473\nepoch: 7300, acc: 0.870, loss: 0.340, lr: 0.000344538203702886\nepoch: 7400, acc: 0.877, loss: 0.338, lr: 0.00033455834899947184\nepoch: 7500, acc: 0.877, loss: 0.336, lr: 0.0003247376870335274\nepoch: 7600, acc: 0.877, loss: 0.335, lr: 0.00031507928272921814\nepoch: 7700, acc: 0.877, loss: 0.333, lr: 0.0003055859186945409\nepoch: 7800, acc: 0.877, loss: 0.332, lr: 0.0002962600992354632\nepoch: 7900, acc: 0.877, loss: 0.330, lr: 0.0002871040547978201\nepoch: 8000, acc: 0.873, loss: 0.329, lr: 0.0002781197468135849\nepoch: 8100, acc: 0.873, loss: 0.328, lr: 0.0002693088729277253\nepoch: 8200, acc: 0.873, loss: 0.326, lr: 0.0002606728725815379\nepoch: 8300, acc: 0.873, loss: 0.325, lr: 0.0002522129329281255\nepoch: 8400, acc: 0.877, loss: 0.324, lr: 0.00024392999505552313\nepoch: 8500, acc: 0.880, loss: 0.323, lr: 0.00023582476049292998\nepoch: 8600, acc: 0.880, loss: 0.322, lr: 0.00022789769797549762\nepoch: 8700, acc: 0.880, loss: 0.321, lr: 0.00022014905044322964\nepoch: 8800, acc: 0.880, loss: 0.320, lr: 0.0002125788422497036\nepoch: 8900, acc: 0.880, loss: 0.319, lr: 0.00020518688655656348\nepoch: 9000, acc: 0.880, loss: 0.318, lr: 0.00019797279289003312\nepoch: 9100, acc: 0.880, loss: 0.316, lr: 0.00019093597483606583\nepoch: 9200, acc: 0.883, loss: 0.315, lr: 0.0001840756578511773\nepoch: 9300, acc: 0.887, loss: 0.314, lr: 0.0001773908871664884\nepoch: 9400, acc: 0.890, loss: 0.313, lr: 0.0001708805357630455\nepoch: 9500, acc: 0.890, loss: 0.312, lr: 0.0001645433123970755\nepoch: 9600, acc: 0.890, loss: 0.311, lr: 0.00015837776965446344\nepoch: 9700, acc: 0.890, loss: 0.310, lr: 0.00015238231201441735\nepoch: 9800, acc: 0.890, loss: 0.309, lr: 0.00014655520390300316\nepoch: 9900, acc: 0.893, loss: 0.308, lr: 0.00014089457771797233\nepoch: 10000, acc: 0.893, loss: 0.307, lr: 0.00013539844180709284\n"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Create dataset\n",
    "X, y = create_data(100, 3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 64)  # first dense layer, 2 inputs (each sample has 2 features), 3 outputs\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "#optimizer = Optimizer_SGD(decay=1e-8, momentum=0.9)\n",
    "optimizer = Optimizer_Adam(decay=4e-8)\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Make a forward pass of our training data thru this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Make a forward pass thru second Dense layer - it takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Calculate loss from output of activation2 so softmax activation\n",
    "    loss = loss_function.forward(activation2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    predictions = np.argmax(activation2.output, axis=1)  # calculate values along first axis\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f}, lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dvalues)\n",
    "    dense2.backward(activation2.dvalues)\n",
    "    activation1.backward(dense2.dvalues)\n",
    "    dense1.backward(activation1.dvalues)\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- not too bad, but let’s increase the `learning_rate`  to 0.05 and change `decay` to 1e-8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 0, acc: 0.360, loss: 1.099, lr: 0.05\nepoch: 100, acc: 0.723, loss: 0.682, lr: 0.04999752506207612\nepoch: 200, acc: 0.800, loss: 0.512, lr: 0.049990050996574775\nepoch: 300, acc: 0.827, loss: 0.419, lr: 0.04997758005043209\nepoch: 400, acc: 0.863, loss: 0.356, lr: 0.04996011596895705\nepoch: 500, acc: 0.887, loss: 0.316, lr: 0.04993766399395728\nepoch: 600, acc: 0.903, loss: 0.284, lr: 0.04991023086111661\nepoch: 700, acc: 0.900, loss: 0.264, lr: 0.049877824796627425\nepoch: 800, acc: 0.920, loss: 0.242, lr: 0.0498404555130797\nepoch: 900, acc: 0.913, loss: 0.229, lr: 0.04979813420460921\nepoch: 1000, acc: 0.920, loss: 0.214, lr: 0.04975087354130951\nepoch: 1100, acc: 0.923, loss: 0.204, lr: 0.04969868766290968\nepoch: 1200, acc: 0.933, loss: 0.195, lr: 0.04964159217172425\nepoch: 1300, acc: 0.920, loss: 0.191, lr: 0.04957960412487896\nepoch: 1400, acc: 0.933, loss: 0.181, lr: 0.04951274202581829\nepoch: 1500, acc: 0.883, loss: 0.257, lr: 0.049441025815100244\nepoch: 1600, acc: 0.917, loss: 0.169, lr: 0.049364476860485375\nepoch: 1700, acc: 0.927, loss: 0.163, lr: 0.04928311794632677\nepoch: 1800, acc: 0.930, loss: 0.159, lr: 0.04919697326226773\nepoch: 1900, acc: 0.930, loss: 0.154, lr: 0.04910606839125617\nepoch: 2000, acc: 0.940, loss: 0.151, lr: 0.049010430296883054\nepoch: 2100, acc: 0.940, loss: 0.148, lr: 0.048910087310054\nepoch: 2200, acc: 0.940, loss: 0.145, lr: 0.04880506911500336\nepoch: 2300, acc: 0.943, loss: 0.142, lr: 0.048695406734660295\nepoch: 2400, acc: 0.947, loss: 0.139, lr: 0.048581132515377004\nepoch: 2500, acc: 0.947, loss: 0.137, lr: 0.048462280111029786\nepoch: 2600, acc: 0.947, loss: 0.134, lr: 0.04833888446650344\nepoch: 2700, acc: 0.950, loss: 0.132, lr: 0.0482109818005708\nepoch: 2800, acc: 0.950, loss: 0.129, lr: 0.048078609588178944\nepoch: 2900, acc: 0.953, loss: 0.127, lr: 0.04794180654215414\nepoch: 3000, acc: 0.947, loss: 0.125, lr: 0.047800612594338286\nepoch: 3100, acc: 0.950, loss: 0.122, lr: 0.0476550688761695\nepoch: 3200, acc: 0.953, loss: 0.123, lr: 0.0475052176987199\nepoch: 3300, acc: 0.767, loss: 0.749, lr: 0.0473511025322044\nepoch: 3400, acc: 0.953, loss: 0.120, lr: 0.047192767984974744\nepoch: 3500, acc: 0.953, loss: 0.117, lr: 0.047030259782012335\nepoch: 3600, acc: 0.953, loss: 0.116, lr: 0.04686362474293419\nepoch: 3700, acc: 0.953, loss: 0.115, lr: 0.046692910759528077\nepoch: 3800, acc: 0.953, loss: 0.114, lr: 0.04651816677283049\nepoch: 3900, acc: 0.953, loss: 0.113, lr: 0.046339442749763586\nepoch: 4000, acc: 0.957, loss: 0.111, lr: 0.04615678965934673\nepoch: 4100, acc: 0.957, loss: 0.110, lr: 0.04597025944849779\nepoch: 4200, acc: 0.957, loss: 0.109, lr: 0.045779905017441204\nepoch: 4300, acc: 0.957, loss: 0.108, lr: 0.04558578019473819\nepoch: 4400, acc: 0.957, loss: 0.107, lr: 0.04538793971195641\nepoch: 4500, acc: 0.957, loss: 0.106, lr: 0.04518643917799511\nepoch: 4600, acc: 0.960, loss: 0.105, lr: 0.04498133505308287\nepoch: 4700, acc: 0.960, loss: 0.104, lr: 0.044772684622464705\nepoch: 4800, acc: 0.960, loss: 0.103, lr: 0.04456054596979586\nepoch: 4900, acc: 0.960, loss: 0.102, lr: 0.04434497795025991\nepoch: 5000, acc: 0.967, loss: 0.102, lr: 0.04412604016342762\nepoch: 5100, acc: 0.963, loss: 0.100, lr: 0.04390379292587474\nepoch: 5200, acc: 0.970, loss: 0.101, lr: 0.043678297243576227\nepoch: 5300, acc: 0.963, loss: 0.097, lr: 0.04344961478409441\nepoch: 5400, acc: 0.967, loss: 0.099, lr: 0.04321780784857837\nepoch: 5500, acc: 0.970, loss: 0.100, lr: 0.042982939343593214\nepoch: 5600, acc: 0.970, loss: 0.099, lr: 0.042745072752796036\nepoch: 5700, acc: 0.970, loss: 0.097, lr: 0.04250427210847695\nepoch: 5800, acc: 0.953, loss: 0.104, lr: 0.04226060196298256\nepoch: 5900, acc: 0.970, loss: 0.092, lr: 0.04201412736003991\nepoch: 6000, acc: 0.970, loss: 0.091, lr: 0.041764913805998484\nepoch: 6100, acc: 0.970, loss: 0.091, lr: 0.041513027241007916\nepoch: 6200, acc: 0.967, loss: 0.089, lr: 0.04125853401014897\nepoch: 6300, acc: 0.967, loss: 0.096, lr: 0.04100150083453588\nepoch: 6400, acc: 0.963, loss: 0.090, lr: 0.040741994782406296\nepoch: 6500, acc: 0.973, loss: 0.088, lr: 0.040480083240217575\nepoch: 6600, acc: 0.970, loss: 0.088, lr: 0.04021583388376586\nepoch: 6700, acc: 0.963, loss: 0.090, lr: 0.03994931464934522\nepoch: 6800, acc: 0.963, loss: 0.090, lr: 0.03968059370496389\nepoch: 6900, acc: 0.970, loss: 0.089, lr: 0.03940973942163448\nepoch: 7000, acc: 0.973, loss: 0.084, lr: 0.03913682034475463\nepoch: 7100, acc: 0.980, loss: 0.082, lr: 0.03886190516559515\nepoch: 7200, acc: 0.970, loss: 0.083, lr: 0.03858506269291113\nepoch: 7300, acc: 0.973, loss: 0.082, lr: 0.03830636182469295\nepoch: 7400, acc: 0.977, loss: 0.080, lr: 0.03802587152007248\nepoch: 7500, acc: 0.980, loss: 0.080, lr: 0.03774366077140049\nepoch: 7600, acc: 0.973, loss: 0.080, lr: 0.037459798576510786\nepoch: 7700, acc: 0.970, loss: 0.081, lr: 0.03717435391118595\nepoch: 7800, acc: 0.980, loss: 0.078, lr: 0.03688739570184001\nepoch: 7900, acc: 0.980, loss: 0.077, lr: 0.0365989927984322\nepoch: 8000, acc: 0.977, loss: 0.078, lr: 0.03630921394762722\nepoch: 8100, acc: 0.980, loss: 0.077, lr: 0.0360181277662148\nepoch: 8200, acc: 0.980, loss: 0.076, lr: 0.03572580271480377\nepoch: 8300, acc: 0.980, loss: 0.075, lr: 0.035432307071803025\nepoch: 8400, acc: 0.980, loss: 0.075, lr: 0.03513770890770358\nepoch: 8500, acc: 0.970, loss: 0.076, lr: 0.034842076059673724\nepoch: 8600, acc: 0.980, loss: 0.075, lr: 0.034545476106480955\nepoch: 8700, acc: 0.983, loss: 0.073, lr: 0.03424797634375167\nepoch: 8800, acc: 0.977, loss: 0.074, lr: 0.03394964375958191\nepoch: 8900, acc: 0.980, loss: 0.072, lr: 0.0336505450105096\nepoch: 9000, acc: 0.970, loss: 0.075, lr: 0.03335074639786025\nepoch: 9100, acc: 0.980, loss: 0.072, lr: 0.033050313844476605\nepoch: 9200, acc: 0.977, loss: 0.073, lr: 0.03274931287184286\nepoch: 9300, acc: 0.980, loss: 0.070, lr: 0.03244780857761376\nepoch: 9400, acc: 0.980, loss: 0.070, lr: 0.03214586561355804\nepoch: 9500, acc: 0.980, loss: 0.069, lr: 0.03184354816392586\nepoch: 9600, acc: 0.980, loss: 0.069, lr: 0.03154091992424932\nepoch: 9700, acc: 0.980, loss: 0.069, lr: 0.031238044080584084\nepoch: 9800, acc: 0.983, loss: 0.068, lr: 0.03093498328920125\nepoch: 9900, acc: 0.983, loss: 0.068, lr: 0.030631799656736635\nepoch: 10000, acc: 0.980, loss: 0.067, lr: 0.030328554720805104\n"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Create dataset\n",
    "X, y = create_data(100, 3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 64)  # first dense layer, 2 inputs (each sample has 2 features), 3 outputs\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "#optimizer = Optimizer_SGD(decay=1e-8, momentum=0.9)\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, decay=1e-8)\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Make a forward pass of our training data thru this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Make a forward pass thru second Dense layer - it takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Calculate loss from output of activation2 so softmax activation\n",
    "    loss = loss_function.forward(activation2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    predictions = np.argmax(activation2.output, axis=1)  # calculate values along first axis\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f}, lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dvalues)\n",
    "    dense2.backward(activation2.dvalues)\n",
    "    activation1.backward(dense2.dvalues)\n",
    "    dense1.backward(activation1.dvalues)\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2) \n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it doesn't get much better than that, both for accuracy and loss\n",
    "- while Adam significantly outperformed the other optimizers, and is usually the best optimizer, that’s not always the case\n",
    "- it’s a good idea to try the Adam optimizer first, but it's also important to try others\n",
    "- sometimes simple SGD or SGD + momentum performs better than Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good Initial Hyperparameters\n",
    "- for SGD, a good initial learning rate is 1.0 with a decay of 0.1 \n",
    "- for Adam, a good initial learning rate is 0.001 (1e-3) with a decay of 0.0001 (1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluding Remarks\n",
    "- we achieved 98% accuracy and an almost perfect loss of 0 on the generated training section\n",
    "- this is certainly exciting, but you will soon learn to fear results this good, or at least to approach them with caution\n",
    "- there are cases in which you can achieve valid results of such a high degree, but, in this case, we’ve been ignoring a major concept in machine learning: out-of-sample testing data, which is the subject of the next section"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}