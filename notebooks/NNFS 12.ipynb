{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: Validation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in the optimization chapter, we used hyperparameter tuning to derive better results from the training data\n",
    "- to clarify, we should not check different hyperparameters using the test dataset; if we do that, we will introduce bias towards the testing data, which would severely dilute the meaningfulness of its intended purpose\n",
    "- this is why we did not perform backpropagation/update any weights/biases when we tested our model's accuracy on the newly generated testing data \n",
    "---\n",
    "- hyperparameter tuning can be performed using yet another dataset called **validation data**\n",
    "- the test data must contain real out-of-sample data, but with a validation dataset, we have more freedom \n",
    "- if we can afford to set aside some training for validation purposes, we can take it as an out-of-sample dataset, similar to the test dataset\n",
    "- now we can search for the optimal hyperparameters using the validation dataset and still be able to truly test our model at the end using the test data\n",
    "---\n",
    "- there are situations when we'll be short on data and cannot afford to create another dataset\n",
    "- in this situation, we can temporarily split the training data into a smaller training dataset and validation dataset for hyperparameter tuning\n",
    "- afterwards, with the final hyperparameter set, we can train the model on all the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "- another option for when we are short on data is to employ **cross-validation**\n",
    "- cross-validation is used when we have a small training dataset and cannot afford to spare any data for validation purposes \n",
    "- to perform cross-validation, we split the training dataset into a given number of parts, let's say 5\n",
    "- we then train the model on the first 4 chunks and validate on the last\n",
    "- we then swap samples: for example, if we have 5 chunks (chunks A, B, C, D, and E), we may first train on A, B, C and D, then validate on E, then, we can train on A, B, C, E, and validate on D, doing this until weâ€™ve performed validation on each of the 5 sample groups\n",
    "- this way, we do not lose any training data and we validate using the data that was not used for training during any given iteration\n",
    "- this validation method is often called **k-fold cross-validation**; here, our k is 5"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}