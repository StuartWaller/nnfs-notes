{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 0, acc: 0.360, loss: 1.099, lr: 0.05\nepoch: 100, acc: 0.723, loss: 0.682, lr: 0.04999752506207612\nepoch: 200, acc: 0.800, loss: 0.512, lr: 0.049990050996574775\nepoch: 300, acc: 0.827, loss: 0.419, lr: 0.04997758005043209\nepoch: 400, acc: 0.863, loss: 0.356, lr: 0.04996011596895705\nepoch: 500, acc: 0.887, loss: 0.316, lr: 0.04993766399395728\nepoch: 600, acc: 0.903, loss: 0.284, lr: 0.04991023086111661\nepoch: 700, acc: 0.900, loss: 0.264, lr: 0.049877824796627425\nepoch: 800, acc: 0.920, loss: 0.242, lr: 0.0498404555130797\nepoch: 900, acc: 0.913, loss: 0.229, lr: 0.04979813420460921\nepoch: 1000, acc: 0.920, loss: 0.214, lr: 0.04975087354130951\nepoch: 1100, acc: 0.923, loss: 0.204, lr: 0.04969868766290968\nepoch: 1200, acc: 0.933, loss: 0.195, lr: 0.04964159217172425\nepoch: 1300, acc: 0.920, loss: 0.191, lr: 0.04957960412487896\nepoch: 1400, acc: 0.933, loss: 0.181, lr: 0.04951274202581829\nepoch: 1500, acc: 0.883, loss: 0.257, lr: 0.049441025815100244\nepoch: 1600, acc: 0.917, loss: 0.169, lr: 0.049364476860485375\nepoch: 1700, acc: 0.927, loss: 0.163, lr: 0.04928311794632677\nepoch: 1800, acc: 0.930, loss: 0.159, lr: 0.04919697326226773\nepoch: 1900, acc: 0.930, loss: 0.154, lr: 0.04910606839125617\nepoch: 2000, acc: 0.940, loss: 0.151, lr: 0.049010430296883054\nepoch: 2100, acc: 0.940, loss: 0.148, lr: 0.048910087310054\nepoch: 2200, acc: 0.940, loss: 0.145, lr: 0.04880506911500336\nepoch: 2300, acc: 0.943, loss: 0.142, lr: 0.048695406734660295\nepoch: 2400, acc: 0.947, loss: 0.139, lr: 0.048581132515377004\nepoch: 2500, acc: 0.947, loss: 0.137, lr: 0.048462280111029786\nepoch: 2600, acc: 0.947, loss: 0.134, lr: 0.04833888446650344\nepoch: 2700, acc: 0.950, loss: 0.132, lr: 0.0482109818005708\nepoch: 2800, acc: 0.950, loss: 0.129, lr: 0.048078609588178944\nepoch: 2900, acc: 0.953, loss: 0.127, lr: 0.04794180654215414\nepoch: 3000, acc: 0.947, loss: 0.125, lr: 0.047800612594338286\nepoch: 3100, acc: 0.950, loss: 0.122, lr: 0.0476550688761695\nepoch: 3200, acc: 0.953, loss: 0.123, lr: 0.0475052176987199\nepoch: 3300, acc: 0.767, loss: 0.749, lr: 0.0473511025322044\nepoch: 3400, acc: 0.953, loss: 0.120, lr: 0.047192767984974744\nepoch: 3500, acc: 0.953, loss: 0.117, lr: 0.047030259782012335\nepoch: 3600, acc: 0.953, loss: 0.116, lr: 0.04686362474293419\nepoch: 3700, acc: 0.953, loss: 0.115, lr: 0.046692910759528077\nepoch: 3800, acc: 0.953, loss: 0.114, lr: 0.04651816677283049\nepoch: 3900, acc: 0.953, loss: 0.113, lr: 0.046339442749763586\nepoch: 4000, acc: 0.957, loss: 0.111, lr: 0.04615678965934673\nepoch: 4100, acc: 0.957, loss: 0.110, lr: 0.04597025944849779\nepoch: 4200, acc: 0.957, loss: 0.109, lr: 0.045779905017441204\nepoch: 4300, acc: 0.957, loss: 0.108, lr: 0.04558578019473819\nepoch: 4400, acc: 0.957, loss: 0.107, lr: 0.04538793971195641\nepoch: 4500, acc: 0.957, loss: 0.106, lr: 0.04518643917799511\nepoch: 4600, acc: 0.960, loss: 0.105, lr: 0.04498133505308287\nepoch: 4700, acc: 0.960, loss: 0.104, lr: 0.044772684622464705\nepoch: 4800, acc: 0.960, loss: 0.103, lr: 0.04456054596979586\nepoch: 4900, acc: 0.960, loss: 0.102, lr: 0.04434497795025991\nepoch: 5000, acc: 0.967, loss: 0.102, lr: 0.04412604016342762\nepoch: 5100, acc: 0.963, loss: 0.100, lr: 0.04390379292587474\nepoch: 5200, acc: 0.970, loss: 0.101, lr: 0.043678297243576227\nepoch: 5300, acc: 0.963, loss: 0.097, lr: 0.04344961478409441\nepoch: 5400, acc: 0.967, loss: 0.099, lr: 0.04321780784857837\nepoch: 5500, acc: 0.970, loss: 0.100, lr: 0.042982939343593214\nepoch: 5600, acc: 0.970, loss: 0.099, lr: 0.042745072752796036\nepoch: 5700, acc: 0.970, loss: 0.097, lr: 0.04250427210847695\nepoch: 5800, acc: 0.953, loss: 0.104, lr: 0.04226060196298256\nepoch: 5900, acc: 0.970, loss: 0.092, lr: 0.04201412736003991\nepoch: 6000, acc: 0.970, loss: 0.091, lr: 0.041764913805998484\nepoch: 6100, acc: 0.970, loss: 0.091, lr: 0.041513027241007916\nepoch: 6200, acc: 0.967, loss: 0.089, lr: 0.04125853401014897\nepoch: 6300, acc: 0.967, loss: 0.096, lr: 0.04100150083453588\nepoch: 6400, acc: 0.963, loss: 0.090, lr: 0.040741994782406296\nepoch: 6500, acc: 0.973, loss: 0.088, lr: 0.040480083240217575\nepoch: 6600, acc: 0.970, loss: 0.088, lr: 0.04021583388376586\nepoch: 6700, acc: 0.963, loss: 0.090, lr: 0.03994931464934522\nepoch: 6800, acc: 0.963, loss: 0.090, lr: 0.03968059370496389\nepoch: 6900, acc: 0.970, loss: 0.089, lr: 0.03940973942163448\nepoch: 7000, acc: 0.973, loss: 0.084, lr: 0.03913682034475463\nepoch: 7100, acc: 0.980, loss: 0.082, lr: 0.03886190516559515\nepoch: 7200, acc: 0.970, loss: 0.083, lr: 0.03858506269291113\nepoch: 7300, acc: 0.973, loss: 0.082, lr: 0.03830636182469295\nepoch: 7400, acc: 0.977, loss: 0.080, lr: 0.03802587152007248\nepoch: 7500, acc: 0.980, loss: 0.080, lr: 0.03774366077140049\nepoch: 7600, acc: 0.973, loss: 0.080, lr: 0.037459798576510786\nepoch: 7700, acc: 0.970, loss: 0.081, lr: 0.03717435391118595\nepoch: 7800, acc: 0.980, loss: 0.078, lr: 0.03688739570184001\nepoch: 7900, acc: 0.980, loss: 0.077, lr: 0.0365989927984322\nepoch: 8000, acc: 0.977, loss: 0.078, lr: 0.03630921394762722\nepoch: 8100, acc: 0.980, loss: 0.077, lr: 0.0360181277662148\nepoch: 8200, acc: 0.980, loss: 0.076, lr: 0.03572580271480377\nepoch: 8300, acc: 0.980, loss: 0.075, lr: 0.035432307071803025\nepoch: 8400, acc: 0.980, loss: 0.075, lr: 0.03513770890770358\nepoch: 8500, acc: 0.970, loss: 0.076, lr: 0.034842076059673724\nepoch: 8600, acc: 0.980, loss: 0.075, lr: 0.034545476106480955\nepoch: 8700, acc: 0.983, loss: 0.073, lr: 0.03424797634375167\nepoch: 8800, acc: 0.977, loss: 0.074, lr: 0.03394964375958191\nepoch: 8900, acc: 0.980, loss: 0.072, lr: 0.0336505450105096\nepoch: 9000, acc: 0.970, loss: 0.075, lr: 0.03335074639786025\nepoch: 9100, acc: 0.980, loss: 0.072, lr: 0.033050313844476605\nepoch: 9200, acc: 0.977, loss: 0.073, lr: 0.03274931287184286\nepoch: 9300, acc: 0.980, loss: 0.070, lr: 0.03244780857761376\nepoch: 9400, acc: 0.980, loss: 0.070, lr: 0.03214586561355804\nepoch: 9500, acc: 0.980, loss: 0.069, lr: 0.03184354816392586\nepoch: 9600, acc: 0.980, loss: 0.069, lr: 0.03154091992424932\nepoch: 9700, acc: 0.980, loss: 0.069, lr: 0.031238044080584084\nepoch: 9800, acc: 0.983, loss: 0.068, lr: 0.03093498328920125\nepoch: 9900, acc: 0.983, loss: 0.068, lr: 0.030631799656736635\nepoch: 10000, acc: 0.980, loss: 0.067, lr: 0.030328554720805104\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "# Our sample dataset\n",
    "def create_data(n, k):\n",
    "    X = np.zeros((n*k, 2))  # data matrix (each row = single example)\n",
    "    y = np.zeros(n*k, dtype='uint8')  # class labels\n",
    "    for j in range(k):\n",
    "        ix = range(n*j, n*(j+1))\n",
    "        r = np.linspace(0.0, 1, n)  # radius\n",
    "        t = np.linspace(j*4, (j+1)*4, n) + np.random.randn(n)*0.2  # theta\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = j\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, inputs, neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(inputs, neurons)\n",
    "        self.biases = np.zeros((1, neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from input ones, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dvalues = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from input ones\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable, \n",
    "        # let's make a copy of values first\n",
    "        self.dvalues = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative \n",
    "        self.dvalues[self.inputs <= 0] = 0 \n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        self.dvalues = dvalues.copy()\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = y_pred.shape[0]\n",
    "\n",
    "        # Probabilities for target values - only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_pred = y_pred[range(samples), y_true]\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(y_pred)\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        if len(y_true.shape) == 2:\n",
    "            negative_log_likelihoods *= y_true\n",
    "\n",
    "        # Overall loss\n",
    "        data_loss = np.sum(negative_log_likelihoods) / samples\n",
    "        return data_loss\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        samples = dvalues.shape[0]\n",
    "\n",
    "        self.dvalues = dvalues.copy()  # Copy so we can safely modify\n",
    "        self.dvalues[range(samples), y_true] -= 1\n",
    "        self.dvalues = self.dvalues / samples\n",
    "\n",
    "\n",
    "class Optimizer_Adam:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.current_learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum  with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums /  (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        # Update cache with squared current gradients --> this is where the RMSProp flavor to Adam comes into play\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache +  (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "        # Get corrected cachebias\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    " \n",
    "        # Vanilla SGD parameter update + normalization with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "X, y = create_data(100, 3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 64)  # first dense layer, 2 inputs (each sample has 2 features), 3 outputs\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "#optimizer = Optimizer_SGD(decay=1e-8, momentum=0.9)\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, decay=1e-8)\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Make a forward pass of our training data thru this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Make a forward pass thru second Dense layer - it takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Calculate loss from output of activation2 so softmax activation\n",
    "    loss = loss_function.forward(activation2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    predictions = np.argmax(activation2.output, axis=1)  # calculate values along first axis\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f}, lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dvalues)\n",
    "    dense2.backward(activation2.dvalues)\n",
    "    activation1.backward(dense2.dvalues)\n",
    "    dense1.backward(activation1.dvalues)\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2) \n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing or Out-of-Sample Data\n",
    "- thus far, we've trained our neural network on generated data\n",
    "- this generated data is created based on a very clear set of rules set forth in the `create_data()` function\n",
    "- the expectation is that a well-trained neural network will learn a representation of these rules and be able to apply this representation to predict classes of additional, unseen generated data\n",
    "---\n",
    "- by having a massive amount of tunable parameters they are exceptional at “fitting” to data\n",
    "- the complexity of neural networks is both a gift and a curse, which is something we try to balance\n",
    "- with enough neurons, a model could easily memorize a dataset; however, with too few neurons, the model would not generalize well\n",
    "- we do not simply solve problems with neural networks by using the most neurons possible (will overfit training set)\n",
    "---\n",
    "- at the moment, we’re uncertain whether our latest neural network’s 98% accuracy is due to actually learning to meaningfully represent the underlying data-generation function or instead just **overfitting** the data\n",
    "- as previously explained, overfitting is effectively just memorizing the data without any understanding of it\n",
    "- an overfit model will do very well predicting the data that it has already seen, but very poorly on new, unseen data\n",
    "---\n",
    "- without knowing if a model overfitting, we cannot trust the model’s results\n",
    "- for this reason, it’s essential to have both training and testing data\n",
    "- training data should only be used to train a model\n",
    "- the testing, or out-of-sample data, should only be used to validate a model's performance after training\n",
    "---\n",
    "- **it is essential both datasets differ enough to prove the model's ability to generalize**\n",
    "- in time-series data, for example, a better approach than random selection is to take multiple slices of data, entire blocks of time, and reserve those for testing\n",
    "- in our case, we can use our data-generating function to easily create new data that will serve as out-of-sample/testing data: `X_test, y_test = create_data(100, 3)`\n",
    "- with this new data, we'll evaluate the model’s performance by doing a forward pass and calculating loss and accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "validation, acc: 0.803, loss: 0.983\n"
    }
   ],
   "source": [
    "# Create test dataset\n",
    "X_test, y_test = create_data(100, 3)\n",
    "\n",
    "# Make a forward pass of our testraining data thru this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# Make a forward pass thru activation function - we take output of previous layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Make a forward pass thru second Dense layer - it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Make a forward pass thru activation function - we take output of previous layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Calculate loss from output of activation2 so softmax activation\n",
    "loss = loss_function.forward(activation2.output, y_test)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "predictions = np.argmax(activation2.output, axis=1)  # calculate values along first axis\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- while 80% accuracy and a loss of 0.983 is not terrible, this contrasts our \"near-perfect\" training results\n",
    "- this is evidence that our neural network is overfitting the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alleviating Overfitting\n",
    "- to reduce overfitting, one option is to adjust the size of the model \n",
    "- if a model is not learning, try a larger model; if your model is learning well, try a smaller model\n",
    "- a general rule when selecting initial model hyperparameters is to find the smallest model possible that still learns\n",
    "- trying different variations of model settings is called **hyperparameter searching**\n",
    "- initially, you can very quickly try different settings (e.g., layer sizes) and see if the models are learning \n",
    "- if they are, train the models fully and compare results to pick the best set of hyperparameters\n",
    "- another option is to create a list of different hyperparameter sets and train the model in a loop using each of those sets at a time to pick the best set at the end\n",
    "- this is because the less neurons you have, the less chance you have that the model will overfit the training data\n",
    "- less neurons can mean it’s easier for a neural network to generalize (actually learn the meaning of the data) compared to memorizing the data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}