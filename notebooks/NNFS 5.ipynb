{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Cross-Entropy Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the **loss function** is the algorithm that quantifies how wrong a model is\n",
    "- since we are performing a classification task, we will use the **cross-entropy** loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_output = [0.7, 0.1, 0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in the code above, we have 3 class confidences; let's assume that the desired prediction is the 1st class (index 0)\n",
    "- if that's the desired prediction, then the desired probability distribution is `[1, 0, 0]`\n",
    "- arrays/vectors that look like this are called **one-hot**, which implies that only one of the values is \"hot\" (1), while the others are not (0)\n",
    "---\n",
    "- the code below depicts cross-entropy given a softmax output of `[0.7, 0.1, 0.2]` and a desired output of `[1, 0, 0]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.35667494393873245"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "softmax_output = [0.7, 0.1, 0.2]  # example output from the output layer of the neural network\n",
    "target_output = [1, 0, 0]  # ground truth\n",
    "\n",
    "loss = -(math.log(softmax_output[0])*target_output[0] + \n",
    "         math.log(softmax_output[1])*target_output[1] + \n",
    "         math.log(softmax_output[2])*target_output[2])\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- that's the complete cross-entropy calculation, but after a closer look, we can yield the same result from just the first line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.35667494393873245"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "loss = -math.log(softmax_output[0])\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- any mention of **log** will always refer to the **natural logarithm** \n",
    "- the natural log represents the solution for the x-term in the equation $e^x = b$\n",
    "- for example, $e^x = 5.2$ is solved by `np.log(5.2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1.6486586255873816"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "b = 5.2\n",
    "np.log(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "5.199999999999999"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "math.e ** 1.6486586255873816"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilities for 3 samples\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2], # each vector is a class\n",
    "                            [0.1, 0.5, 0.4], # each vector has 3 samples\n",
    "                            [0.02, 0.9, 0.08]]) # this is a batch of 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in this example there are 3 classes; let’s say we’re trying to classify something as a “dog,” “cat,” or “human” \n",
    "- a dog is class 0 (index 0), a cat class 1 (index 1), and a human class 2 (index 2)\n",
    "- let's assume the batch of three sample inputs to this neural network is mapped to target values of a dog, a cat, and another cat\n",
    " - so the targets are `[0, 1, 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "class_targets = [0, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.7\n0.5\n0.9\n"
    }
   ],
   "source": [
    "softmax_outputs = np.array([[0.7, 0.1, 0.2], # each vector is a distribution \n",
    "                            [0.1, 0.5, 0.4], # this batch has 3 distributions\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "class_targets = [0, 1, 1]\n",
    "\n",
    "for targ_idx, distribution in zip(class_targets, softmax_outputs):\n",
    "    print(distribution[targ_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- again, the `zip()` function let's us iterate over multiple iterables at the same time in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.7 0.5 0.9]\n"
    }
   ],
   "source": [
    "print(softmax_outputs[[0, 1, 2], class_targets]) # nicer output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the `[0, 1, 2]` are the indices of each distribution in the batch\n",
    "- we’re going to have as many indices as distributions in our entire batch, so we can use `range()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.7 0.5 0.9]\n"
    }
   ],
   "source": [
    "print(softmax_outputs[range(len(softmax_outputs)), class_targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we apply negative log to each value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.35667494 0.69314718 0.10536052]\n"
    }
   ],
   "source": [
    "print(-np.log(softmax_outputs[range(len(softmax_outputs)), class_targets])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- finally, what we really want is the average loss per batch \n",
    "- NumPy has a method that computes this average on arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.38506088005216804"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "neg_log = -np.log(softmax_outputs[range(len(softmax_outputs)), class_targets])\n",
    "average_loss = np.mean(neg_log)\n",
    "average_loss # average loss of entire batch of 3 distributions (classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let’s convert our loss code into a class for convenience down the line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy:\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Probabilities for target values\n",
    "        y_pred = y_pred[range(samples), y_true]\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(y_pred)\n",
    "\n",
    "        # Overall loss\n",
    "        data_loss = np.mean(negative_log_likelihoods)\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `y_pred` = outputs | `y_true` = class targets\n",
    "- this class performs all the error calculations that we derived throughout this chapter and can be used as an object\n",
    "- for example, using the manually-created output and targets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.38506088005216804"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "loss = loss_function.forward(softmax_outputs, class_targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "- again, loss is a useful metric for optimizing a model\n",
    "- **accuracy**, however, describes how often the largest confidence is the correct class in terms of a percent\n",
    "- to find this accuracy, we will use the `argmax()` values from the `softmax_outputs` and then compare these to the targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "acc: 0.6666666666666666\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "softmax_outputs = [[0.7, 0.2, 0.1],  # probabilities for 3 samples\n",
    "                   [0.5, 0.1, 0.4],  # values swapped here compared to examples above to create lower accuracy\n",
    "                   [0.02, 0.9, 0.08]]\n",
    "targets = [0, 1, 1]  # target (ground truth) labels for 3 samples\n",
    "\n",
    "predictions = np.argmax(softmax_outputs, axis=1)  # calculate values along first axis=1 (rows)\n",
    "accuracy = np.mean(predictions==targets) # True evaluates to 1; False to 0\n",
    "\n",
    "print('acc:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's add this accuracy measurement to the end of our code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.33333333 0.33333333 0.33333333]\n [0.3333332  0.33333321 0.33333358]\n [0.333333   0.33333302 0.33333397]\n [0.33333271 0.33333275 0.33333454]\n [0.33333247 0.33333252 0.33333501]]\nloss: 1.0986095611088502\naccuracy: 0.33\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def create_data(points, classes):\n",
    "    X = np.zeros((points*classes, 2))\n",
    "    y = np.zeros(points*classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(points*class_number, points*(class_number+1))\n",
    "        r = np.linspace(0.0, 1, points)  # radius\n",
    "        t = np.linspace(class_number*4, (class_number+1)*4, points) + np.random.randn(points)*0.05\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = class_number\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, inputs, neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(inputs, neurons)\n",
    "        self.biases = np.zeros((1, neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from input ones, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy:\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Probabilities for target values\n",
    "        y_pred = y_pred[range(samples), y_true]\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(y_pred)\n",
    "\n",
    "        # Average loss\n",
    "        data_loss = np.sum(negative_log_likelihoods) / samples\n",
    "        return data_loss\n",
    "\n",
    "# Create dataset\n",
    "X, y = create_data(100, 3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)  # first dense layer, 2 inputs (each sample has 2 features), 3 outputs\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(3, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Make a forward pass of our training data thru this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Make a forward pass thru activation function - we take output of previous layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Make a forward pass thru second Dense layer - it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Make a forward pass thru activation function - we take output of previous layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Let's see output of few first samples:\n",
    "print(activation2.output[:5])\n",
    "\n",
    "# Calculate loss from output of activation2 (softmax activation)\n",
    "loss = loss_function.forward(activation2.output, y)\n",
    "\n",
    "# Let's print loss value\n",
    "print('loss:', loss)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "predictions = np.argmax(activation2.output, axis=1)  # calculate values along first axis\n",
    "accuracy = np.mean(predictions==y)\n",
    "\n",
    "# Print accuracy\n",
    "print('accuracy:', accuracy) "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}